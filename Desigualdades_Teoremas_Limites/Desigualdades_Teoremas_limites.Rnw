\documentclass{article}
\usepackage[sc]{mathpazo}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{latexsym}
\usepackage{geometry}
\usepackage{bigints}
\geometry{verbose,tmargin=1.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
\usepackage{url}
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=2,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=false]
 {hyperref}
\hypersetup{
 pdfstartview={XYZ null null 1}}
\begin{document}
<<setup, include=FALSE, cache=FALSE>>=
library(knitr)
library(ggplot2)
library(grid)
# set global chunk options
opts_chunk$set(fig.path='figure/minimal-', fig.align='center', fig.show='hold')
options(formatR.arrow=TRUE,width=90)
@


\title{Desigualdades y Teoremas l\'imites}


\author{Curso: Introducci\'on a la Estad\'istica y Probabilidades CM-274}
\date{}
\maketitle

\setlength{\unitlength}{1in}
\begin{picture}(6,.1) 
\put(0,0) {\line(1,0){6.25}}         
\end{picture}


\vspace{0.5cm}

Las desigualdades y el comportamiento asint\'otico de secuencias de variables aleatorias es un parte importante de la teoria de las probabilidades. El principal contexto  involucra una secuencia $X_1, X_2, \dots $ de variables aleatorias independientes id\'enticamente distribuidas con media $\mu$ y varianza $\sigma^2$.  


\vspace{0.3cm}

Las desigualdades y los teoremas l\'imite son importantes por varias razones

\vspace{0.3cm}

\begin{itemize}
\item Las desigualdes como es el caso de  la \textit{desigualdad de Hoeffding} puede aplicarse al caso especial de variables de Bernoulli id\'enticamente distribuidas, que es la manera en que se  aplica frecuentemente en combinatoria y en inform\'atica. 
\item Los teoremas l\'imite proporcionan conceptualmente una interpretaci\'on de la probabilidad y esperanza, en t\'erminos de secuencias independiente y id\'enticas.
\item Juegan un rol en Inferencia y estad\'istica, en la presencia de grandes conjuntos de datos.
\end{itemize}

\vspace{0.2cm}

En esta secci\'on presentamos algunas de las m\'as importantes desigualdades de la Teor\'ia de las \mbox{Probabilidades} y ciertas aplicaciones que conllevan.



\vspace{0.5cm}

\textbf{Desigualdad de Markov} Sea $X$ es una variable no negativa y supongamos que  $\mathbb{E}(X)$ existe. 

\[
\mathbb{P}(X > a)\leq \frac{\mathbb{E}(X)}{a}.
\]

\vspace{0.2cm}

Para alg\'un $a > 0$.

\vspace{0.2cm}


Fijemos un n\'umero positivo $a$ y consideremos una variable aleatoria $Y_a$ definida

\vspace{0.2cm}

\[
Y_a = \begin{cases}
0, & \text{si}\ \ X  < a\\
a, & \text{si}\ \ X \leq a
\end{cases}
\]

\vspace{0.2cm}

Luego 

\vspace{0.2cm}

\[
Y_a \leq X
\]


\vspace{0.2cm}

lo que implica 

\vspace{0.2cm}

\[
E(Y_a) \leq E(X)
\]

\vspace{0.2cm}

\[
E(X) \geq E(Y_a) = aP(Y_a = a) = aP(X \geq a)
\]

\vspace{0.2cm}

Por tanto : 

\vspace {0.2cm}

\[
aP(X \leq a) \leq E(X).
\]

\newpage

Este resultado es ilustrado mediante el siguiente gr\'afico

\begin{figure}[h]
\centering
\includegraphics[scale=.55]{m1.png}
\end{figure}

\vspace{0.3cm}

Donde en la parte a) muestra el \texttt{pdf} de una variable aleatoria no negativa $X$. La parte b) muestra el \texttt{pmf} de una variable $Y_a$, que se construye de la siguiente manera: Toda la masa de probabilidad en el \texttt{pdf} de X que se encuentra entre $0$ y $a$ se asigna a $0$, y toda la masa que se encuentra por encima de $a$ es asignada a $a $ Puesto que la masa se desplaza a la izquierda, la esperanza s\'olo puede disminuir.


\vspace{0.3cm}

\textbf{Ejemplo} Sea $X$ la variable aleatoria que denota la edad  (en a\~nos) de un ni\~no de un determinado colegio. Si la edad promedio de edad de ese colegio es $12.5$ a\~nos, entonces usando la desigualdad de Markov, la probabilidad que un ni\~no es tiene por lo menos 20 a\~nos satisface la desigualdad 

\vspace{0.2cm}

\[
P(X \geq 20) \leq 12.5/20 = 0.6250?
\]

\vspace{0.5cm}

En general si $X$ es una variable aleatoria y $h$ una funci\'on no decreciente  y no negativa. La esperanza $h(X)$ asumiendo que existe, es dada por

\vspace{0.3cm}

\[
E(h(X)) = \bigintsss_{-\infty}^{\infty}h(u)f_{X}(u)du
\]

\vspace{0.2cm}

y escribimos 

\vspace{0.2cm}

\[
\bigintsss_{-\infty}^{\infty}h(u)f_{X}(u)du \geq \bigintsss_{a}^{\infty}h(u)f_{X}(u)du \geq h(a)\bigintsss_{t}^{\infty}f_{X}(u)du = h(a)P(X \geq a).
\]

\vspace{0.3cm}

Esto conduce a la desigualdad de Markov: $P(X \geq a) \leq \dfrac{E(h(X))}{h(a)} $ para $a > 0$.




\vspace{0.5cm}

La desigualdad de Chebyshev, dice que si una variable aleatoria tiene una peque\~na varianza, entonces la probabilidad que toma un valor lejos de la media es tambi\'en peque\~na.

\vspace{0.5cm}

\textbf{Desigualdad de Chebyshev} Sea $\mu = \mathbb{E}(X)$ y $\sigma^2 = V(X)$. Entonces para un $t > 0$

\vspace{0.2cm}

\[
\mathbb{P}(\vert X- \mu \vert \geq t) \leq \frac{\sigma^2}{t^2}\qquad y \qquad \mathbb{P}(\vert Z \vert \geq k) \leq \frac{1}{k^2}.
\]

\vspace{0.2cm}

donde $Z = (X - \mu)/\sigma$. En particular $\mathbb{P}(\vert Z \vert > 2) \leq 1/4$ \qquad y \qquad  $\mathbb{P}(\vert Z \vert > 3) \leq 1/9$.

\vspace{0.5cm}

Aplicando la desigualdad de Markov, podemos concluir que

\vspace{0.2cm}

\[
P((X -\mu)^2 \leq t^2) \leq \dfrac{E(X -\mu)^2}{t^2} = \dfrac{\sigma^2}{t^2}.
\]

\vspace{0.3cm}

Para completar el evento $(X - \mu)^2 \geq t^2$ es id\'entico al evento $\vert X - \mu\vert \geq t$, as\'i

\vspace{0.2cm}

\[
P(\vert X - \mu \vert \leq t) = P((X -\mu)^2 \leq t^2) \leq \dfrac{\sigma^2}{t^2}.
\]

Para la segunda parte se coloca $t = k\sigma$.

\vspace{0.3cm}

\textbf{Ejemplo} Supongamos que probamos un modelo de predicci\'on sobre un conjunto de $n$  casos de prueba. Sea $X_i = 1$ si el predictor es incorrecto y $X_i = 1$ si el predictor es correcto. Entonces $\overline{X}_n = n^{-1}\displaystyle\sum_{i =1}^{n}X_i$ es la taza de error observado. Cada $X_i$ puede ser considerado una variable de Bernoulli con media desconocida $p$. Intuitivamente $\overline{X}_n$ debe ser cercano a $p$, pero que tan probable es que $\overline{X}_n$ est\'e fuera de un entorno de tama\~no $\epsilon$, sabiendo que $V(\overline{X}_n =V(X_1)/n = p(1 -p)/n$ y


\vspace{0.3cm}


\[
P(\vert \overline{X}_n -p \vert > \epsilon) \leq \dfrac{V(\overline{X}_n)}{\epsilon^{2}} = \dfrac{p(1 -p)}{n\epsilon^2} \leq \dfrac{1}{4n\epsilon^2}
\]


\vspace{0.3cm}

desde que $p(1 -p) \leq \frac{1}{4}$ para todo $p$. Para $\epsilon = .2$ y $n = 100$, la cota es $.0625$.

\vspace{0.8cm}

\textbf{\large{Desigualdad de Hoeffding}}


\vspace{0.3cm}

Empezamos con un resultado importante

\vspace{0.5cm}


\textbf{Propiedad 1} Supongamos que $\mathbb{E}(X) = 0$ y que $a \leq x \leq b$. Entonces

\vspace{0.3cm}

\[
\mathbb{E}(e^{tX}) \leq e^{t^2(b -a)^2/8}.
\]

\vspace{0.3cm}

Para desarrollar la desigualdad de  Hoeffding, necesitamos el \texttt{m\'etodo de Chernoff}

\vspace{0.3cm}

\textbf{M\'etodo de Chernoff} Sea $X$ una variable aleatoria. Entonces

\vspace{0.2cm}

\[
\mathbb{P}(X > \epsilon) \leq \inf_{t \geq 0}e^{-t\epsilon}\mathbb{E}(e^{tX}).
\]


\vspace{0.2cm}

Este m\'etodo puede ser derivado de la desigualdad de Markov en la funci\'on $h(x) = e^{tx}$ que la funci\'on generadora de momentos es $E(e^{tX})$.


\vspace{0.2cm}


Para alg\'un $t > 0$

\vspace{0.2cm}

\[
P(X >\epsilon) = P(e^ X > e^{\epsilon}) = P(e^{tX} > e^{t\epsilon}) \leq e^{-t\epsilon}E(e^{tX}).
\]

Desde que esto es cierto para $t \geq 0$ el resultado sigue.


\vspace{0.8cm}



Sean $Y_1, Y_2, \dots, Y_n$ observaciones indepedientes, tal que $\mathbb{E}(Y_i)=0 $ y $a_i \leq Y_i \leq b_i$. Sea $\epsilon > 0$. Entonces, para alg\'un $ t > 0$

\vspace{0.2cm}

\[
\mathbb{P}(\vert \overline{Y}_n - \mu \vert \geq \epsilon) \leq 2e^{-2n\epsilon^2/(b -a)^2}.
\]


\vspace{0,3cm}


Sin p\'erdida de generalidad, supongamos $\mu = 0$. Primero se tiene

\vspace{0.3cm}

\begin{align*}
P(\vert \overline{Y}_n\vert \geq \epsilon) = P( \overline{Y}_n \geq \epsilon) + P( \overline{Y}_n\vert \leq -\epsilon) \\
= P( \overline{Y}_n \geq \epsilon) + P( -\overline{Y}_n \geq \epsilon)
\end{align*}

\vspace{0.3cm}

Usamos el m\'etodo de chernoff. Para alg\'un $t > 0$, desde la desigualdad de Markov

\vspace{0.2cm}

\begin{align*}
P( \overline{Y}_n \geq \epsilon) = P\Bigl(\displaystyle\sum_{i =1}^{n}Y_i \geq n\epsilon\Bigr) = P(e^{\sum_{i =1}^{n}y_i}\geq e^{n\epsilon})\\
= P \Bigl(e^{t\sum_{i =1}^{n}y_i}\geq e^{tn\epsilon}\Bigr) \leq e^{-tn\epsilon}E\Bigl(e^{t\sum_{i =1}^{n}y_i} \Bigr)\\
=e^{-tn\epsilon}\displaystyle\prod_{i}(e^{tY_i}) = e^{-tn\epsilon}(E(e^{tY_i}))^{n}
\end{align*}


\vspace{0.3cm}

Por la propiedad 1: $E(e^{tY_i}) \leq e^{t^2(b -a)^2/8}$. As\'i

\vspace{0.3cm}

\[
P( \overline{Y}_n \geq \epsilon) \leq e^{-tn\epsilon}e^{t^2n(b -a)^2/8}
\]

\vspace{0.3cm}

Esto se reduce al m\'inimo cuando $t = 4\epsilon/(b -a)^2$ dando 

\vspace{0.3cm}

\[
P( \overline{Y}_n \geq \epsilon) \leq e^{-2n\epsilon^2/(b -a)^2}
\]

\vspace{0.3cm}

Aplicando el mismo argumento a $P( - \overline{Y}_n \geq \epsilon)$ se produce el resultado.


\vspace{0.8cm}

\textbf{Corolario} Si $X_1, X_2, \cdots, X_n$ son independientes con $\mathbb{P}(a \leq X_i \leq b) = 1$   y una media com\'un $\mu$, entonces con probabilidad de al menos
$1 -\delta$


\vspace{0.3cm}

\[
\vert \overline{X}_n - \mu \vert \leq \sqrt{\dfrac{(b -a)^2}{2n}\log\Bigl(\dfrac{2}{\delta}\Bigr)}.
\]


\vspace{0.3cm}

\textbf{Ejemplo} Sea $X_1, X_2, \dots, X_n \sim \mbox{Bernoulli}(p)$. Desde la desigualdad Hoeffding se tiene

\[
\mathbb{P}(|\overline{X_n} -p | > \epsilon) \leq 2\epsilon^{-2n\epsilon^2}.
\]


\vspace{0.5cm}



\textbf{Teorema de McDiarmid} Sea $X_1, X_2, \dots X_n$ variables aleatorias independientes. Suponganse que
\[
\sup_{x_1,\dots, x_n, x^{'}_{i}}\Bigl\vert g(x_1,\dots, x_{i - 1}, x_i, x_{i + 1}, \dots, x_n ) - g(x_1,\dots, x_{i - 1}, x^{'}_i, x_{i + 1}, \dots, x_n )\Bigr\vert \leq c_i
\]

para $i = 1,\dots, n$. Entonces

\[
\mathbb{P}\Bigl(g(X_1,\dots, X_n) -\mathbb{E}(g(X_1,\dots, X_n))\geq \epsilon \Bigr) \leq \exp\Bigl\{-\frac{2\epsilon^2 }{\sum_{i = 1}^{n}c^{2}_i}    \Bigr\}.
\]



\vspace{0.3cm}


\textbf{Ejemplo} Supongamos que lanzamos $m$ pelotas en $n$ recipientes. ?` Qu\'e fracci\'on de los recipientes est\'an vacios?.

Sea $M$ el n\'umero de recipientes vacios y sea $F = M/n$ la fracci\'on de recipientes vacios. Podemos escribir $Z = \sum_{i =1}^{n}Z_i$, donde $Z_i = 1$ de un recipiente $i$ est\'a vac\'io y $Z_i = 0$ en otro caso. Entonces

\[
\mu = \mathbb{E}(Z) = \sum_{i =1}^{n}\mathbb{E}(Z_i) = n(1 -1/n)^m = ne^{m\log(1 -1/n)} \approx ne^{-m/n}.
\]


y  $\theta =\mathbb{E}(F) = \mu/n \approx e^{-m/n}$. ?` Cu\'an cerca est\'a Z de $\mu$?.

\vspace{0.3cm}


Para ello definamos las variables $X_1, \dots, X_m$ donde $X_s = i$ si la pelota $s$ cae en el recipiente $i$. Entonces $Z = g(X_1,\dots, X_m)$. Si movemos una pelota a un recipiente diferente, entonces $Z$ puede cambiar a lo m\'as $1$. As\'i del Teorema de McDiarmind con $c_i = 1$ muestra que

\[
\mathbb{P}(|Z - \mu| > t) \leq 2e^{-2t^2/m}.
\]

\vspace{0.3cm}

Como las fracci\'on de recipientes vacios es $F=Z/m$ con media $\theta = \mu/n$. Tenemos

\vspace{0.3cm}

\[
\mathbb{P}(|F - \theta| > t) = \mathbb{P}(|Z - \mu| > nt) \leq 2e^{-2n^2t^2/m}.
\]

\vspace{0.5cm}



\textbf{Desigualdad de Cauchy -Schwartz} Si $X$ y $Y$ tienen varianza finita, entonces

\[
\mathbb{E}\vert XY\vert \leq \sqrt{\mathbb{E}(X^2)\mathbb{E}(Y^2)}.
\]

\vspace{0.3cm}


La desigualdad de Cauchy-Schwarz puede ser escrita como

\[
\mbox{Cov}^2(X,Y) \leq \delta_{X}^{2}\delta_{Y}^{2}.
\]

\textbf{Desigualdad de Jensen} Si $g$ es convexa, entonces
\[
\mathbb{E}g(X) \geq g(\mathbb{E}X)
\]

Si $g$ es c\'oncava

\[
\mathbb{E}g(X) \leq g(\mathbb{E}X).
\]


\vspace{0.5cm}

\textbf{\large{Ejemplo}}

\vspace{0.3cm}

\textbf{Distancia de Kullback-Leibler} 

\vspace{0.2cm}

Definamos la distancia de \textbf{Kullback-Leibler} entre dos densidades $f_1$ y $f_2$ dada
\[
D(f_1, f_2) = \int{f_1(x)\log\Bigl(\frac{f_1(x)}{f_2(x)}\Bigr)dx}
\]


Nota que $D(f_1,f_1) = 0$ y usando la desigualdad de Jensen se cumple que  $D(f_1, f_2) \geq 0$. La distancia de Kullbacker-Leibler es una medida de la informaci\'on que se pierde cuando $f_1$ se aproxima a $f_2$. Un ejemplo en R.

<<fooq, comment = NA, prompt =TRUE, eval= FALSE>>=
set.seed(1000)
X<- rexp(10000, rate=0.2)
Y<- rexp(10000, rate=0.4)
KL.dist(X, Y, k=5)
KLx.dist(X, Y, k=5)
#Distancia teorica = (0.2-0.4)^2/(0.2*0.4) = 0.5.
@

El anterior sript usa el paquete \textbf{FNN } \textit{Fast Nearest Neighbor Search Algorithms and Applications} de $R$. 


\vspace{0.3cm}


La distancia de Kullbacker-Leibler entre una distribuci\'on Gaussiana $p$ con media $\mu_1$ y varianza $\sigma^2_1$ y una distribuci\'on Gaussiana $q$ con media $\mu_2$ y varianza $\sigma^2_2$ es dado por

\[
D(p,q) = \log \frac{\sigma_2}{\sigma_1} + \frac{\sigma^2_1 + (\mu_1 - \mu_2)^2}{2\sigma^2_2} - \frac{1}{2}.
\]


\vspace{0.3cm}


\textbf{Teorema} Supongamos que $X_n \geq 0$ y para cada $\epsilon >0$,

\[
\mathbb{P}(X_n > \epsilon) \leq c_1e^{-c_2n\epsilon^2}
\]

para alg\'un $c_2 >0$ y $c_1 > 1/e$. Entonces

\[
\mathbb{E}(X_n) \leq \sqrt{\dfrac{C}{n}}.
\]

donde $C = (1 + \log(c_1))/c_2$

\vspace{0.3cm}

\textbf{Teorema} Sean $X_1,\dots, X_n$ variables aleatorias. Supongamos que existe un $\delta >0$ tal que $\mathbb{E}(e^{tX_i}) \leq e^{t^2\delta^2/2}$ para todo $t > 0$. Entonces 


\[
\mathbb{E}\Bigl( \max_{1 \leq i \leq n}X_i \Bigr) \leq \delta \sqrt{2 \log n}.
\]


\vspace{0.3cm}

En Estad\'istica, Probabilidades y Machine Learning, usamos la notaci\'on $o_{P}$ y $O_{P}$. Por ejemplo, $a_n = o(1)$ significa que $a_n \rightarrow 0$ cuando $n \rightarrow \infty$. $a_n = o(b_n)$ significa $a_n/b_n = o(1)$.

\vspace{0.2cm}

$a_n = O(1)$ (eventualmente acotados) significa que para un $n$ una cantidad muy grande, $\vert a_n \vert \leq C$ para alg\'un $C >0$. $a_n = O(b_n)$, significa que $a_n/b_n = O(1)$.

\vspace{0.2cm}

Escribimos $a_n \sim b_n$ si ambos $a/b$ y  $b/a$ son eventualmente acotados ($a_n = \Theta(b_n)$). Veamos estas cosas en el terreno de las probabilidades, es decir $Y_n = o_P(1)$ significa que para cada $\epsilon > 0$,

\[
\mathbb{P}(\vert Y_n \vert > \epsilon ) \rightarrow 0.
\]
Decimos que $Y_n = o_P(a_n)$ si, $Y_n/a_n = o_{P}(1)$. 

\vspace{0.3cm}

Si tenemos $Y_n = O_P(1)$ si, para cada $\epsilon > 0$, existe un $C > 0$ tal que

\[
\mathbb{P}(\vert Y_n \vert > C) \leq \epsilon.
\]
\vspace{0.3cm}

Se dice que $Y_n = O_P(a_n)$ si $Y_n/a_n = O_p(1)$. Sean $Y_1, Y_2, \dots, Y_n$ lanzamientos de monedas, esto es $Y_i \in \{0,1 \}$. Sea $p = \mathbb{P}(Y_i = 1)$. Sea 

\[
\hat{p_n} = \frac{1}{n}\sum_{i = 1}^{n}Y_i
\]
Se prueba que: $\hat{p_n} - p = o_P(1)$ y   $\hat{p_n} - p = O_P(1/\sqrt{n})$.

\vspace{0.8cm}


{\Large{La ley de promedios}}

\vspace{0.5cm}

Si lanzamos un dado $5$ millones de veces y se mantiene un registro de los datos. El promedio de los n\'umeros  los cuales fueron lanzados es $3.500867$. Desde que la media de cada lanzamiento es $\frac{1}{6}(1 + 2 + \cdots +6) = 3\frac{1}{2}$, este resultado muestra  que para un $x_i$ el $i$ \'esimo lanzamiento, el promedio

\vspace{0.3cm}

\[
a_n = \dfrac{1}{n}(x_i + x_2 + \cdots + x_n)
\]

\vspace{0.3cm}

se acerca a la media $3\dfrac{1}{2}$, cuando $n \rightarrow \infty$.

\vspace{0.2cm}

En general si tenemos una secuencia $X_1, X_2, \dots$ de variables aleatorias identicamente distribuidas e independientes cada una teniendo una media $\mu$, deberiamos probar que el promedio

\vspace{0.3cm}

\[
\dfrac{1}{n}(X_1 + X_2 +\cdots +X_n)
\]

\vspace{0.2cm}

converge cuando $n \rightarrow \infty$ a la media $\mu$.

\vspace{0.5cm}

\textbf{Definici\'on} Decimos que la secuencia $x_1, x_2, \dots$ de variables aleatorias \texttt{converge en media  cuadrada a la variables x} si

\vspace{0.3cm}

\[
E([X_n - x]^2)\rightarrow 0 \ \ \text{cuando}\ \ n\rightarrow \infty
\]

\vspace{0.2cm}

y se escribe \texttt{$X_n \rightarrow x$ en media cuadrada cuando $n \rightarrow \infty$}.

\vspace{0.5cm}


\textbf{Ejemplo} Sea $X_n$ una secuencia de variables aleatorias discretas con un \texttt{pmf}

\vspace{0.2cm}

\[
P(X_n = 1) =  \dfrac{1}{n},  \ \ \ \ P(X_n = 2) = 1 - \dfrac{1}{n}.
\]

\vspace{0.2cm}

Entonces $X_n$ converge a la variable aleatoria $2$ en media cuadrada cuando $n \rightarrow \infty$, desde que

\vspace{0.3cm}

\begin{align*}
E([X_n - 2]^2) = (1 -2)\dfrac{1}{n} + (2 -2)^2\Bigl(1 - \dfrac{1}{n}\Bigr)\\
= \dfrac{1}{n} \rightarrow 0 \ \ \ \text{cuando}\ \ n \rightarrow \infty.
\end{align*}


\vspace{0.8cm}


\textbf{Ley de los grandes n\'umeros(1)} Sea una secuencia $X_1, X_2, \dots $ una secuencia de variables independientes cada una con media $\mu$ y varianza $\sigma^2$. El promedio de los primeros $n$ de las $X_i$ satisfacen cuando $n \rightarrow \infty$.

\vspace{0.3cm}

\[
\dfrac{1}{n}(X_1 +X_2 + \cdots + X_n) \rightarrow \mu \ \ \ \text{en media cuadrada}.
\]


\vspace{0.5cm}


Sea $S_n = X_1 + X_2 + \cdots X_n$, la n-\'esima suma parcial de los $X_i$. Entonces

\vspace{0.3cm}

\[
E\Bigl(\dfrac{1}{n}S_n\Bigr) = \dfrac{1}{n}E(X_1 +X_2 + \cdots + X_n) = \dfrac{1}{n}n\mu = \mu.
\]

\vspace{0.3cm}

y as\'i

\vspace{0.3cm}

\begin{align*}
E\Bigl(\Bigl[\dfrac{1}{n}S_n - \mu \Bigr]^2\Bigr) &= var\Bigl(\dfrac{1}{n}S_n\Bigr)\\
  &= \dfrac{1}{n^2}var(X_1 +X_2 + \cdots + X_n)\\
  &= \dfrac{1}{n^2}(var X_1 + \cdots  + var X_n)\\
  &=\dfrac{1}n{n^2}\sigma^2 = \dfrac{1}{n}\sigma^2 \rightarrow 0 \ \ \ \text{cuando}\ \ n \rightarrow \infty
\end{align*}

\vspace{0.3cm}

y as\'i, $n^{-1}S_n \rightarrow \mu$ en media cuadr\'atica cuando $n \rightarrow \infty$.

\vspace{0.5cm}

\textbf{Definici\'on} Decimos que la secuencia de variables aleatoria $X_1, X_2, \dots$ de variables aleatorias \texttt{converge en probabilidad } a $X$ si

\vspace{0.3cm}

\[
\texttt{Para todo} \ \ \epsilon >0 \ \ \ P(\vert X_n - X\vert > \epsilon) \rightarrow \ \ \text{cuando}\ \ n \rightarrow \infty
\]

\vspace{0.3cm}

Si esto se cumple, escribimos \texttt{$X_n \rightarrow X$ en probabilidad cuando $n \rightarrow \infty$ }

\vspace{0.5cm}


\textbf{Teorema } Si $X_1, X_2, \dots $ es una secuencia de variables aleatorias y $X_n \rightarrow X$ en media cuadrada cuando $n \rightarrow \infty$ entonces $X_n \rightarrow X$ en probabilidad.


\vspace{0.5cm} 

Para hacer la prueba de este teorema, escribamos la \texttt{la desigualdad de Chebyshev} de la siguiente manera

\vspace{0.3cm}

Si $Y$ es una variable aleatoria y $E(Y^2) < \infty$, entonces

\vspace{0.2cm}

\[
P(\vert Y \vert \geq t) \leq \dfrac{1}{t^2}E(Y^2)\ \ \ \text{para}\ \ \ t >0
\]


\vspace{0.5cm}

Luego aplicando la desigualdad de Chebyshev a la variable aleatoria $Y = X_n - X$ para encontrar que 

\vspace{0.3cm}

\[
P(\vert X_n -X\vert > \epsilon) \leq \dfrac{1}{\epsilon^2}E([X_n -X]^2)\ \ \ \epsilon > 0.
\]

\vspace{0.3cm}

Si $X_n \rightarrow X$ en media cuadrada cuando $n \rightarrow \infty$, el lado derecho de la desigualdad tiende a $0$ cuando $n \rightarrow \infty$ y as\'i tiende a $0$ para todo $\epsilon > 0$, como es requerido. El caso contrario no se cumple.

\vspace{0.8cm}

\textbf{Ley d\'ebil de los grandes n\'umeros} Sea $X_1, X_2, \dots$ una secuencia de variables aleatorias, cada una con media $\mu$ y varianza $\sigma^2$. El promedio de los primeros $n$ de los $X_i$ satisface, cuando $n \rightarrow \infty$

\vspace{0.3cm}

\[
\dfrac{1}{n}(X_1 +X_2 + \cdots + X_n) \rightarrow \mu \ \ \ \text{en probabilidad}.
\]

\vspace{0.5cm}


\textbf{Ejemplo} Sea $X_n$ una secuencia de variables aleatorias que convergen en probabilidad, pero  no en media cuadrada con un \texttt{pmf}


\vspace{0.3cm}

\[
P(X_n = 0) =  1 - \dfrac{1}{n},  \ \ \ \ P(X_n = n) = \dfrac{1}{n}.
\]


\vspace{0.2cm}

Entonces, para $\epsilon > 0$ y para $n$ dado 

\vspace{0.2cm}


\[
P(\vert X_n\vert > \epsilon) = P(X_n = n) = \dfrac{1}{n} \rightarrow 0 \ \ \ \text{cuando}\ \ n \rightarrow \infty
\]

\vspace{0.2cm}

dando que $X_n \rightarrow \infty $ en probabilidad. Por otro lado 

\vspace{0.2cm}

\begin{align*}
E([X_n - 0]^{2}) = E(X_n^2) = 0\Bigl(1 -\dfrac{1}{n}\Bigr) + n^2\dfrac{1}{n}\\
= n \rightarrow \infty \ \  \text{cuando} \ \ n \rightarrow \infty.
\end{align*}

\vspace{0.2cm}

as\'i $X_n$ no converge a $0$ en media cuadrada.


\vspace{0.8cm}

\textbf{Teorema del l\'imite central} Sea $X_1, X_2, \dots $ variables aleatorias identicamente distribuidas e independientes cada una con media $\mu$ y varianza distinta de cero $\sigma^2$. Por la ley de los grandes n\'umeros, la suma $S_n = X_1 +X_2+ \cdots +X_n$ es casi tan grande que $n\mu$ para valores de $n$ muy grandes. Un siguiente paso es determinar el orden de la diferencia $S_n - n\mu$, que resulta ser de orden $\sqrt{n}$.

\vspace{0.2cm}

Se define la versi\'on est\'andar de $S_n$

\vspace{0.2cm}

\[
Z_n = \dfrac{S_n -E(S_n)}{\sqrt{var(S_n)}}
\]

\vspace{0.2cm}

Esta es una funci\'on lineal $Z_n = a_nS_n + b_n$ de $S_n$, donde $a_n$ y $b_n$ tienen que ser elegida de manera que $E(Z_n) = 0$ y $var(Z_n) = 1$. Adem\'as

\vspace{0.2cm}

\begin{align*}
E(S_n ) &= E(X_1) + E(X_2) + \cdots + E(X_n )\\
&= n\mu
\end{align*}

\vspace{0.3cm}


Tambi\'en 

\vspace{0.2cm}

\begin{align*}
var(S_n) &= var(X_1) + var(X_2) + \cdots +var(X_n)\\
&= n\sigma^2
\end{align*}

\vspace{0.2cm}

y as\'i

\vspace{0.2cm}

\[
Z_n = \dfrac{S_n - n\mu}{\sigma\sqrt{n}}
\]


Muchas de las propiedades de $Z_n$ se pueden sintetizar en el teorema de l\'imite central:

\vspace{0.3cm}

Sean $X_1, X_2, \dots $ variables aleatorias identicamente distribuidas e independiente, cada una con media $\mu$ y varianza distinta de cero $\sigma^2$. La vesrsi\'on est\'andar 

\vspace{0.2cm}

\[
Z_n = \dfrac{S_n - n\mu}{\sigma\sqrt{n}}
\]

\vspace{0.2cm}

de la suma $S_n = X_1 +X_2 + \cdots + X_n$, satisface cuando $n \rightarrow \infty $.

\vspace{0.3cm}

\[
P(Z_n \leq x) \rightarrow \bigintss_{-\infty}^{x}\dfrac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}u^2}du\ \ \ \text{para}\ \ x\in \mathbb{R}.
\]

\vspace{0.2cm}

El lado derecho de la \'ultima de la ecuaci\'on es s\'olo la funci\'on de distribuci\'on de la distribuci\'on normal con media $0$ y varianza $1$, as\'i que esta expresi\'on se puede escribir como

\vspace{0.2cm}

\[
P(Z_n \leq x) \rightarrow P(Y \leq x) \ \ \ \text{para}\ \ x \in \mathbb{R}.
\]

\vspace{0.2cm}

donde $Y$ es la variable aleatoria con esta distribuci\'on normal est\'andar.


\vspace{0.2cm}

Para la prueba(parcial) de este teorema usaremos el m\'etodo de las funciones generadoras de momentos

\vspace{0.5cm}

\textbf{Teorema de continuidad} Sea $X_1, X_2, \dots$ una secuencia de variables aleatorias con funciones generadores de momentos $M_1, M_2, \dots$ y suponiendo que para $n \rightarrow \infty$

\vspace{0.3cm}

\[
M_n(t) \rightarrow e^{\frac{1}{2}t^2}\ \  \text{para } \ \ t\in \mathbb{R}
\]

\vspace{0.2cm}

Entonces

\vspace{0.2cm}

\[
P(Z_n \leq x) \rightarrow \bigintss_{-\infty}^{x}\dfrac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}u^2}du \ \ \ \text{para}\ \ x\in \mathbb{R}.
\]

\vspace{0.3cm}

En otras palabras, la funci\'on distribuci\'on de $Z_n$ converge a la funci\'on distribuci\'on de la distribuci\'on  normal si la funci\'on generadora de momentos de $Z_n$ converge a la funci\'on generadora de momentos de la distribuci\'on normal. 

\vspace{0.2cm}

Usamos estas propiedades  para probar el \texttt{teorema del l\'imite central} en el caso que las $X_i$ tienen una funci\'on generadora de momentos

\vspace{0.3cm}


\[
M_{X}(t) = E(\exp(tX_i))\ \ \ \text{para}\ \  i=1,2 \dots
\]

\vspace{0.2cm}


haciendo hincapi\'e en que el teorema del l\'imite central es v\'alida incluso cuando esta esperanza no existe, siempre que la media y la varianza de la $X_i$ sean finitas.
\end{document}

