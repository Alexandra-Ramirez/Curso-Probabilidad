\documentclass{article}
\usepackage[sc]{mathpazo}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{latexsym}
\usepackage{geometry}
\usepackage{bigints}
\geometry{verbose,tmargin=1.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
\usepackage{url}
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=2,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=false]
 {hyperref}
\hypersetup{
 pdfstartview={XYZ null null 1}}
\begin{document}
<<setup, include=FALSE, cache=FALSE>>=
library(knitr)
library(ggplot2)
library(grid)
# set global chunk options
opts_chunk$set(fig.path='figure/minimal-', fig.align='center', fig.show='hold')
options(formatR.arrow=TRUE,width=90)
@


\title{Desigualdades y Teoremas l\'imites}


\author{Curso: Introducci\'on a la Estad\'istica y Probabilidades CM-274}
\date{}
\maketitle

\setlength{\unitlength}{1in}
\begin{picture}(6,.1) 
\put(0,0) {\line(1,0){6.25}}         
\end{picture}


\vspace{0.5cm}

Las desigualdades y el comportamiento asint\'otico de secuencias de variables aleatorias es un parte importante de la teoria de las probabilidades. El principal contexto  involucra una secuencia $X_1, X_2, \dots $ de variables aleatorias independientes id\'enticamente distribuidas con media $\mu$ y varianza $\sigma^2$.  


\vspace{0.3cm}

Las desigualdades y los teoremas l\'imite son importantes por varias razones

\vspace{0.3cm}

\begin{itemize}
\item Las desigualdes como es el caso de  la \textit{desigualdad de Hoeffding} puede aplicarse al caso especial de variables de Bernoulli id\'enticamente distribuidas, que es la manera en que se  aplica frecuentemente en combinatoria y en inform\'atica. 
\item Los teoremas l\'imite proporcionan conceptualmente una interpretaci\'on de la probabilidad y esperanza, en t\'erminos de secuencias independiente y id\'enticas.
\item Juegan un rol en Inferencia y estad\'istica, en la presencia de grandes conjuntos de datos.
\end{itemize}

\vspace{0.2cm}

En esta secci\'on presentamos algunas de las m\'as importantes desigualdades de la Teor\'ia de las \mbox{Probabilidades} y ciertas aplicaciones que conllevan.



\vspace{0.5cm}

\textbf{Desigualdad de Markov} Sea $X$ es una variable no negativa y supongamos que  $\mathbb{E}(X)$ existe. 

\[
\mathbb{P}(X > a)\leq \frac{\mathbb{E}(X)}{a}.
\]

\vspace{0.2cm}

Para alg\'un $a > 0$.

\vspace{0.2cm}


Fijemos un n\'umero positivo $a$ y consideremos una variable aleatoria $Y_a$ definida

\vspace{0.2cm}

\[
Y_a = \begin{cases}
0, & \text{si}\ \ X  < a\\
a, & \text{si}\ \ X \leq a
\end{cases}
\]

\vspace{0.2cm}

Luego 

\vspace{0.2cm}

\[
Y_a \leq X
\]


\vspace{0.2cm}

lo que implica 

\vspace{0.2cm}

\[
\mathbb{E}(Y_a) \leq \mathbb{E}(X)
\]

\vspace{0.2cm}

\[
\mathbb{E}(X) \geq \mathbb{E}(Y_a) = a\mathbb{P}(Y_a = a) = a\mathbb{P}(X \geq a)
\]

\vspace{0.2cm}

Por tanto : 

\vspace {0.2cm}

\[
a\mathbb{P}(X \leq a) \leq \mathbb{E}(X).
\]

\newpage

Este resultado es ilustrado mediante el siguiente gr\'afico

\begin{figure}[h]
\centering
\includegraphics[scale=.55]{m1.png}
\end{figure}

\vspace{0.3cm}

Donde en la parte a) muestra el \texttt{pdf} de una variable aleatoria no negativa $X$. La parte b) muestra el \texttt{pmf} de una variable $Y_a$, que se construye de la siguiente manera: Toda la masa de probabilidad en el \texttt{pdf} de X que se encuentra entre $0$ y $a$ se asigna a $0$, y toda la masa que se encuentra por encima de $a$ es asignada a $a $ Puesto que la masa se desplaza a la izquierda, la esperanza s\'olo puede disminuir.


\vspace{0.3cm}

\textbf{Ejemplo} Sea $X$ la variable aleatoria que denota la edad  (en a\~nos) de un ni\~no de un determinado colegio. Si la edad promedio de edad de ese colegio es $12.5$ a\~nos, entonces usando la desigualdad de Markov, la probabilidad que un ni\~no es tiene por lo menos 20 a\~nos satisface la desigualdad 

\vspace{0.2cm}

\[
\mathbb{P}(X \geq 20) \leq 12.5/20 = 0.6250?
\]

\vspace{0.5cm}

En general si $X$ es una variable aleatoria y $h$ una funci\'on no decreciente  y no negativa. La esperanza $h(X)$ asumiendo que existe, es dada por

\vspace{0.3cm}

\[
\mathbb{E}(h(X)) = \bigintsss_{-\infty}^{\infty}h(u)f_{X}(u)du
\]

\vspace{0.2cm}

y escribimos 

\vspace{0.2cm}

\[
\bigintsss_{-\infty}^{\infty}h(u)f_{X}(u)du \geq \bigintsss_{a}^{\infty}h(u)f_{X}(u)du \geq h(a)\bigintsss_{t}^{\infty}f_{X}(u)du = h(a)\mathbb{P}(X \geq a).
\]

\vspace{0.3cm}

Esto conduce a la desigualdad de Markov: $\mathbb{P}(X \geq a) \leq \dfrac{\mathbb{E}(h(X))}{h(a)} $ para $a > 0$.


\vspace{0.5cm}

La desigualdad de Chebyshev, dice que si una variable aleatoria tiene una peque\~na varianza, entonces la probabilidad que toma un valor lejos de la media es tambi\'en peque\~na.

\vspace{0.5cm}

\textbf{Desigualdad de Chebyshev} Sea $\mu = \mathbb{E}(X)$ y $\sigma^2 = \mathbb{V}(X)$. Entonces para un $t > 0$

\vspace{0.2cm}

\[
\mathbb{P}(\vert X- \mu \vert \geq t) \leq \frac{\sigma^2}{t^2}\qquad y \qquad \mathbb{P}(\vert Z \vert \geq k) \leq \frac{1}{k^2}.
\]

\vspace{0.2cm}

donde $Z = (X - \mu)/\sigma$. En particular $\mathbb{P}(\vert Z \vert > 2) \leq 1/4$ \qquad y \qquad  $\mathbb{P}(\vert Z \vert > 3) \leq 1/9$.

\vspace{0.5cm}

Aplicando la desigualdad de Markov, podemos concluir que

\vspace{0.2cm}

\[
\mathbb{P}((X -\mu)^2 \leq t^2) \leq \dfrac{\mathbb{E}(X -\mu)^2}{t^2} = \dfrac{\sigma^2}{t^2}.
\]

\vspace{0.3cm}

Para completar el evento $(X - \mu)^2 \geq t^2$ es id\'entico al evento $\vert X - \mu\vert \geq t$, as\'i

\vspace{0.2cm}

\[
\mathbb{P}(\vert X - \mu \vert \leq t) = \mathbb{P}((X -\mu)^2 \leq t^2) \leq \dfrac{\sigma^2}{t^2}.
\]

Para la segunda parte se coloca $t = k\sigma$.

\vspace{0.3cm}

\textbf{Ejemplo} Supongamos que probamos un modelo de predicci\'on sobre un conjunto de $n$  casos de prueba. Sea $X_i = 1$ si el predictor es incorrecto y $X_i = 1$ si el predictor es correcto. Entonces $\overline{X}_n = n^{-1}\displaystyle\sum_{i =1}^{n}X_i$ es la taza de error observado. Cada $X_i$ puede ser considerado una variable de Bernoulli con media desconocida $p$. Intuitivamente $\overline{X}_n$ debe ser cercano a $p$, pero que tan probable es que $\overline{X}_n$ est\'e fuera de un entorno de tama\~no $\epsilon$, sabiendo que $\mathbb{V}(\overline{X}_n) =\mathbb{V}(X_1)/n = p(1 -p)/n$ y


\vspace{0.3cm}


\[
\mathbb{P}(\vert \overline{X}_n -p \vert > \epsilon) \leq \dfrac{\mathbb{V}(\overline{X}_n)}{\epsilon^{2}} = \dfrac{p(1 -p)}{n\epsilon^2} \leq \dfrac{1}{4n\epsilon^2}
\]


\vspace{0.3cm}

desde que $p(1 -p) \leq \frac{1}{4}$ para todo $p$. Para $\epsilon = .2$ y $n = 100$, la cota es $.0625$.

\vspace{0.8cm}

\textbf{\large{Desigualdad de Hoeffding}}


\vspace{0.3cm}

Empezamos con un resultado importante

\vspace{0.5cm}


\textbf{Propiedad 1} Supongamos que $\mathbb{E}(X) = 0$ y que $a \leq x \leq b$. Entonces

\vspace{0.3cm}

\[
\mathbb{E}(e^{tX}) \leq e^{t^2(b -a)^2/8}.
\]

\vspace{0.3cm}

Para desarrollar la desigualdad de  Hoeffding, necesitamos el \texttt{m\'etodo de Chernoff}

\vspace{0.3cm}

\textbf{M\'etodo de Chernoff} Sea $X$ una variable aleatoria. Entonces

\vspace{0.2cm}

\[
\mathbb{P}(X > \epsilon) \leq \inf_{t \geq 0}e^{-t\epsilon}\mathbb{E}(e^{tX}).
\]


\vspace{0.2cm}

Este m\'etodo puede ser derivado de la desigualdad de Markov en la funci\'on $h(x) = e^{tx}$ que la funci\'on generadora de momentos es $\mathbb{E}(e^{tX})$.


\vspace{0.2cm}


Para alg\'un $t > 0$

\vspace{0.2cm}

\[
\mathbb{P}(X >\epsilon) = \mathbb{P}(e^ X > e^{\epsilon}) = \mathbb{P}(e^{tX} > e^{t\epsilon}) \leq e^{-t\epsilon}\mathbb{E}(e^{tX}).
\]

\vspace{0.2cm}

Desde que esto es cierto para $t \geq 0$ el resultado sigue.


\vspace{0.8cm}



Sean $Y_1, Y_2, \dots, Y_n$ observaciones indepedientes, tal que $\mathbb{E}(Y_i)=0 $ y $a_i \leq Y_i \leq b_i$. Sea $\epsilon > 0$. Entonces, para alg\'un $ t > 0$

\vspace{0.2cm}

\[
\mathbb{P}(\vert \overline{Y}_n - \mu \vert \geq \epsilon) \leq 2e^{-2n\epsilon^2/(b -a)^2}.
\]


\vspace{0,3cm}


Sin p\'erdida de generalidad, supongamos $\mu = 0$. Primero se tiene

\vspace{0.3cm}

\begin{align*}
\mathbb{P}(\vert \overline{Y}_n\vert \geq \epsilon) = \mathbb{P}( \overline{Y}_n \geq \epsilon) + \mathbb{P}( \overline{Y}_n\vert \leq -\epsilon) \\
= \mathbb{P}( \overline{Y}_n \geq \epsilon) + \mathbb{P}( -\overline{Y}_n \geq \epsilon)
\end{align*}

\vspace{0.3cm}

Usamos el m\'etodo de chernoff. Para alg\'un $t > 0$, desde la desigualdad de Markov

\vspace{0.2cm}

\begin{align*}
\mathbb{P}( \overline{Y}_n \geq \epsilon) = \mathbb{P}\Bigl(\displaystyle\sum_{i =1}^{n}Y_i \geq n\epsilon\Bigr) = \mathbb{P}(e^{\sum_{i =1}^{n}y_i}\geq e^{n\epsilon})\\
= \mathbb{P} \Bigl(e^{t\sum_{i =1}^{n}y_i}\geq e^{tn\epsilon}\Bigr) \leq e^{-tn\epsilon}\mathbb{E}\Bigl(e^{t\sum_{i =1}^{n}y_i} \Bigr)\\
=e^{-tn\epsilon}\displaystyle\prod_{i}(e^{tY_i}) = e^{-tn\epsilon}(\mathbb{E}(e^{tY_i}))^{n}
\end{align*}


\vspace{0.3cm}

Por la propiedad 1: $\mathbb{E}(e^{tY_i}) \leq e^{t^2(b -a)^2/8}$. As\'i

\vspace{0.3cm}

\[
\mathbb{P}( \overline{Y}_n \geq \epsilon) \leq e^{-tn\epsilon}e^{t^2n(b -a)^2/8}
\]

\vspace{0.3cm}

Esto se reduce al m\'inimo cuando $t = 4\epsilon/(b -a)^2$ dando 

\vspace{0.3cm}

\[
\mathbb{P}( \overline{Y}_n \geq \epsilon) \leq e^{-2n\epsilon^2/(b -a)^2}
\]

\vspace{0.3cm}

Aplicando el mismo argumento a $\mathbb{P}( - \overline{Y}_n \geq \epsilon)$ se produce el resultado.


\vspace{0.8cm}

\textbf{Corolario} Si $X_1, X_2, \cdots, X_n$ son independientes con $\mathbb{P}(a \leq X_i \leq b) = 1$   y una media com\'un $\mu$, entonces con probabilidad de al menos
$1 -\delta$


\vspace{0.3cm}

\[
\vert \overline{X}_n - \mu \vert \leq \sqrt{\dfrac{(b -a)^2}{2n}\log\Bigl(\dfrac{2}{\delta}\Bigr)}.
\]


\vspace{0.3cm}

\textbf{Ejemplo} Sea $X_1, X_2, \dots, X_n \sim \mbox{Bernoulli}(p)$. Desde la desigualdad Hoeffding se tiene

\vspace{0.2cm}

\[
\mathbb{P}(\vert \overline{X}_n -p \vert > \epsilon) \leq 2\epsilon^{-2n\epsilon^2}.
\]

\vspace{0.2cm}

donde $\overline{X}_n = n^{-1}\displaystyle\sum_{i = 1}^{n}X_i$

\vspace{0.5cm}

Sea $X_1, X_2, \dots, X_n \sim \mbox{Bernoulli}(p)$. Sea $n = 100$ y $\epsilon= .2$. Por la desigualdad de Chebyshev, tenemos

\vspace{0.3cm}

\[
\mathbb{P}(\vert \overline{X}_n - p\vert > \epsilon) \leq .0625
\]

\vspace{0.2cm}

De acuerdo a la desigualdad de Hoeffding

\vspace{0.2cm}

\[
\mathbb{P}(\vert \overline{X}_n - p\vert > .2) \leq 2e^{-2(100)(.2)^2} = .00067
\]

\vspace{0.2cm}

que es mucho m\'as peque\~no que $.0625$.


\vspace{0.3cm}

La desigualdad de Hoffding proporciona una manera de crear un \texttt{intervalo de confianza} para un par\'ametro binomial $p$. Sea $\alpha > 0 $ y sea

\vspace{0.3cm}

\[
\epsilon_n = \sqrt{\dfrac{1}{2n}\log\Bigl(\dfrac{2}{\alpha} \Bigr)}
\]

\vspace{0.3cm}

Por la desigualdad de Hofffing:

\[
\mathbb{P}(\vert \overline{X}_n - p \vert > \epsilon_n) \leq 2e^{-2n\epsilon^2_n} = \alpha.
\]

\vspace{0.3cm}

Sea $C = (\overline{X}_n -\epsilon_n, \overline{X}_n + \epsilon_n )$. Entonces $\mathbb{P}(p \notin C) = \mathbb{P}(\vert \overline{X}_n - p\vert > \epsilon_n) \leq \alpha$. As\'i, $\mathbb{P}(p \in C) \geq 1 -\alpha$, esto es, el intervalo aleatorio
$C$ contiene el param\'etro $p$ con probabilidad $1 - \alpha$.

\vspace{0.2cm}

Llamamos a $C$ un \texttt{intervalo de confianza $1 -\alpha$}.


\vspace{0.8cm}

El siguiente resultado extiende la desigualdad de Hoeffding a funciones m\'as generales $g(x_1, \dots, x_n)$. Consideramos la desigualdad de McDiarmind (Bounded Difference inequality).

\vspace{0.5cm}

\textbf{Teorema de McDiarmid} Sea $X_1, X_2, \dots X_n$ variables aleatorias independientes. Suponganse que
\[
\sup_{x_1,\dots, x_n, x^{'}_{i}}\Bigl\vert g(x_1,\dots, x_{i - 1}, x_i, x_{i + 1}, \dots, x_n ) - g(x_1,\dots, x_{i - 1}, x^{'}_i, x_{i + 1}, \dots, x_n )\Bigr\vert \leq c_i
\]

\vspace{0.2cm}

para $i = 1,\dots, n$. Entonces

\vspace{0.2cm}

\[
\mathbb{P}\Bigl(g(X_1,\dots, X_n) -\mathbb{E}(g(X_1,\dots, X_n))\geq \epsilon \Bigr) \leq \exp\Bigl\{-\frac{2\epsilon^2 }{\sum_{i = 1}^{n}c^{2}_i}    \Bigr\}.
\]


\vspace{0.3cm}


\vspace{0.5cm}

Si tomamos $g(x_1,\dots, x_n) = n^{-1}\displaystyle\sum_{i = 1}^{n}x_i$, es la desigualdad de Hoeffding.

\vspace{0.5cm}


\textbf{Ejemplo} Supongamos que lanzamos $m$ pelotas en $n$ recipientes. ?` Qu\'e fracci\'on de los recipientes est\'an vacios?.

\vspace{0.2cm}

Sea $Z$ el n\'umero de recipientes vacios y sea $F = Z/n$ la fracci\'on de recipientes vacios. Podemos escribir $Z = \sum_{i =1}^{n}Z_i$, donde $Z_i = 1$ de un recipiente $i$ est\'a vac\'io y $Z_i = 0$ en otro caso. Entonces

\vspace{0.3cm}

\[
\mu = \mathbb{E}(Z) = \displaystyle\sum_{i =1}^{n}\mathbb{E}(Z_i) = n(1 -1/n)^m = ne^{m\log(1 -1/n)} \approx ne^{-m/n}.
\]

\vspace{0.3cm}

y  $\theta =\mathbb{E}(F) = \mu/n \approx e^{-m/n}$. ?` Cu\'an cerca est\'a Z de $\mu$?. Se debe notar que las variables $Z_i$ no son independientes, por lo que no se puede aplicar la desigualdad de Hoeffding. En este caso procedemos con otro procedimiento

\vspace{0.3cm}


Definamos las variables $X_1, \dots, X_m$ donde $X_s = i$ si la pelota $s$ cae en el recipiente $i$. Entonces $Z = g(X_1,\dots, X_m)$. Si movemos una pelota a un recipiente diferente, entonces $Z$ puede cambiar a lo m\'as $1$. As\'i del Teorema de McDiarmind con $c_i = 1$ tenemos

\vspace{0.3cm}

\[
\mathbb{P}(\vert Z - \mu\vert  > t) \leq 2e^{-2t^2/m}.
\]

\vspace{0.3cm}

Como las fracci\'on de recipientes vacios es $F=Z/m$ con media $\theta = \mu/n$. Tenemos

\vspace{0.3cm}

\[
\mathbb{P}(\vert F - \theta\vert > t) = \mathbb{P}(\vert Z - \mu\vert> nt) \leq 2e^{-2n^2t^2/m}.
\]

\vspace{0.8cm}


\textbf{\large{Cotas para los valores esperados}}

\vspace{0.5cm}


\textbf{Desigualdad de Cauchy -Schwartz} Si $X$ y $Y$ tienen varianza finita, entonces

\[
\mathbb{E}\vert XY\vert \leq \sqrt{\mathbb{E}(X^2)\mathbb{E}(Y^2)}.
\]

\vspace{0.3cm}


La desigualdad de Cauchy-Schwarz puede ser escrita como

\[
\mbox{Cov}^2(X,Y) \leq \delta_{X}^{2}\delta_{Y}^{2}.
\]

\textbf{Desigualdad de Jensen} Si $g$ es convexa, entonces
\[
\mathbb{E}g(X) \geq g(\mathbb{E}X)
\]

Si $g$ es c\'oncava

\[
\mathbb{E}g(X) \leq g(\mathbb{E}X).
\]


\vspace{0.3cm}

En efecto, sea $L(x)= a +bx$ una l\'inea tangente a $g(x)$ en el punto $\mathbb{E}(X)$. Desde que $g$ es convexa, est\'a por encima de la l\'inea $L(X)$. As\'i

\vspace{0.3cm}

\[
\mathbb{E}g(X) \geq \mathbb{E}L(X) = \mathbb{E}(a +bX) = a + b\mathbb{E}(X) = L(\mathbb{E}(X)) = g(\mathbb{E}X).
\]

\vspace{0.2cm}

De la desigualdad de Jensen, se cumple $\mathbb{E}(X^2) \geq (\mathbb{E}X)^2$ y si $X$ es positiva, entonces $\mathbb{E}(1/X) \geq 1/\mathbb{E}(X)$. Desde que $\log$ es c\'oncava, $\mathbb{E}(\log X) \leq \log\mathbb{E}(X)$.

\vspace{0.5cm}


\textbf{Distancia de Kullback-Leibler} 

\vspace{0.3cm}

Definamos la distancia de \textbf{Kullback-Leibler} entre dos densidades $f_1$ y $f_2$ dada

\vspace{0.3cm}

\[
D(f_1, f_2) = \bigintsss{f_1(x)\log\Bigl(\frac{f_1(x)}{f_2(x)}\Bigr)dx}
\]

\vspace{0.3cm}

Nota que $D(f_1,f_1) = 0$ y usando la desigualdad de Jensen se cumple que  $D(f_1, f_2) \geq 0$. En efecto sea $X \sim f_1$. Entonces

\vspace{0.2cm}


\[
-D(f_1, f_2) = \mathbb{E}\log\Bigl(\dfrac{f_1(X)}{f_2(X)} \Bigr) \leq \log\mathbb{E}\Bigl(\dfrac{f_1(X)}{f_2(X)} \Bigr) = \log\bigintsss f_1(x)\dfrac{f_2(x)}{f_1(x)}dx = \log\bigintsss f_2(x)dx = \log(1) = 0
\]

\vspace{0.2cm}

As\'i $D(f_1, f_2) \geq 0$, es decir $D(f_1, f_2) \geq 0$.

\vspace{0.2cm}

La distancia de Kullbacker-Leibler es una medida de la informaci\'on que se pierde cuando $f_1$ se aproxima a $f_2$. Un ejemplo en R.

\vspace{0.2cm}

<<fooq, comment = NA, prompt =TRUE, eval= FALSE>>=
set.seed(1000)
X<- rexp(10000, rate=0.2)
Y<- rexp(10000, rate=0.4)
KL.dist(X, Y, k=5)
KLx.dist(X, Y, k=5)
@

\vspace{0.2cm}

El anterior script usa el paquete \textbf{FNN } \textit{Fast Nearest Neighbor Search Algorithms and Applications} de $R$. 


\vspace{0.5cm}


La distancia de Kullbacker-Leibler entre una distribuci\'on Gaussiana $f_1$ con media $\mu_1$ y varianza $\sigma^2_1$ y una distribuci\'on Gaussiana $f_2$ con media $\mu_2$ y varianza $\sigma^2_2$ es dado por

\vspace{0.3cm}


\[
D(f_1,f_2) = \log \frac{\sigma_2}{\sigma_1} + \frac{\sigma^2_1 + (\mu_1 - \mu_2)^2}{2\sigma^2_2} - \frac{1}{2}.
\]


\vspace{0.3cm}

Si tenemos una cota exponencial sobre $\mathbb{P}(X_n > \epsilon$. En este caso, podemos acotar $\mathbb{E}(X_n)$ como sigue

\vspace{0.5cm}


\textbf{Teorema} Supongamos que $X_n \geq 0$ y para cada $\epsilon >0$,

\vspace{0.3cm}

\[
\mathbb{P}(X_n > \epsilon) \leq c_1e^{-c_2n\epsilon^2}
\]

\vspace{0.3cm}

para alg\'un $c_2 >0$ y $c_1 > 1/e$. Entonces

\vspace{0.3cm}

\[
\mathbb{E}(X_n) \leq \sqrt{\dfrac{C}{n}}.
\]


\vspace{0.3cm}

donde $C = (1 + \log(c_1))/c_2$

\vspace{0.3cm}

Ahora consideramos acotar el m\'aximo de un conjunto  de variables aleatorios

\vspace{0.5cm}


\textbf{Teorema} Sean $X_1,\dots, X_n$ variables aleatorias. Supongamos que existe un $\delta >0$ tal que $\mathbb{E}(e^{tX_i}) \leq e^{t^2\delta^2/2}$ para todo $t > 0$. Entonces 

\vspace{0.3cm}

\[
\mathbb{E}\Bigl( \max_{1 \leq i \leq n}X_i \Bigr) \leq \delta \sqrt{2 \log n}.
\]


\vspace{0.3cm}

En efecto por la desigualdad de Jensen,

\vspace{0.3cm}

\begin{align*}
\exp\Bigl\{t\mathbb{E}\Bigl(\max_{1 \leq i \leq n}X_i \Bigr)\Bigr\} &\leq \mathbb{E}\Bigl(\exp\Bigl\{t\max_{1 \leq i \leq n} X_i\Bigr\}\Bigr)\\
&= \mathbb{E}\Bigl(\max_{1 \leq i \leq n}\exp\{tX_i\}\Bigr) \leq \displaystyle\sum_{i =1}^{n}\mathbb{E}(\exp\{tX_i \}) \leq ne^{t^2\sigma^2/2}.
\end{align*}


\vspace{0.3cm}

As\'i,

\vspace{0.3cm}

\[
\mathbb{E}\Bigl(\max_{1 \leq i \leq n}X_i \Bigr)\leq \dfrac{\log n}{t} + \dfrac{t\sigma^2}{2}
\]

\vspace{0.3cm}

El resultado se sigue poniendo $t = \sqrt{2\log n}/\sigma$.


\vspace{0.5cm}

La siguiente desigualdad es \'util para cotas de probabilidad para variables aleatorias normales

\vspace{0.3cm}

\textbf{Teorema} Sea $Z \sim N(0,1)$. Entonces

\vspace{0.2cm}

\[
\mathbb{P}(\vert Z \vert > t) \leq \sqrt{\dfrac{2}{\pi}}\dfrac{e^{-t^2/2}}{t}
\]


\vspace{0.8cm}


En Estad\'istica, Probabilidades y Machine Learning, usamos la notaci\'on $o_{P}$ y $O_{P}$. Por ejemplo, $a_n = o(1)$ significa que $a_n \rightarrow 0$ cuando $n \rightarrow \infty$. $a_n = o(b_n)$ significa $a_n/b_n = o(1)$.


\vspace{0.3cm}

$a_n = O(1)$ (eventualmente acotados) significa que para un $n$ una cantidad muy grande, $\vert a_n \vert \leq C$ para alg\'un $C >0$. $a_n = O(b_n)$, significa que $a_n/b_n = O(1)$.

\vspace{0.3cm}

Escribimos $a_n \sim b_n$ si ambos $a/b$ y  $b/a$ son eventualmente acotados. En ciencia de la computaci\'on esto se escribe $a_n = \Theta(b_n)$.

\vspace{0.3cm}

Veamos estas cosas en el terreno de las probabilidades, es decir $Y_n = o_P(1)$ significa que para un  $\epsilon > 0$,

\vspace{0.3cm}

\[
\mathbb{P}(\vert Y_n \vert > \epsilon ) \rightarrow 0.
\]

\vspace{0.3cm}

Decimos que $Y_n = o_P(a_n)$ si, $Y_n/a_n = o_{P}(1)$. 

\vspace{0.3cm}

Si tenemos $Y_n = O_P(1)$ si, para cada $\epsilon > 0$, existe un $C > 0$ tal que

\vspace{0.3cm}

\[
\mathbb{P}(\vert Y_n \vert > C) \leq \epsilon.
\]

\vspace{0.3cm}

Se dice que $Y_n = O_P(a_n)$ si $Y_n/a_n = O_p(1)$. 

\vspace{0.3cm}

Usando la desigualdad Hoeffding, para mostrar que las proporciones muestrales son $O_{p}(1/\sqrt{n})$ alrededor de la media.  Sea $Y_1, Y_2, \dots, Y_n$ lanzamientos de monedas, esto es $Y_i \in \{0,1 \}$. Sea $p = \mathbb{P}(Y_i = 1)$. Sea 

\vspace{0.3cm}

\[
\hat{p}_n = \frac{1}{n}\sum_{i = 1}^{n}Y_i
\]

\vspace{0.3cm}

Mostremos que : $\hat{p}_n - p = o_P(1)$ y   $\hat{p}_n - p = O_P(1/\sqrt{n})$.

\vspace{0.5cm}

En efecto tenemos que

\vspace{0.3cm}

\[
\mathbb{P}(\vert \hat{p}_n - p\vert > \epsilon) \leq 2e^{-2n\epsilon^2}\rightarrow 0
\]

\vspace{0.3cm}

y as\'i $\hat{p}_n - p = o_P(1)$. Tambi\'en

\vspace{0.3cm}

\begin{align*}
\mathbb{P}(\sqrt{n}(\hat{p}_n - p) > C) &= \mathbb{P}\Bigl(\vert \hat{p}_n - p \vert > \dfrac{C}{\sqrt{n}}\Bigr)\\
& \leq 2e^{-2C^2} < \epsilon 
\end{align*}

\vspace{0.3cm}

si escogemos un $C$ grande. As\'i $\sqrt{n}(\hat{p}_{n} - p) = O_{P}(1)$ produciendo luego que

\vspace{0.3cm}

\[
\hat{p}_{n} - p = O_p\Bigl(\dfrac{1}{\sqrt{n}} \Bigr)
\]
\vspace{0.8cm}


{\Large{La ley de promedios}}

\vspace{0.5cm}

Si lanzamos un dado $5$ millones de veces y se mantiene un registro de los datos. El promedio de los n\'umeros  los cuales fueron lanzados es $3.500867$. Desde que la media de cada lanzamiento es $\frac{1}{6}(1 + 2 + \cdots +6) = 3\frac{1}{2}$, este resultado muestra  que para un $x_i$ el $i$ \'esimo lanzamiento, el promedio

\vspace{0.3cm}

\[
a_n = \dfrac{1}{n}(x_i + x_2 + \cdots + x_n)
\]

\vspace{0.3cm}

se acerca a la media $3\dfrac{1}{2}$, cuando $n \rightarrow \infty$.

\vspace{0.2cm}

En general si tenemos una secuencia $X_1, X_2, \dots$ de variables aleatorias identicamente distribuidas e independientes cada una teniendo una media $\mu$, deberiamos probar que el promedio

\vspace{0.3cm}

\[
\dfrac{1}{n}(X_1 + X_2 +\cdots +X_n)
\]

\vspace{0.2cm}

converge cuando $n \rightarrow \infty$ a la media $\mu$.

\vspace{0.5cm}

\textbf{Definici\'on} Decimos que la secuencia $x_1, x_2, \dots$ de variables aleatorias \texttt{converge en media  cuadrada a la variables x} si

\vspace{0.3cm}

\[
\mathbb{E}([X_n - x]^2)\rightarrow 0 \ \ \text{cuando}\ \ n\rightarrow \infty
\]

\vspace{0.2cm}

y se escribe \texttt{$X_n \rightarrow x$ en media cuadrada cuando $n \rightarrow \infty$}.

\vspace{0.5cm}


\textbf{Ejemplo} Sea $X_n$ una secuencia de variables aleatorias discretas con un \texttt{pmf}

\vspace{0.2cm}

\[
\mathbb{P}(X_n = 1) =  \dfrac{1}{n},  \ \ \ \ \mathbb{P}(X_n = 2) = 1 - \dfrac{1}{n}.
\]

\vspace{0.2cm}

Entonces $X_n$ converge a la variable aleatoria $2$ en media cuadrada cuando $n \rightarrow \infty$, desde que

\vspace{0.3cm}

\begin{align*}
\mathbb{E}([X_n - 2]^2) = (1 -2)\dfrac{1}{n} + (2 -2)^2\Bigl(1 - \dfrac{1}{n}\Bigr)\\
= \dfrac{1}{n} \rightarrow 0 \ \ \ \text{cuando}\ \ n \rightarrow \infty.
\end{align*}


\vspace{0.8cm}


\textbf{Ley de los grandes n\'umeros(1)} Sea una secuencia $X_1, X_2, \dots $ una secuencia de variables independientes cada una con media $\mu$ y varianza $\sigma^2$. El promedio de los primeros $n$ de las $X_i$ satisfacen cuando $n \rightarrow \infty$.

\vspace{0.3cm}

\[
\dfrac{1}{n}(X_1 +X_2 + \cdots + X_n) \rightarrow \mu \ \ \ \text{en media cuadrada}.
\]


\vspace{0.5cm}


Sea $S_n = X_1 + X_2 + \cdots X_n$, la n-\'esima suma parcial de los $X_i$. Entonces

\vspace{0.3cm}

\[
\mathbb{E}\Bigl(\dfrac{1}{n}S_n\Bigr) = \dfrac{1}{n}\mathbb{E}(X_1 +X_2 + \cdots + X_n) = \dfrac{1}{n}n\mu = \mu.
\]

\vspace{0.3cm}

y as\'i

\vspace{0.3cm}

\begin{align*}
\mathbb{E}\Bigl(\Bigl[\dfrac{1}{n}S_n - \mu \Bigr]^2\Bigr) &= \mathbb{V}\Bigl(\dfrac{1}{n}S_n\Bigr)\\
  &= \dfrac{1}{n^2}\mathbb{V}(X_1 +X_2 + \cdots + X_n)\\
  &= \dfrac{1}{n^2}(\mathbb{V}X_1 + \cdots  + \mathbb{V}X_n)\\
  &=\dfrac{1}n{n^2}\sigma^2 = \dfrac{1}{n}\sigma^2 \rightarrow 0 \ \ \ \text{cuando}\ \ n \rightarrow \infty
\end{align*}

\vspace{0.3cm}

y as\'i, $n^{-1}S_n \rightarrow \mu$ en media cuadr\'atica cuando $n \rightarrow \infty$.

\vspace{0.5cm}

\textbf{Definici\'on} Decimos que la secuencia de variables aleatoria $X_1, X_2, \dots$ de variables aleatorias \texttt{converge en probabilidad } a $X$ si

\vspace{0.3cm}

\[
\texttt{Para todo} \ \ \epsilon >0 \ \ \ \mathbb{P}(\vert X_n - X\vert > \epsilon) \rightarrow \ \ \text{cuando}\ \ n \rightarrow \infty
\]

\vspace{0.3cm}

Si esto se cumple, escribimos \texttt{$X_n \rightarrow X$ en probabilidad cuando $n \rightarrow \infty$ }

\vspace{0.5cm}


\textbf{Teorema } Si $X_1, X_2, \dots $ es una secuencia de variables aleatorias y $X_n \rightarrow X$ en media cuadrada cuando $n \rightarrow \infty$ entonces $X_n \rightarrow X$ en probabilidad.


\vspace{0.5cm} 

Para hacer la prueba de este teorema, escribamos la \texttt{la desigualdad de Chebyshev} de la siguiente manera

\vspace{0.3cm}

Si $Y$ es una variable aleatoria y $\mathbb{E}(Y^2) < \infty$, entonces

\vspace{0.2cm}

\[
\mathbb{P}(\vert Y \vert \geq t) \leq \dfrac{1}{t^2}\mathbb{E}(Y^2)\ \ \ \text{para}\ \ \ t >0
\]


\vspace{0.5cm}

Luego aplicando la desigualdad de Chebyshev a la variable aleatoria $Y = X_n - X$ para encontrar que 

\vspace{0.3cm}

\[
\mathbb{P}(\vert X_n -X\vert > \epsilon) \leq \dfrac{1}{\epsilon^2}\mathbb{E}([X_n -X]^2)\ \ \ \epsilon > 0.
\]

\vspace{0.3cm}

Si $X_n \rightarrow X$ en media cuadrada cuando $n \rightarrow \infty$, el lado derecho de la desigualdad tiende a $0$ cuando $n \rightarrow \infty$ y as\'i tiende a $0$ para todo $\epsilon > 0$, como es requerido. El caso contrario no se cumple.

\vspace{0.8cm}

\textbf{Ley d\'ebil de los grandes n\'umeros} Sea $X_1, X_2, \dots$ una secuencia de variables aleatorias, cada una con media $\mu$ y varianza $\sigma^2$. El promedio de los primeros $n$ de los $X_i$ satisface, cuando $n \rightarrow \infty$

\vspace{0.3cm}

\[
\dfrac{1}{n}(X_1 +X_2 + \cdots + X_n) \rightarrow \mu \ \ \ \text{en probabilidad}.
\]

\vspace{0.5cm}


\textbf{Ejemplo} Sea $X_n$ una secuencia de variables aleatorias que convergen en probabilidad, pero  no en media cuadrada con un \texttt{pmf}


\vspace{0.3cm}

\[
\mathbb{P}(X_n = 0) =  1 - \dfrac{1}{n},  \ \ \ \ \mathbb{P}(X_n = n) = \dfrac{1}{n}.
\]


\vspace{0.2cm}

Entonces, para $\epsilon > 0$ y para $n$ dado 

\vspace{0.2cm}


\[
\mathbb{P}(\vert X_n\vert > \epsilon) = \mathbb{P}(X_n = n) = \dfrac{1}{n} \rightarrow 0 \ \ \ \text{cuando}\ \ n \rightarrow \infty
\]

\vspace{0.2cm}

dando que $X_n \rightarrow \infty $ en probabilidad. Por otro lado 

\vspace{0.2cm}

\begin{align*}
\mathbb{E}([X_n - 0]^{2}) = \mathbb{E}(X_n^2) = 0\Bigl(1 -\dfrac{1}{n}\Bigr) + n^2\dfrac{1}{n}\\
= n \rightarrow \infty \ \  \text{cuando} \ \ n \rightarrow \infty.
\end{align*}

\vspace{0.2cm}

as\'i $X_n$ no converge a $0$ en media cuadrada.

\vspace{0.5cm}


\textbf{Ejemplo} Considere el lanzamiento de  una moneda donde la probabilidad de obtener cara es $p$. Sea $X_i$  el resultado de un lanzamiento $(0-1)$. Por lo tanto, $p = \mathbb{P}(X_i = 1) = \mathbb{E}(X_i)$. La fracci\'on de caras  despu\'es de $n$ lanzamientos  es  $\overline{X}_n$ . De acuerdo con la ley de los grandes n\'umeros, $\overline{X}_n$ converge en probabilidad a $p$. Esto no quiere decir que $\overline{X}_n$ se num\'ericamente igual $p$. Esto significa que, cuando $n$ es grande, la distribuci\'on de $\overline{X}_n$ est\'a  'concentrado' alrededor $p$.

\vspace{0.3cm}

Supongamos que $p = 1/2$, encontremos el valor de $n$ para que $\mathbb{P}(.4\leq \overline{X}_n \leq .6) \geq .7$. Para ello tenemos que $\mathbb{P}(\overline{X}_n) = p = 1/2$ y $\mathbb{V}(\overline{X}_n) = \sigma^2/n = p(1 -p)/n = 1/n$. Por la desigualdad de Chebyshev

\vspace{0.3cm}

\begin{align*}
\mathbb{P}(.4\leq \overline{X}_n \leq .6) = \mathbb{P}(\vert \overline{X}_n - \mu) \leq .1)\\
= 1 - \mathbb{P}(\vert \overline{X}_n - \mu\vert > .1)\\
\geq 1 - \dfrac{1}{4n(.1)^2} = 1 - \frac{25}{n}
\end{align*}

\vspace{0.3cm}

Esta expresi\'on es mayor que $.7$ si $n = 84$.
\vspace{0.8cm}

\textbf{Teorema del l\'imite central} Sea $X_1, X_2, \dots $ variables aleatorias identicamente distribuidas e independientes cada una con media $\mu$ y varianza distinta de cero $\sigma^2$. Por la ley de los grandes n\'umeros, la suma $S_n = X_1 +X_2+ \cdots +X_n$ es casi tan grande que $n\mu$ para valores de $n$ muy grandes. Un siguiente paso es determinar el orden de la diferencia $S_n - n\mu$, que resulta ser de orden $\sqrt{n}$.

\vspace{0.2cm}

Se define la versi\'on est\'andar de $S_n$

\vspace{0.2cm}

\[
Z_n = \dfrac{S_n -\mathbb{E}(S_n)}{\sqrt{\mathbb{V}(S_n)}}
\]

\vspace{0.2cm}

Esta es una funci\'on lineal $Z_n = a_nS_n + b_n$ de $S_n$, donde $a_n$ y $b_n$ tienen que ser elegida de manera que $E(Z_n) = 0$ y $var(Z_n) = 1$. Adem\'as

\vspace{0.2cm}

\begin{align*}
\mathbb{E}(S_n ) &= \mathbb{E}(X_1) + \mathbb{E}(X_2) + \cdots + \mathbb{E}(X_n )\\
&= n\mu
\end{align*}



Tambi\'en 

\vspace{0.2cm}

\begin{align*}
\mathbb{V}(S_n) &= \mathbb{V}(X_1) + \mathbb{V}(X_2) + \cdots +\mathbb{V}(X_n)\\
&= n\sigma^2
\end{align*}


y as\'i

\vspace{0.2cm}

\[
Z_n = \dfrac{S_n - n\mu}{\sigma\sqrt{n}}
\]


Muchas de las propiedades de $Z_n$ se pueden sintetizar en el teorema de l\'imite central:

\vspace{0.3cm}

Sean $X_1, X_2, \dots $ variables aleatorias identicamente distribuidas e independiente, cada una con media $\mu$ y varianza distinta de cero $\sigma^2$. La vesrsi\'on est\'andar 

\vspace{0.2cm}

\[
Z_n = \dfrac{S_n - n\mu}{\sigma\sqrt{n}}
\]

\vspace{0.2cm}

de la suma $S_n = X_1 +X_2 + \cdots + X_n$, satisface cuando $n \rightarrow \infty $.

\vspace{0.3cm}

\[
\mathbb{P}(Z_n \leq x) \rightarrow \bigintss_{-\infty}^{x}\dfrac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}u^2}du = \Phi(x)\ \ \ \text{para}\ \ x\in \mathbb{R}.
\]

\vspace{0.2cm}

El lado derecho de la \'ultima de la ecuaci\'on es s\'olo la funci\'on de distribuci\'on de la distribuci\'on normal con media $0$ y varianza $1$, as\'i que esta expresi\'on se puede escribir como

\vspace{0.2cm}

\[
\mathbb{P}(Z_n \leq x) \rightarrow \mathbb{P}(Y \leq x) \ \ \ \text{para}\ \ x \in \mathbb{R}.
\]


\vspace{0.3cm}


donde $Y$ es la variable aleatoria con esta distribuci\'on normal est\'andar. Tambi\'en podemos  escribir el teorema  de la siguiente forma 


\vspace{0.3cm}


\[
\lim_{n \rightarrow \infty}\mathbb{P}(Z_n \leq x) = \Phi(x)\ \ \text{para cada $x$}.
\]

\vspace{0.2cm}


El teorema del l\'imite central, nos permite calcular probabilidades relacionadas a $Z_n$ como si $Z_n$ fuese normal. Desde que la normalidad es preservada bajo transformaciones lineales, esto es equivalente  a tratar $S_n$ como una distribuci\'on normal con media $n\mu$ y varianza $n\sigma^2$. 

\vspace{0.3cm}


Sea $S_n = X_1 +X_2+ \cdots +X_n$ donde las $X_i$ son variables aleatorias identicamente distribuidas e independientes cada una con media $\mu$ y varianza distinta de cero $\sigma^2$. Si $n$ es muy grande, la probabilidad $\mathbb{P}(S_n \leq c)$ puede ser aproximado considerando $S_n$ como si fuese normal de acuerdo  al siguiente procedimiento

\vspace{0.2cm}

\begin{itemize}
\item Calculamos la media $n\mu$ y la varianza $n\sigma^2$ de $S_n$.
\item Calculamos el valor normalizado $z = (c - n\mu)/\sigma\sqrt{n}$.
\item Usamos la aproximaci\'on 

\[
\mathbb{P}(S_n \leq c) \approx \Phi(z)
\]

\vspace{0.2cm}

donde $\Phi(z)$ es de disponible desde la tabla del \texttt{cdf} de la distribuci\'on est\'andar. 
\end{itemize}


\vspace{0.5cm}


\textbf{Ejemplo} Cargamos en un avi\'on $100$ paquetes cuyos pesos son  variables aleatorias independientes  que se distribuyen uniformemente entre $5$ y $50$ libras. ?`Cu\'al es la probabilidad de que el peso total es superior a 3000 libras?.

\vspace{0.2cm}

No es f\'acil calcular el \texttt{cdf} del peso total y la probabilidad deseada, pero una  respuesta aproximada se puede obtener usando el teorema del l\'imite central.

\vspace{0.3cm}

Para calcular $\mathbb{P}(S_100 > 3000)$, donde $S_100$ es la suma de los pesos de los $100$ paquetes. La media y la varianza de los pesis de un \'unico paquete es 

\vspace{0.3cm}

\[
\mu = \dfrac{5 + 50}{2} = 27.5 \ \ \ \ \sigma^{2} = \dfrac{(50 -5)^2}{12} = 168.75
\]


\vspace{0.3cm}

basados en la f\'ormulas para la media y la varianza del \texttt{pdf} uniforme. Calculemos el valor normalizado 

\vspace{0.3cm}

\[
z = \dfrac{3000 - 100\cdot27.5}{\sqrt{168.75 -100}} = \dfrac{250}{129.9} = 1.92
\]

\vspace{0.3cm}

y usamos las tablas normal est\'andar para obtener la aproximaci\'on

\vspace{0.3cm}

\[
\mathbb{P}(S_100 \leq 3000) \approx \Phi(1.92) = 0.9726
\]

\vspace{0.3cm}

y as\'i la probabilidad pedida es

\vspace{0.3cm}

\[
\mathbb{P}(S_100 > 3000) = 1 - \mathbb{P}(S_100 \leq 3000) \approx 1 - 0.97626 = 0.0274
\]
\vspace{0.5cm}


Para la prueba(parcial) de este teorema usaremos el m\'etodo de las funciones generadoras de momentos

\vspace{0.5cm}

\textbf{Teorema de continuidad} Sea $X_1, X_2, \dots$ una secuencia de variables aleatorias con funciones generadores de momentos $M_1, M_2, \dots$ y suponiendo que para $n \rightarrow \infty$

\vspace{0.3cm}

\[
\mathbb{M}_n(t) \rightarrow e^{\frac{1}{2}t^2}\ \  \text{para } \ \ t\in \mathbb{R}
\]

\vspace{0.2cm}

Entonces

\vspace{0.2cm}

\[
\mathbb{P}(Z_n \leq x) \rightarrow \bigintss_{-\infty}^{x}\dfrac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}u^2}du \ \ \ \text{para}\ \ x\in \mathbb{R}.
\]

\vspace{0.3cm}

En otras palabras, la funci\'on distribuci\'on de $Z_n$ converge a la funci\'on distribuci\'on de la distribuci\'on  normal si la funci\'on generadora de momentos de $Z_n$ converge a la funci\'on generadora de momentos de la distribuci\'on normal. 

\vspace{0.2cm}

Usamos estas propiedades  para probar el \texttt{teorema del l\'imite central} en el caso que las $X_i$ tienen una funci\'on generadora de momentos

\vspace{0.3cm}


\[
\mathbb{M}_{X}(t) = \mathbb{E}(\exp(tX_i))\ \ \ \text{para}\ \  i=1,2 \dots
\]

\vspace{0.2cm}


haciendo hincapi\'e en que el teorema del l\'imite central es v\'alida incluso cuando esta esperanza no existe, siempre que la media y la varianza de la $X_i$ sean finitas.

\vspace{0.3cm}

Sea $U_i = X_i - \mu$. Entonces $U_1, U_2, \dots$ son variables aleatorias id\'enticamente distribuidas e independientes con media y varianza dado por

\vspace{0.2cm}

\[
\mathbb{E}(U_i) = 0, \ \ \ \ \mathbb{E}(U_i^2) = \mathbb{V}(U_i) = \sigma^2,
\]

\vspace{0.2cm}

y funci\'on generadora de momentos

\vspace{0.2cm}

\[
\mathbb{M}_{U}(t) = \mathbb{M}_{X}(t)e^{-\mu t}
\]

\vspace{0.2cm}

Ahora 

\vspace{0.2cm}

\[
Z_n = \dfrac{S_n - n\mu}{\sigma \sqrt{n}} = \dfrac{1}{\sigma\sqrt{n}}\displaystyle\sum_{i = 1}^{n}U_i
\]

\vspace{0.2cm}

La funci\'on generadora de momentos de $Z_n$

\vspace{0.2cm}

\begin{align*}
\mathbb{M}_n(t) = \mathbb{E}(\exp(tZ_n)) &= \mathbb{E}\Bigl(\exp \Bigl(\dfrac{t}{\sigma\sqrt{n}}\displaystyle\sum_{i = 1}^{n}U_i\Bigr) \Bigr)\\
&= \Bigl[\mathbb{M}_{U}\Bigl( \dfrac{1}{\sigma\sqrt{n}}\Bigr) \Bigr]^{n}
\end{align*}

\vspace{0.3cm}

Expandamos $\mathbb{M}_U(x)$ como una serie de potencia alrededor de $x = 0$, para conocer el comportamiento de $\mathbb{M}_U(t/(\sigma\sqrt{n})$ para $n \rightarrow \infty$.

\vspace{0.3cm}

\begin{align*}
\mathbb{M}_U(x) &= 1 + x\mathbb{E}(U_1) +\frac{1}{2}x^2\mathbb{E}(U_1^2) + o(x^2)\\
 &= 1 + \frac{1}{2}\sigma^2x^2 + o(x^2)
\end{align*}

\vspace{0.3cm}

Reemplazando este resultado con $x = t/(\sigma\sqrt{n}$ y $t$ fijo

\vspace{0.3cm}

\[
\mathbb{M}_n(t) = \Bigl[1 + \dfrac{t^2}{2n} +o\Bigl(\dfrac{1}{n} \Bigr) \Bigr]^{n} \rightarrow e^{\frac{1}{2}t^2} \ \ \ \text{cuando}\ \ n\rightarrow \infty .
\]

\vspace{0.3cm}

y por el teorema de continuidad tenemos el resultado del teorema del l\'imite central. La prueba requiere la existencia de $\mathbb{M}_X(t)$ para valores de $t$ cerca de $0$.

\vspace{0.5cm}

\textbf{Muestreo estad\'istico} Una fracci\'on $p$ desconocida de la poblaci\'on son caballeros jedi. Se desea estimar $p$ con un error no superior a $0.005$ sobre   una muestra de individuos (se supone que contestan con la verdad). ?`Qu\'e tan grande es esa muestra?

\vspace{0.3cm}

Supongamos que se elige una muestra de $N$ individuos. Sea $X_i$ la funci\'on indicador del evento de que la i-\'esima persona admite ser un caballero jedi, y suponiendo  que las $X_i$ son variables aleatorias independientes, de Bernoulli con par\'ametro $p$, escribimos entonces

\vspace{0.3cm}

\[
S_n = \displaystyle\sum_{i = 1}^{n}X_i
\]


\vspace{0.3cm}

Elegimos para estimar $p$ con la media muestral $n^{-1}S_n$,  al siguiente operador $\hat{p}$ seg\'un la notaci\'on estad\'istica.

\vspace{0.3cm}

Deseamos elegir $n$ suficientemente grande para que $\vert \hat{p} - p \vert \leq 0,005$. Esto no se puede hacer, ya que $\vert \hat{p} - p \vert$ es una variable aleatoria que puede (aunque con una peque\~na probabilidad) tomar un valor mayor que $0.005$ para un $n$ dado. El enfoque aceptado es establecer un nivel m\'aximo de la probabilidad en la que se permite la ocurrencia de un error. Por convenci\'on, tomamos este como  $0.05$ que nos lleva al siguiente problema: encontrar un $n$ tal que

\vspace{0.3cm}

\[
\mathbb{P}(\vert \hat{p} - p \vert \leq 0.005) \geq 0.95
\]

\vspace{0.3cm}

Como $S_n$ es la suma de variables aleatorias id\'enticamente distribuidas con media $p$ y varianza $p(1 -p)$. La probabilidad de arriba puede ser escrita como

\vspace{0.3cm}

\begin{align*}
\mathbb{P}\Bigl(\Bigl\vert \dfrac{S_n}{n} - p\Bigr\vert  \leq 0.005\Bigr) &= \mathbb{P}\Bigl( \dfrac{\vert S_n - np\vert}{\sqrt{np(1 -p)}} \leq 0.005\sqrt{\dfrac{n}{p(1 - p)}}\Bigr)\\
&= \mathbb{P}\Bigl( \dfrac{\vert S_n - np\vert}{\sqrt{\mathbb{V}(S_n)}} \leq 0.005\sqrt{\dfrac{n}{p(1 - p)}}\Bigr)
\end{align*}

\vspace{0.3cm}

Por el teorema del l\'imite central, $(S_n - \mathbb{E}(S_n))/\sqrt{\mathbb{V}(s_n)}$ converge  a la distribuci\'on normal y as\'i la probabilidad final puede ser aproximada por una integral de funci\'on densidad normal. Un inconveniente  a este resultado es que el rango de esta integral depende del valor de $p$ que es desconocido.

\vspace{0.3cm}

Desde que $p(1 -p) \leq \frac{1}{4}$ para $p \in [0,1]$

\vspace{0.3cm}

\[
\mathbb{P}\Bigl( \dfrac{\vert S_n - \mathbb{E}(S_n)\vert}{\sqrt{\mathbb{V}(S_n)}} \leq 0.005\sqrt{\dfrac{n}{p(1 - p)}}\Bigr)\geq \mathbb{P}\Bigl( \dfrac{\vert S_n - \mathbb{E}(S_n)\vert}{\sqrt{\mathbb{V}(S_n)}} \leq 0.005\sqrt{4n}\Bigr)
\]

\vspace{0.3cm}

y el lado derecho es aproximadamente $\mathbb{P}(\vert N \vert \leq 0.005\sqrt{4n}$, donde $N$ es normal con media $0$ y varianza $1$. Por tanto

\vspace{0.3cm}

\begin{align*}
\mathbb{P}(\vert \hat{p} - p \vert \leq 0.005) &\gtrapprox \bigintsss_{-0.005\sqrt{4n}}^{0.005\sqrt{4n}}\dfrac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}u^2}du \\
& = 2\Phi(0.005\sqrt{4n}) -1
\end{align*}

\vspace{0.3cm}

donde $\Phi$ es la funci\'on de distribuci\'on de $N$. Consultando a las tablas, encontramos que es mayor que $0.95$ si $0.005\sqrt{4n}\geq 1.96$, es decir cuando $n \gtrapprox 40.000$.


\vspace{0.5cm}

En el caso multivariarado el teorema del l\'imite central, se escribe de la siguiente manera: Para $X_1, \dots X_n$ vectores aleatorios id\'enticamente distribuidos independientes, donde 

\vspace{0.3cm}

\[
X_i = \begin{pmatrix}
X_{1i}\\
X_{2i}\\
\vdots \\
X{ki}
\end{pmatrix}
\]

\vspace{0.2cm}

con media 

\vspace{0.2cm}

\[
\mu = \begin{pmatrix}
\mu_{1}\\
\mu_{2}\\
\vdots \\
\mu_{k}
\end{pmatrix}
 = \begin{pmatrix}
\mathbb{E}(X_{1i})\\
\mathbb{E}(X_{2i})\\
\vdots\\
\mathbb{E}(X_{ki})
\end{pmatrix}
\]

\vspace{0.3cm}

y matriz varianza $\Sigma$. Sea 

\vspace{0.3cm}

\[
\overline{X} = \begin{pmatrix}
\overline{X}_1\\
\overline{X}_2\\
\vdots \\
\overline{X}_{k}
\end{pmatrix}
\]

\vspace{0.3cm}

donde $\overline{X}_j = n^{-1}\displaystyle\sum_{i =1}^{n}X_{ji}$. Entonces

\vspace{0.3cm}

\[
\sqrt{n}(\overline{X} - \mu) \rightarrow N(0, \Sigma).
\]
\end{document}

