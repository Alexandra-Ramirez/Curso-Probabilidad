\documentclass{article}
\usepackage[sc]{mathpazo}
\usepackage[T1]{fontenc}
\usepackage{bigints}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{amsmath} 
\usepackage{mathtools}
\usepackage{latexsym}
\geometry{verbose,tmargin=1.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
\usepackage{url}
\usepackage{graphicx}
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=2,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=false]
 {hyperref}
\hypersetup{
 pdfstartview={XYZ null null 1}}
\begin{document}
<<setup, include=FALSE, cache=FALSE>>=
library(knitr)
library(ggplot2)
library(grid)
library(animation)
# set global chunk options
opts_chunk$set(fig.path='figure/minimal-', fig.align='center', fig.show='hold')
options(formatR.arrow=TRUE,width=90)
@

\title{Notas de R para Machine Learning}
\date{}
\author{Edwin Uchupe Pipa, 
Oswaldo Ruesta Morales, Victor Ruiz Ccori}
\maketitle

{\Large {Introducci\'on al Machine Learning }}


\vspace{0.5cm}

El Machine Learning es parte  de la  inteligencia artificial, en la cual, \texttt{los ordenadores adquieren la capacidad de aprender}, sin la necesidad de ser programados expl\'icitamente.

Esta centrado en el desarrollo de programas inform\'ticos, a los que se les puede ense\~nar a crecer y cambiar
cuando se exponen a nuevos datos

\texttt{A menudo, los algoritmos de Machine Learnig se clasifican en supervisados y no supervisados}.

\begin{enumerate}
\item Aprendizaje supervisado: En resumidas cuentas, pueden aplicar lo aprendido en el pasado a los nuevos datos. En este trabajo se hablar\'a de 

\begin{itemize}
\item Regresi\'on Lineal.
\item Regresi\'on Log\'istica. 
\item \'Arboles de decisi\'on.

\end{itemize}
\item  Aprendizaje no supervisado: Estos algoritmos pueden extraer \texttt{inferencias} de conjuntos de datos. En este trabajo se hablar\'a de

\begin{itemize}
\item K-Means
\item KNN (K-Nearest Neighbors)
\item Algoritmo EM
\end{itemize}

\end{enumerate}

 Adem\'as, en los siguiente programas usaremos distintas librer\'ias
 
 <<fo1, prompt =TRUE, comment =NA, eval=FALSE>>=
install.packages("HSAUR")
install.packages("tools")
install.packages("rpart")
install.packages("rpart.plot")
install.packages("e1071")
install.packages("mclust")
install.packages("fpc")
install.packages("class")
install.packages("UsingR")
install.packages("gmodels")
install.packages("psych")
install.packages("mclust")
@


\vspace{0.5cm}

\textbf{Regresi\'on Lineal}

\vspace{0.3cm}

Es un modelo matem\'atico, que se usa cuando se desea aproximar una dependencia entre una variable dependiente, varias variables independientes, y un t\'ermino aleatorio.


\vspace{0.3cm}

 Un ejemplo de regresi\'on lineal simple:
 
 

<<fo2, prompt =TRUE, comment =NA, eval=FALSE>>=
data(iris)
lsfit(iris$Petal.Length, iris$Petal.Width)$coefficients

# Se usa la funcion lsfit, para encontrar el ajuste de minimos cuadrados

plot(iris$Petal.Length, iris$Petal.Width, pch=21, bg=c("red","green3","blue")
     [unclass(iris$Species)], xlab="Petal length", ylab="Petal width")
abline(lsfit(iris$Petal.Length, iris$Petal.Width)$coefficients, col="black")

# Luego, con plot() se grafica la dispersion, y con abline la recta ajustada

summary(lm(Petal.Width ~ Petal.Length, data=iris))
@

\vspace{0.5cm}

\textbf{\'Arboles de decisi\'on }

\vspace{0.3cm}

En simples cuentas, es una forma anal\'itica (adem\'as de gr\'afica) de representar los sucesos que surgen ante una decisi\'on, y ayudan a elegir la m\'as acertada en cuanto a la probabilidad. Para usar los \'arboles en R se necesitan las librer\'ias \texttt{rpart} y \texttt{rpart.plot}, para lo cual es necesario instalarlas (si es que no se tienen ya), y cargarlas en el espacio de trabajo

<<fo3, prompt =TRUE, comment =NA, eval=FALSE>>=
library("rpart")
library("rpart.plot")
@

\texttt{rpart} sirve para realizar estos \'arboles de decisi\'on, mientras que \texttt{rpart.plot} sirve para graficar estos \'arboles

\vspace{0.3cm}

 Un ejemplo sencillo de un \'arbol de decisi\'on podr\'ia ser el siguiente:

<<fo4, prompt =TRUE, comment =NA, eval=FALSE>>=
s <- sample(150,100)
Entrenamiento_iris <- iris[s,]
Test_iris <- iris[-s,]
Arbol <- rpart(Species ~ .,Entrenamiento_iris)
Arbol
@

\vspace{0.3cm}

Hasta aqu\'i lo \'unico que se ha hecho es un algoritmo que usa el conjunto de \texttt{datos  iris} para crear el \'arbol de decisi\'on. Con estos datos, el algoritmo aprende las reglas que crean el \'arbol.

\vspace{0.3cm}

Luego, se realiza la predicci\'on y se mide la precisi\'on del \'arbol en cuesti\'on.

\vspace{0.3cm}

<<fo5, prompt =TRUE, comment =NA, eval=FALSE>>=
p <- predict(Arbol,Test_iris,type = "class")
table(Test_iris[,5],p)
@

\vspace{0.3cm}

Lo \'unico que faltar\'ia, ser\'ia graficar el \'arbol

\vspace{0.3cm}

<<fo6, prompt =TRUE, comment =NA, eval=FALSE>>=
rpart.plot(Arbol)
@


\vspace{0.5cm}

\textbf{K-Means }

\vspace{0.3cm}


\texttt{K-Means} es un m\'etodo de agrupamiento, el cual construye una partici\'on de un conjunto de $n$ \mbox{observaciones} en $k$ grupos, cada observaci\'on pertenece al grupo cuyo valor medio es m\'as cercano

Se presenta aqu\'i un ejemplo de como se puede ejecutar el \texttt{K-means} en R. Para este ejemplo se van a necesitar las librer\'ias y el siguiente c\'odigo y mostrar una tabla con las especies en iris y la clasificaci\'on de estas 

<<fo7, prompt =TRUE, comment =NA, eval=FALSE>>=
library(e1071)
library(mclust)
library(fpc)

km <- kmeans(iris[,1:4], 3)
layout(matrix(1:1, ncol = 1))
plot(iris[,1], iris[,2], col=km$cluster)
points(km$centers[,c(1,2)], col=1:3, pch=8, cex=2)
@

\vspace{0.3cm}

Con esto se grafica el modelo inicial.

\vspace{0.3cm}

<<fo8, prompt =TRUE, comment =NA, eval=FALSE>>=
table(km$cluster, iris$Species)
sampleiris <- iris[sample(1:150, 40),]
distance <- dist(sampleiris[,-5], method="euclidean")
cluster <- hclust(distance, method="average")
plot(cluster, hang=-1, label=sampleiris$Species)
@

\vspace{0.3cm}

Se crea ahora la variable \texttt{resultado}

\vspace{0.3cm}

<<fo9, prompt =TRUE, comment =NA, eval=FALSE>>=
resultado <- cmeans(iris[,-5], 3, 100, m=2, method="cmeans")
plot(iris[,1], iris[,2], col=resultado$cluster)
points(resultadot$centers[,c(1,2)], col=1:3, pch=8, cex=2)
result$membership[1:3,]
@

\vspace{0.3cm}

Escribimos una  tabla de las espacies con la variable \texttt{result\$membership} (los primeros 3 valores)

<<fo10, prompt =TRUE, comment =NA, eval=FALSE>>=
table(iris$Species, resultado$cluster)

mc <- Mclust(iris[,1:4], 3)
plot(mc, what=c('classification'), dimens=c(3,4))
table(iris$Species, mc$classification)
@

\vspace{0.3cm}

Esta nueva tabla simplemente ilustra la dependencia de las especies con la variable \texttt{mc} creada \mbox{anteriormente}

<<fo11, prompt =TRUE, comment =NA, eval=FALSE>>=
cluster <- dbscan(sampleiris[,-5], eps=0.6, MinPts=4)
plot(cluster, sampleiris)
plot(cluster, sampleiris[,c(1,4)])
@

Los puntos de aviso en el cl\'uster $0$ son outliers no asignados. Se finaliza con una tabla que contiene los resultados finales


<<fo12, prompt =TRUE, comment =NA, eval=FALSE>>=
table(cluster$cluster, sampleiris$Species)
@


Otros ejemplo de \texttt{K-Means:}


<<fo13, prompt =TRUE, comment =NA, eval=FALSE>>=
require(graphics)
@

Se presenta en este caso, un ejemplo de 2 dimensiones, Se combinan $100 $ valores  aleatorios de la distribucion normal en una matriz de $2$ columnas,

\vspace{0.3cm}

<<fo14, prompt =TRUE, comment =NA, eval=FALSE>>=
x <- rbind(matrix(rnorm(100, sd = 0.3), ncol = 2),
           matrix(rnorm(100, mean = 1, sd = 0.3), ncol = 2))

colnames(x) <- c("x", "y")
@

\vspace{0.3cm}


y se les llama $x, y$ y  se usa la funcion \texttt{kmeans}

\vspace{0.3cm}

<<fo15, prompt =TRUE, comment =NA, eval=FALSE>>=
cl <- kmeans(x, 2)
plot(x, col = cl$cluster)
points(cl$centers, col = 1:2, pch = 8, cex = 2)
@

\vspace{0.3cm}

Se escribe  la funci \'on \texttt{ss} que devuelve la suma de cuadrados

\vspace{0.3cm}

<<fo16, prompt =TRUE, comment =NA, eval=FALSE>>=
ss <- function(x) sum(scale(x, scale = FALSE)^2)
@

\vspace{0.3cm}

Se ajusta los centros de agrupaci\'on a cada observaci\'on

\vspace{0.3cm}

<<fo17, prompt =TRUE, comment =NA, eval=FALSE>>=
fitted.x <- fitted(cl);  head(fitted.x)
resid.x <- x - fitted(cl)

cbind(cl[c("betweenss", "tot.withinss", "totss")],
      c(ss(fitted.x), ss(resid.x),    ss(x)))
stopifnot(all.equal(cl$ totss,        ss(x)),
          all.equal(cl$ tot.withinss, ss(resid.x)),
          all.equal(cl$ betweenss,    ss(fitted.x)),
          all.equal(cl$ betweenss, cl$totss - cl$tot.withinss),
          all.equal(ss(x), ss(fitted.x) + ss(resid.x))
)

kmeans(x,1)$withinss
@

\vspace{0.3cm}

Este es un cluster trivial. Es aqu\'i donde el aleatorio comienza a ayudar, pues los cluster son demasiados

\vspace{0.3cm}

<<fo18, prompt =TRUE, comment =NA, eval=FALSE>>=
(cl <- kmeans(x, 5, nstart = 25))
plot(x, col = cl$cluster)
points(cl$centers, col = 1:5, pch = 8)
@

\vspace{0.5cm}

\textbf{ KNN (K- Nearest Neighbors) }

\vspace{0.3cm}

Es un m\'etodo de clasificaci\'on no param\'etrico, que sirve para estimar el valor de la funci\'on de densidad de probabilidad de que un elemento  pertenezca a una clase a partir de la informaci\'on que se le proporciona.

Se crea una variable \texttt{entrenamiento (train)}, y otra \texttt{test}, con las  que luego se usar\'a la funci\'on \texttt{knn}, luego de crear el factor \texttt{cl}, y por \'ultimo se usa la funci\'on \texttt{atributes}, que muestra tanto los niveles, como la clase y la probabilidad de cada uno.


\vspace{0.3cm}

<<fo19, prompt =TRUE, comment =NA, eval=FALSE>>=
library(class)
train <- rbind(iris3[1:25,,1], iris3[1:25,,2], iris3[1:25,,3])
test <- rbind(iris3[26:50,,1], iris3[26:50,,2], iris3[26:50,,3])
cl <- factor(c(rep("s",25), rep("c",25), rep("v",25)))
knn(train, test, cl, k = 3, prob=TRUE)
attributes(.Last.value)
@

\vspace{0.3cm}

 Para el trabajo, su usar\'a la \texttt{tabla iris} de la   librer\'ia \texttt{UsingR}

\vspace{0.3cm}

 <<fo20, prompt =TRUE, comment =NA, eval=FALSE>>=
data("iris")
str(iris)
table(iris$Species)
head(iris)
iris
set.seed(9850)
runif(5)
gp <- runif(nrow(iris))
gp
@

\vspace{0.3cm}

Hasta aqu\'i, se hacen valores aleatorios de la distribuci\'on uniforme, con la funci\'on  \texttt{runif} y se asignan a \texttt{gp}.


Con el siguiente c\'odigo, se est\'a haciendo una funci\'on \texttt{normalize}, que resta el valor m\'inimo del vector, y lo divide entre el rango. 


\vspace{0.3cm}

 <<fo21, prompt =TRUE, comment =NA, eval=FALSE>>=
iris <- iris[order(gp),]
str(iris)
head(iris)
head(iris, 10)
str(iris)
summary(iris[, c(1, 2, 3, 4)])
normalize <- function(x){
  return((x - min(x))/(max(x) - min(x)))
}

@

\vspace{0.3cm}


Se crea la variable \texttt{iris\_n}, que usa la funci\'on \texttt{as.data.frame}, que usa las $4$ primeras filas de \texttt{iris} y la funci\'on \texttt{normalize}, adem\'as de las variables \texttt{iris\_train, iris\_test, iris\_train\_target} e \texttt{iris\_test\_target}



 <<fo22, prompt =TRUE, comment =NA, eval=FALSE>>=
normalize(c(1, 2, 3, 4, 5))
normalize(c(10, 20, 30, 40, 50))
iris_n <- as.data.frame(lapply(iris[,c(1, 2, 3, 4)], normalize))
str(iris_n)
summary(iris_n)
str(iris)
iris_train <- iris_n[1:139, ]
iris_train <- iris_n[1:129, ]
iris_test <- iris_n[130:150, ]
iris_train_target <- iris[1:129, 5]
iris_test_target <- iris[130:150, 5]
@

\vspace{0.3cm}


Lo unico que resta es usar nuevamente la funci\'on \texttt{knn}, con las variables mostradas, y mostrar todo en una tabla con \texttt{iris\_test\_target}


\vspace{0.3cm}
 <<fo23, prompt =TRUE, comment =NA, eval=FALSE>>=
ml <- knn(train = iris_train, test = iris_test, cl = iris_train_target, k = 13)
ml
table(iris_test_target, ml)
@

\end{document}