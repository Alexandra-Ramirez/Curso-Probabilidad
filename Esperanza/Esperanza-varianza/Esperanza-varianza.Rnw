\documentclass{article}
\usepackage[sc]{mathpazo}
\usepackage[T1]{fontenc}
\usepackage{bigints}
\usepackage[latin1]{inputenc}
\usepackage{geometry}
\usepackage{amsmath} 
\usepackage{mathtools}
%\usepackage[spanish]{babel}
\usepackage{latexsym}
\geometry{verbose,tmargin=1.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
\usepackage{url}
\usepackage{subfig}
\usepackage{graphicx}
\newcommand{\R}{\mathbb{R}}
\newcommand{\bb}[1]{\textbf{#1}}
\DeclareMathOperator{\rank}{\textbf{rango}}
\DeclareMathOperator{\proy}{\textbf{proy}}
\DeclareMathOperator{\nulll}{\textbf{nul}}
\DeclareMathOperator{\diag}{\textbf{diag}}
\DeclareMathOperator{\col}{\textbf{col}}
\DeclareMathOperator{\fila}{\textbf{fila}}
\DeclareMathOperator{\dimm}{dim}
\DeclareMathOperator{\Traz}{Tr}
%\theoremstyle{definition}
\everymath{\displaystyle}
\newtheorem{ejemplo}{{Ejemplo }}[section]
\newtheorem{teo}{{Teorema}}[section]
\newtheorem{defi}{{Definici\'on}}[section]
\newtheorem{pros}{{Proposici\'on}}[section]
\newtheorem{cor}{{Corolario}}[section]
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=2,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=false]
 {hyperref}
\hypersetup{
 pdfstartview={XYZ null null 1}}
\begin{document}

<<setup, include=FALSE, cache=FALSE>>=
library(knitr)
library(ggplot2)
library(grid)
#library(animation)
# set global chunk options
opts_chunk$set(fig.path='figure/minimal-', fig.align='center', fig.show='hold')
options(formatR.arrow=TRUE,width=90)
@

%\title{\underline{\textbf{Notas de mat\'ematica}}}
%\date{}
%\maketitle
\hspace*{0.5\linewidth}
\begin{minipage}{0.6\linewidth}
Curso: Introducci\'on a la Probabilidad y Estad\'istica CM -274\par
Esperanza y varianza de  variables aleatorias \par
Otros t\'opicos
\end{minipage}


\vspace {0.8cm}

\section{Esperanza y varianza de una variable aleatoria}
 
\vspace{0.3cm}

Al igual que las variables en \'algebra, las variables aleatorias tienen valores, pero estos valores s\'olo pueden determinarse a partir de experimentos aleatorios. Para caracterizar todos los resultados posibles de tal experimento usamos funciones de distribuci\'on, que proporcionan una especificaci\'on completa de una variable aleatoria sin la necesidad de definir un espacio de probabilidad subyacente. Especificar funciones de distribuci\'on no plantea dificultades matem\'aticas inherentes. 

\vspace{0.2cm}

Sin embargo, en las aplicaciones con frecuencia no es posible determinar la funci\'on de distribuci\'on completamente. A menudo una representaci\'on m\'as simple es posible, en lugar de especificar toda la funci\'on especificamos un conjunto de \texttt{valores estad\'isticos}. Un valor estad\'istico es el promedio de una funci\'on de una variable aleatoria. Esto puede ser pensado como el valor que se obtiene de promediar los resultados de un gran n\'umero de experimentos aleatorios en el enfoque de frecuencia de la probabilidad  y en  el marco axiom\'atico  dicho promedio corresponde a la operaci\'on  de la \texttt{esperanza} aplicado a una funci\'on de una variable aleatoria.

\vspace{0.2cm}

Como se sabe en  el marco axiom\'atico, una variable aleatoria se caracteriza completamente por su funci\'on de distribuci\'on acumulativa o de forma equivalente, por su funci\'on de masa o de densidad  de  probabilidad. Sin embargo, no siempre es posible y en muchos casos no es necesario, tener esta informaci\'on completa. En algunos casos, un solo n\'umero que captura alguna caracter\'istica promedio es suficiente. Tales n\'umeros o valores estad\'isticos,  incluyen  la \texttt{esperanza}, la \texttt{mediana} y la \texttt{moda} de la variable aleatoria. La esperanza  de una variable aleatoria se calcula de manera similar  cuando se calcula el valor promedio de un conjunto de n\'umeros. El valor promedio de un conjunto de $n$  n\'umeros se forma multiplicando cada n\'umero por $1/n$ y a\~nadiendo los resultados. La esperanza de una variable aleatoria que toma los valores $\{x_1, x_2, \dots, x_n\}$ es obtenida multiplicando,  cada proporci\'on de veces que ocurre y agreg\'ando al resultado, es decir, se forma $\displaystyle \sum_{i =1}^{n}x_ip_i$.

\vspace{0.2cm}

El \texttt{valor medio} de una variable aleatoria $X$  tambi\'en se le llama  \texttt{esperanza}  de $X$. La mediana de un conjunto de n\'umeros es el  n\'umero (o n\'umeros) que se encuentra en el centro del conjunto una vez que los n\'umeros est\'an dispuestos en orden ascendente o descendente. La \texttt{mediana} de una variable aleatoria $X$ es cualquier n\'umero $m$ que satisface:

\[
\mathbb{P}(X < m) \leq 1/2 \ \ \text{y}\ \ \mathbb{P}(X > m) \leq 1/2,
\]

o equivalentemente,

\[
\mathbb{P}(X < m) = \mathbb{P}(X > m)
\]

\vspace{0.2cm}

La \texttt{moda} es el valor posible m\'as probable. Para una variable aleatoria, es el valor para el cual la funci\'on de masa de probabilidad o la funci\'on de densidad de probabilidad alcanza su m\'aximo. Si $m$ es un modo de una variable aleatoria $X$, entonces $\mathbb{P}(X= m) \geq \mathbb{P}(X =x)$ para todos los valores de $x$ . Al igual que la mediana  y a diferencia del valor medio, una variable aleatoria puede tener m\'ultiples modas.


\begin{ejemplo}
\normalfont Si $X$ es la variable aleatoria que denota el n\'umero de puntos obtenidos en el  lanzamiento de un dado, entonces hay seis modas (ya que cada n\'umero de puntos es igualmente probable y todos los resultados alcanzan el mismo valor m\'aximo de $1/6$), la mediana es cualquier n\'umero en el intervalo $(3, 4)$. S\'olo hay un valor medio, que es igual a $(1 + 2 + 3 + 4 + 5 + 6)/6 = 3.5$.
\end{ejemplo}

\vspace{0.2cm}

La definici\'on de la media y el hecho de que sea un n\'umero \'unico lo convierte, desde un punto de vista matem\'atico, en un valor mucho m\'as atractivo para trabajar que la mediana o la moda. Este valor puede ampliarse f\'acilmente para proporcionar una caracterizaci\'on completa de una variable aleatoria.

\vspace{0.2cm}

De hecho, una variable aleatoria puede ser completamente caracterizada por un conjunto de valores, llamados \texttt{momentos}. Estos momentos se definen en t\'erminos  de la funci\'on de distribuci\'on, pero generalmente no se necesita calcular la funci\'on de distribuci\'on. Puede demostrarse que, si dos variables aleatorias tienen los mismos momentos de todos los \'ordenes, entonces tambi\'en tienen la misma distribuci\'on.


\vspace{0.2cm}

El primer momento, es la media o esperanza de una variable aleatoria $X$ y es dado por la integral de Riemann-Stieltjes:

\[
\mathbb{E}(X) = \bigintss_{-\infty}^{\infty}xdF(x).
\]

\vspace{0.2cm}

Para una variable aleatoria continua, se tiene que:

\[
\mathbb{E}(X) = \bigintss_{-\infty}^{\infty}xf_X(x) dx.
\]

donde $f_X(x)$ es la funci\'on de densidad de probabilidad de $X$. Si $X$ es una variable aleatoria discreta cuyos valores $\{x_1, x_2, \dots\}$, tienen las correspondientes probabilidades $\{p_1, p_2, \dots \}$, entonces la integral, es una suma y como hemos indicado anteriormente:

\[
\mathbb{E}(X) = \sum_{i =1}^{\infty}x_ip_i.
\]

\vspace{0.2cm}

Cuando una variable aleatoria discreta toma valores enteros sobre los enteros no negativos, podemos escribir, la esperanza como:

\begin{align*}
\mathbb{E}(X) &= p_1 + 2p_2 + 3p_3 +4p_4+ \cdots \\
 &= (p_1 + p_2 +p_3 + p_4 + \cdots) + (p_2 + p_3 + p_4 + \cdots ) + (p_3 + p_4 + \cdots ) + \cdots,
\end{align*}

Lo que conduce a una interesante f\'ormula:


\[
\mathbb{E}(X) = \sum_{i =1}^{\infty}\mathbb{P}(X \geq i)
\]


\vspace{0.2cm}

El n-\'esimo momento de una variable aleatoria $X$ es definida  de manera similar. Para una variable aleatoria continua es dado por:

\[
\mathbb{E}(X^n) = \bigints_{\infty}^{\infty}x^nf_X(x)dx,
\]

mientras que para una variable aleatoria discreta $X$, se tiene que:

\[
\mathbb{E}(X^n) = \sum_{i}x_{i}^np_i.
\]

\vspace{0.2cm}

Los primeros dos momentos son los m\'as importantes. El primero, es conocido como la esperanza o valor medio de $X$, el segundo momento es llamado \texttt{media cuadrada} de $X$ ambos  est\'an definidos como:

\[
\mathbb{E}(X^2) = \begin{cases}
\bigints_{-\infty}^{\infty}x^2f_X(x)dx &\ \text{si X es continua}\\
\displaystyle \sum_{i}x_i^2p_i &\ \ \text{si X es discreta}.
\end{cases}
\]

\vspace{0.2cm}

Los momentos centrales son tambi\'en importantes. Ellos son los momentos de la diferencia entre la variable aleatoria $X$ y el valor medio $\mathbb{E}(X)$. El n-\'esimo momento central es dado por:

\[
\mathbb{E}[(X - \mathbb{E}(X))^n] = \begin{cases}
\bigints_{-\infty}^{\infty}(X - \mathbb{E}(X))^nf_X(x)dx &\ \text{si X es continua}\\
\sum_{i}(x_i - \mathbb{E}(X))^np_i  &\ \ \text{si X es discreta}
\end{cases}
\]


\vspace{0.2cm}

El segundo momento central es llamada \texttt{varianza} de la variable aleatoria $X$ y es escrita como:

\[
\mathbb{V}(X) = \mathbb{E}[(X - \mathbb{E}(X))^2] = \begin{cases}
\bigints_{-\infty}^{\infty}(x - \mathbb{E}(X))^2f_X(x)dx &\ \text{si X es continua}\\
\sum_{i}(x_i - \mathbb{E}(X))^2p_i  &\ \ \text{si X es discreta}
\end{cases}
\]

\vspace{0.2cm}

Una f\'ormula usada para la varianza es:

\[
\mathbb{V}(x) = \mathbb{E}(X^2) - [\mathbb{E}(X)]^2.
\]

\vspace{0.2cm}

De esta ecuaci\'on, se tiene que la varianza, no puede ser negativa y caracteriza la dispersi\'on de la variable aleatoria $X$ con respecto a la media. La ra\'iz cuadrada de la varianza, es la \texttt{desviaci\'on est\'andar} de  $X$, denotada como $\sigma_X$. La desviaci\'on est\'andar produce un n\'umero cuyas unidades son iguales a la de la variable aleatoria y por esta raz\'on proporciona una imagen m\'as clara de la dispersi\'on de la variable aleatoria respecto de  su media. Una caracterizaci\'on final de una variable aleatoria  es el \texttt{coeficiente de variaci\'on}, escrito como $C_X$ y definido como:

\[
C_X = \frac{\sigma_X}{\mathbb{E}(X)},
\]

y su  utilidad reside en el hecho de que es una medida adimensional de la variabilidad de $X$ . A menudo, en lugar del coeficiente de variaci\'on, se usa el \texttt{coeficiente de variaci\'on cuadr\'atico}, que se define como:

\[
C_X^2 = \frac{\mathbb{V}(X)}{(\mathbb{E}(X))^2}.
\]


\begin{ejemplo}
\normalfont Considera la variable aleatoria $X$ que denota el n\'umero total de puntos obtenidos cuando dos dados  son lanzados simult\'aneamente. Anteriormente, vimos que la funci\'on de masa de probabilidad de $X$ est\'a dada por:

\vspace{0.2cm}

\begin{figure}[h]
\centering
\includegraphics[scale=.55]{e1.png}
\end{figure}

\vspace{0.2cm}

En este caso la esperanza de $X$ es dado por:

\begin{align*}
\mathbb{E}(X) &= \sum_{i =1}^{\infty}x_ip_i = \sum_{i =2}^{12}x_ip_i\\
 &= 2\frac{1}{36} + 3\frac{2}{36} + 4\frac{3}{36}+ 5\frac{4}{36} +  6\frac{5}{36} +  7\frac{6}{36} +  8\frac{5}{36} +  9\frac{4}{36} +  10\frac{3}{36} +  11\frac{2}{36} +  12\frac{1}{36}\\
 &= \frac{252}{36} = 7.0
\end{align*}

\vspace{0.2cm}

El c\'alculo de la varianza de $X$, se procede como sigue:

\vspace{0.2cm}

\begin{align*}
\mathbb{V}(X) &= \sum_{i =2}^{12}(x_i - 7)^2p_i \\
&=  5^2\frac{1}{36} + 4^2\frac{2}{36} + 3^2\frac{3}{36}+ 2^2\frac{4}{36} +  1^2\frac{5}{36} +  0^2\frac{6}{36} +  1^2\frac{5}{36} +  2^2\frac{4}{36} +  3^3\frac{3}{36} +  4^2\frac{2}{36} +  5^2\frac{1}{36} = 5.8333.
\end{align*}

\vspace{0.2cm}

Se sigue que  la desviaci\'on est\'andar $X$ es $\sqrt{5.8333} = 2.4152$. 
\end{ejemplo}

\begin{ejemplo}
\normalfont El tiempo transcurrido en minutos entre la colocaci\'on de un pedido de pizza y su entrega es aleatorio con la funci\'on densidad:

\[
f_X(x) = \begin{cases}
1/15 & \text{si}\ 25 < x < 40\\
0 & \text{en otros casos.}
\end{cases}
\]

\begin{itemize}
\item Determina la media y la desviaci\'on est\'andar del tiempo que tarda la pizzer\'ia en entregar la pizza. 
\item Supongamos que la tienda de pizza tarda $12$ minutos en hornear la pizza. Determina la media y la desviaci\'on est\'andar del tiempo que tarda la persona de entrega en entregar la pizza.
\end{itemize}

\vspace{0.2cm}

Sea el tiempo transcurrido entre la colocaci\'on de una orden y su entrega sea en $X$ minutos. Entonces:

\begin{align*}
\mathbb{E}(X) &= \int_{25}^{40}x\frac{1}{15}dx = 32.5 \\
\mathbb{E}(X^2) &= \int_{25}^{40}x^2\frac{1}{15}dx = 1075.
\end{align*}

\vspace{0.2cm}

Por tanto $\mathbb{V}(X) = 1075- (32.5)^2 = 18.75$ y as\'i $\delta_X = \sqrt{\mathbb{V}(X)} = 4.33$.

\vspace{0.2cm}

El tiempo que tarda la persona que entrega la pizza en entregarla  es $X -12$. Por lo tanto, las cantidades deseadas son:

\begin{align*}
\mathbb{E}(X -12) &= \mathbb{E}(X) -12 = 32.5 -12 = 20.5\\
\sigma_{X-12} &= \vert 1\vert \sigma_X = \sigma_X = 4.33
\end{align*}
\end{ejemplo}

\begin{ejemplo}
\normalfont En un juego, un jugador lanza una moneda sucesivamente hasta que consigue una \texttt{Cara}. Si esto ocurre en el $k$-\'esimo  lanzamiento, el jugador gana $2^k$ soles. Por lo tanto, si el resultado del primer lanzamiento  es cara, el jugador gana $2$ soles. Si el resultado del primer lanzamiento es sello y el  segundo lanzamiento es cara, el gana $4$. Si los resultados de los dos primeros lanzamientos son sellos, pero el tercero sale cara, ganar\'a $8$ soles  y as\'i sucesivamente. La pregunta es, para jugar este juego, ?` cu\'anto debe pagar una persona, que est\'a dispuesta a jugar este juego?.


\vspace{0.2cm}

Para responder a esta pregunta, sea $X$ la cantidad de dinero que el jugador gana. Entonces $X$ es una variable aleatoria con el conjunto de valores posibles $\{2, 4, 8, \dots, 2^k, \dots \}$ y

\[
\mathbb{P}(X = 2^k) = \biggl(\frac{1}{2}\biggr)^k,\ \ k = 1, 2, 3,\dots
\]

Por tanto,

\[
\mathbb{E}(X) = \sum_{k =1}^{\infty}2^k \biggl(\frac{1}{2}\biggr)^k = \sum_{k =1}^{\infty}1 = 1 + 1 +1 +\cdots = \infty.
\]

\vspace{0.2cm}

Este resultado muestra que el juego sigue siendo injusto, incluso si una persona paga la mayor cantidad posible para jugar. En otras palabras, este es un juego en el que uno siempre gana, no importa lo caro que es jugar. Para ver cu\'al es la falla, tenga en cuenta que te\'oricamente este juego no es factible de jugar porque requiere una enorme cantidad de dinero. En la pr\'actica, sin embargo, la probabilidad de que un jugador gane $2^k$ soles para un valor $k \rightarrow 0$. Incluso para valores peque\~nos de $k$, ganar es altamente improbable. Por ejemplo, para ganar $2^{30} = 1. 073. 741. 824$, debes conseguir $29$ sellos en una fila seguida por una cara. La probabilidad de que esto suceda es $1$ en $1.073.741.824$, mucho menos de $1$ en un bill\'on.
\end{ejemplo}

\vspace{0.3cm}

\begin{ejemplo}
\normalfont Sea $X_0$,  la cantidad de lluvia que caer\'a en los Estados Unidos el pr\'oximo dia de Navidad. Para $n>0$, sea $X_n$ la cantidad de lluvia que caer\'a en los Estados Unidos en Navidad, $n$ a\~nos despu\'es. Sea $N$ el menor n\'umero de a\~nos que transcurren antes de una lluvia de Navidad mayor que $X_0$. Supongamos que $P(X_i = X_j) = 0$ si $i \neq j$, los sucesos relativos a la cantidad de lluvia en dias de Navidad de diferentes a\~nos son independientes y los $X_n$ son id\'enticamente distribuidos. Hallar el valor esperado de $N$.

\vspace{0.2cm}

Como el valor $N$ es el primer valor de $n$, para que se cumpla $X_n > X_0$,

\begin{align*}
\mathbb{P}(N > n)& = \mathbb{P}(X_0 > X_1, X_0 > X_2, \dots, X_0 > X_n)\\
 &= \mathbb{P}(\max(X_0, X_1, X_2, \dots, X_n) = X_0) = \frac{1}{n + 1}
\end{align*}

\vspace{0.2cm}

donde la \'ultima igualdad procede de la simetr\'ia, no hay m\'as raz\'on para que el m\'aximo est\'e en $x_0$ que estar en $X_i, 0 \leq i \leq n$. Por tanto,

\vspace{0.2cm}

\[
\mathbb{P}(N =n) = \mathbb{P}(N > n -1)- \mathbb{P}(N >n ) = \frac{1}{n} - \frac{1}{n +1} = \frac{1}{n(n +1)}.
\]


\vspace{0.2cm}

Entonces se sigue que:

\[
\mathbb{E}{(N)} = \sum_{n =1}^{\infty}n\mathbb{P}(N =n) = \sum_{n =1}^{\infty} \frac{n}{n(n + 1)} =  \sum_{n =1}^{\infty} \frac{1}{n + 1} = \infty.
\] 

\vspace{0.3cm}

Se debe notar que $\mathbb{P}(N > n -1) = 1/n$ da la probabilidad que, en los Estados Unidos, tendremos que esperar m\'as, digamos, tres a\~nos para una lluvia de navidad que sea  mayor que $X_0$ y que la probabilidad sea  s\'olo $1/4$ y la probabilidad de que debamos esperar m\'as de nueve a\~nos es s\'olo $1/10$. Incluso con probabilidades tan bajas en promedio, deber\'ia tomar infinitamente muchos a\~nos antes de que tengamos m\'as lluvia en un d?a de navidad de la que tendremos el pr\'oximo dia de Navidad.
\end{ejemplo}

\begin{ejemplo}
\normalfont Considera la variable aleatoria continua $M$, cuya funci\'on de densidad de probabilidad es dada por:

\[
f_M(x) = \begin{cases}
e^{-x } &\ \ \text{si}\ \ x \geq 0, \\
0 & \text{en otros casos}.
\end{cases}
\]

\vspace{0.2cm}

En este caso, la esperanza es dada por:

\[
\mathbb{E}(M) = \bigints_{-\infty}^{\infty}xf_M(x)dx = \bigints_{0}^{\infty}xe^{-x}dx = -e^{-x}\biggl|_{0}^{\infty} = 1,
\]

y su varianza se calcula como:

\begin{align*}
\mathbb{V}(M) = \bigints_{0}^{\infty}(x - \mathbb{E}(M))^2e^{-x}dx  = \bigints_{0}^{\infty}(x -1)^2e^{-x}dx \\
= \bigints_{0}^{\infty}x^2e^{-x}dx - 2\bigints_{0}^{\infty}xe^{-x}dx + \bigints_{0}^{\infty}e^{-x}dx = 2 -2 + 1,
\end{align*}

\vspace{0.2cm}

y se muestra que ambas valores son iguales.
\end{ejemplo}

\vspace{0.2cm}

\begin{ejemplo}
\normalfont La funci\'on

\[
f_X(x) =\frac{1}{\pi(1 + x^2)}, \ \ -\infty < x < \infty,
\]

es una funci\'on densidad de probabilidad, pero no tiene un valor para la esperanza. Para ver esto, observa que:

\[
\bigints_{\infty}^{\infty}\frac{dx}{\pi(1 + x^2)} = \frac{1}{\pi}\arctan(x)\Bigg|_{-\infty}^{\infty} = \frac{1}{\pi}\biggl(\frac{\pi}{2} - \frac{-\pi}{2}\biggr) = 1,
\]

y as\'i es una funci\'on densidad. Sin embargo, no converge aboslutamente, desde que:

\[
\bigints_{-\infty}^{\infty}\Biggl \vert \frac{dx}{\pi(1 + x^2)}\Biggr\vert dx = \frac{1}{\pi}\bigints_{0}^{\infty}\frac{2x}{1 +x^2}dx = \frac{1}{\pi}\ln (1 +x^2)\Biggl|_{0}^{\infty} = \infty.
\]
\end{ejemplo}


\vspace{0.2cm}


Afirmamos anteriormente que la desviaci\'on est\'andar de una variable aleatoria es un n\'umero cuyas unidades son las mismas que las de la variable aleatoria y es una medida apropiada de la dispersi\'on de la variable aleatoria sobre su media. Si un valor particular para $X$, digamos $x$, est\'a en un intervalo de $\sigma_X$ alrededor de $\mathbb{E}(X)$, esto es $x \in [\mathbb{E}(x) -\sigma_X, \mathbb{E}(X) + \sigma_X]$, se dice que es "cercano" al valor esperado de $X$. En caso contrario se dice que "est\'a lejos de la media".

\vspace{0.2cm}

La desigualdad de Chebychev,  hace estos conceptos m\'as precisos. Sea $X$ una variable aleatoria cuya esperanza $\mathbb{E}(X) = \mu_X$ y cuya desviaci\'on est\'andar sea dada por $\sigma_X$. La desigualdad de Chebychev indica que:

\[
\mathbb{P}(\vert X -\mu_X \vert \leq k\sigma_X) \geq 1 - \frac{1}{k^2}.
\]

\vspace{0.2cm}

En palabras, esto dice que la probabilidad de que una variable aleatoria est\'e dentro de $\pm k$ desviaciones est\'andar de su valor medio es al menos $1-1/k^2$. Alternativamente, la probabilidad de que una variable aleatoria est\'e fuera de  $\pm k$ desviaciones est\'andar  de su media es como m\'aximo $1/k^2$.

\vspace{0.3cm}


Esto significa que el $75\% (= 1 - 1/2^2)$ de las veces, una variable aleatoria est\'a dentro de dos desviaciones est\'andar de su media y $89\% (= 1 - 1/3^2)$ de las veces , cae dentro de tres desviaciones est\'andar . De hecho resulta que en la mayor\'ia de los casos, estos n\'umeros son subestimaciones de lo que generalmente ocurre. De hecho, para muchas distribuciones, la probabilidad de que una variable aleatoria se encuentre dentro de dos desviaciones est\'andar puede ser tan alta como el $95\%$ o mayor.

\vspace{0.2cm}

Las variables aleatorias que consideraremos,  todas tienen esperanza  finita. Se dice que la esperanza no existe a menos que la integral o sumatoria correspondiente sean absolutamente convergente, es decir, la espernza  s\'olo existe si:

\[
\bigints_{-\infty}^{\infty}\vert x \vert f_X(x)dx < \infty \ \ \ \text{y}\ \ \ \sum_{i = 1}^{\infty}\vert x_i \vert p_i < \infty.
\]


\begin{pros}
\normalfont Para una variable aleatoria continua $X$ con una funci\'on de distribuci\'on $F_X$ y una funci\'on densidad de $f_x$,

\[
\mathbb{E}(X) = \bigints_{0}^{\infty}[1 -F_X(t)]dt - \bigints_{0}^{\infty}F_X(-t) dt.
\]
\end{pros}

Note que:

\begin{align*}
\mathbb{E}(X) &= \bigints_{-\infty}^{\infty}xf_X(x)dx = \bigints_{-\infty}^{0}xf_X(x)dx + \bigints_{0}^{\infty}xf_X(x)dx \\
 &=-\bigints_{-\infty}^{0}\Biggl(\bigints_{0}^{-x}dt\Biggr)f_X(x)dx + \bigints_{0}^{\infty}\Biggl(\bigints_{0}^{x}dt\Biggr)f_X(x)dx\\
 &= -\bigints_{0}^{\infty}\Biggl(\bigints_{-\infty}^{-t}f_X(x)dx\Biggr)dt + \bigints_{0}^{\infty}\Biggl(\bigints_{t}^{\infty}f_X(x)dx\Biggr)dt,
\end{align*}

\vspace{0.2cm}

Donde la \'ultima igualdad,  es obtenida cambiando el orden de la integraci\'on. El teorema sigue desde que :
$\displaystyle\int_{-\infty}^{-t}f_X(x) dx = F_X(-t)$ y $\displaystyle\int_{t}^{\infty}f_X(x) dx = \mathbb{P}(X > t)=  1 -F_X(t)$. En general, para alguna variable aleatoria $X$,

\[
\mathbb{E}(X) = \bigints_{0}^{\infty}\mathbb{P}(X >t)dt - \bigints_{0}^{\infty}\mathbb{P}(X \leq -t)dt.
\]

\vspace{0.2cm}

En particular, si $X$ es no negativa, esto es $\mathbb{P}(X < 0) = 0$, este teorema, establece que:

\[
\mathbb{E}(X) = \bigints_{0}^{\infty}[1 -F_X(t)]dt = \bigints_{0}^{\infty}\mathbb{P}(X > t)dt.
\]

\subsection{Esperanza de funciones de variables aleatorias}

\vspace{0.2cm}

Dado una variable aleatoria discreta $X$ con un conjunto posible de valores $A$ y la funci\'on de masa de probabilidad $p_X(x)$, la esperanza de un nueva variable aleatoria $Y$, que es una funci\'on de $X$, es decir  $Y = h(X)$ se puede escribir como:

\vspace{0.2cm}

\begin{equation}
\mathbb{E}(Y) = \mathbb{E}(h(X)) = \sum_{x \in A}h(x)p_X(x).
\end{equation}

\vspace{0.2cm}

En efecto, sea $\Omega$ un espacio muestral. Sea $h :\mathbb{R} \rightarrow \mathbb{R}$, una funci\'on de variable real y $X: \Omega \rightarrow A \subseteq \mathbb{R}$ es una variable aleatoria con el conjunto de posibles de valores $A$. Como se sabe, $h(X)$, la composici\'on de $g$ y $X$ es una funci\'on desde $\Omega$ al conjunto $h(A) = \{h(x): x\in A\}$. As\'i $h(X)$ es una variable aleatoria con un posible conjunto de valores $g(A)$. Ahora por definici\'on de esperanza:

\vspace{0.2cm}

\[
\mathbb{E}(h(X)) = \sum_{z \in h(A)}z\mathbb{P}(h(X) = z)
\]

\vspace{0.2cm}

Sea $h^{-1}(\{z\}) = \{x: h(x) = z\}$. Debemos notar que no aseveramos que $h$ tiene una funci\'on inversa y en este contexto, consideramos el conjunto $\{x: h(x) =z \}$, que es llamado la imag\'en inversa de $z$ y es denotada por $h^{-1}(\{z\})$. Ahora:

\vspace{0.2cm}

\[
\mathbb{P}(h(X) =z) = \mathbb{P}\biggl(X \in h^{-1}(\{z\})\biggr) = \sum_{\{x:x \in h^{-1}(\{z\}) \}}\mathbb{P}(X =x) = \sum_{ \{x: h(x) =z \}}p_X(x).
\]

\vspace{0.2cm}

\begin{align*}
\mathbb{E}(h(X)) &= \sum_{z \in h(A)}z\mathbb{P}(h(X) = z)  = \sum_{z \in h(A)}z\sum_{ \{x: h(x) =z \}}p_X(x)\\
& = \sum_{z \in h(A)}\sum_{ \{x: h(x) =z \}}zp_X(x) = \sum_{z \in h(A)}\sum_{ \{x: h(x) =z \}}h(x)p_X(x)\\
& = \sum_{x \in A}h(x)p_X(x),
\end{align*}

\vspace{0.2cm}

Donde la \'ultima igualdad se sigue del hecho que la suma es sobre $A$ puede ser llevados en dos escenarios: Podemos sumar primero  todos los $x$ con $h(x) =z$ y entonces para todo $z$.

\vspace{0.2cm}

Dos \'utiles propiedades de la esperanza se sigue a continuaci\'on:

\vspace{0.2cm}

\begin{pros}
\normalfont Sea $X$ una variable aleatoria discreta y sea $a, b \in \mathbb{R}$.

\begin{enumerate}
\item Si $\mathbb{P}(X \geq 0) = 1$ y $\mathbb{E}(X) = 0$, entonces $\mathbb{P}(X = 0) =1$.
\item Tenemos que $\mathbb{E}(aX +b) = a\mathbb{E}(X) + b$.
\end{enumerate}
\end{pros}


\vspace{0.2cm}

\begin{ejemplo}
\normalfont La funci\'on de masa de probabilidad  de una variable aleatoria $X$ es dado por:

\[
p_X(x) = \begin{cases}
x/15 & x = 1, 2, 3, 4, 5 \\
0 & \text{en otros casos  }
\end{cases}
\]

\vspace{0.2cm}

Calculemos la esperanza de $X(6 -X)$.


\vspace{0.3cm}

\[
\mathbb{E}[X(6 -X)] = 5\cdot\frac{1}{15} + 8\cdot\frac{2}{15}+  9\cdot\frac{3}{15} +  8\cdot\frac{4}{15} +  5\cdot\frac{1}{15} +  5\cdot\frac{5}{15} = 7.
\]
\end{ejemplo}

\begin{ejemplo}
\normalfont Para la variable aleatoria discreta $R$ que denota el n\'umero de caras obtenidas en tres lanzamientos de una moneda, la funci\'on de masa de probabilidad est\'a dada por:

\vspace{0.2cm}

\begin{figure}[h]
\centering
\includegraphics[scale=.75]{e2.png}
\end{figure}

\vspace{0.2cm}

Consideremos ahora una situaci\'on de juego en la que un jugador pierde $3$ soles si no aparecen caras, pierde $2$ soles si aparece una cara y  no gana nada  si aparecen dos caras,  pero gana $7$ soles si aparecen tres caras. Calculemos el n\'umero medio de soles ganados o p\'erdidos en el juego. Para ello, definimos la variable aleatoria derivada $Y$ como sigue:

\vspace{0.2cm}

\[
Y = h(X) = \begin{cases}
-3, & X = 0,\\
-2, & X =1, \\
0 & X = 2, \\
7, & X =3, \\
0 & \text{en otros caso.}
\end{cases}
\]

\vspace{0.2cm}

Calculemos $\mathbb{E}(Y)$ y descubramos  que el jugador pierde  25 c\'entimos cada vez que juega:

\vspace{0.2cm}

\[
\mathbb{E}(Y) = \sum_{x = 0,1, 2, 3}h(X)p_X(x) = -3 \times 1/8 -2\times 3/8 + 7\times 1/8 = -1/4.
\]
\end{ejemplo}


\vspace{0.5cm}


Similarmente, para variables aleatorias continuas $X$, la esperanza de alguna funci\'on $h(X)$ de $X$ es dado por:

\vspace{0.2cm}

\begin{equation}
\mathbb{E}(h(X)) = \bigints_{-\infty}^{\infty}h(X)f_X(x)dx.
\end{equation}

En efecto, para verificar el caso continuo, sea $h^{-1}(t, \infty) = \{x:h(x) \in (t, \infty)\} = \{x: h(x) >t\}$ y con una representaci\'on similar para $h^{-1}(-\infty, -t)$. No estamos asumiendo que $h$ tiene una funci\'on invesa. Estamos considerando el conjunto $\{x: h(x)\in (t, \infty) \}$, que es llamada la inversa de $(t, \infty)$ y es denotada por $h^{-1}(t, \infty)$.  Por la proposici\'on (1.1) tenemos:

\begin{align*}
\mathbb{E}(h(X)) &= \bigints_{0}^{\infty}\mathbb{P}(h(X) >t) dt - \bigints_{0}^{\infty}\mathbb{P}(h(X) \leq -t) dt\\
& =  \bigints_{0}^{\infty}\mathbb{P}(X \in h^{-1}(t, \infty)) dt -   \bigints_{0}^{\infty}\mathbb{P}(X \in  h^{-1}(-\infty, -t]) dt\\
& = \bigints_{0}^{\infty}\Biggl(\bigints_{\{x: x \in h^{-1}(t, \infty) \}}f_X(x)dx\Biggr)dt - \bigints_{0}^{\infty}\Biggl(\bigints_{\{x: x \in h^{-1}(-\infty, -t] \}}f_X(x)dx\Biggr)dt \\
& = \bigints_{0}^{\infty}\Biggl(\bigints_{\{x: h(x) > t\}}f_X(x)dx\Biggr)dt - \bigints_{0}^{\infty}\Biggl(\bigints_{\{x: h(x) \leq -t \}}f_X(x)dx\Biggr)dt.
\end{align*}

\vspace{0.2cm}

Ahora cambiamos el orden de integraci\'on para ambas integrables dobles. Desde que

\vspace{0.2cm}

\[
\{(t, x): 0 < t < \infty,\  h(x) > t \} = \{(t,x): h(x)> 0,\  0 < t < h(x) \},
\]

\vspace{0.2cm}

y

\vspace{0.2cm}

\[
\{(t, x): 0 < t < \infty,\  h(x) \leq -t \} = \{(t,x): h(x) < 0,\  0 < t \leq -h(x) \},
\]

\vspace{0.2cm}

Conseguimos :

\begin{align*}
\mathbb{E}(h(X)) &= \bigints_{\{x: h(x)> 0\}}\Biggl( \bigints_{0}^{h(x)}dt\Biggr)f(x)dx - \bigints_{\{x: h(x)<  0\}}\Biggl( \bigints_{0}^{-h(x)}dt\Biggr)f_X(x)dx \\
& = \bigints_{\{x: h(x)> 0\}}h(x)f_X(x)dx + \bigints_{\{x: h(x) < 0\}}h(x)f_X(x)dx \\
& = \bigints_{-\infty}^{\infty}h(x)f_X(x)dx.
\end{align*}

\vspace{0.2cm}

Note que la \'ultima ecuaci\'on se sigue, debido a que $\displaystyle \bigints_{\{x: h(x)= 0\}}h(x)f_X(x) dx = 0$.

\vspace{0.2cm}

\begin{pros}
\normalfont Sea $X$ una variable aleatoria con funci\'on densidad de probabilidad $f_X(x)$. Sean $h_1, h_2,\dots, h_n$, funciones de variable real y $\alpha_1, \alpha_2, \dots, \alpha_n$ n\'umeros reales. Entonces:

\[
\mathbb{E}(\alpha_1h_1(X) + \alpha_2h_2(X) + \cdots + \alpha_nh_n(X)) = \alpha_1\mathbb{E}(h_1(X)) + \alpha_2\mathbb{E}(h_2(X)) + \cdots + \alpha_n\mathbb{E}(h_n(X)).
\]
\end{pros}

\vspace{0.3cm}

\begin{ejemplo}
\normalfont Un punto $X$ es escogido desde el intervalo $(0, \pi/4)$ aleatoriamente. Calcula $\mathbb{E}(\cos 2X)$ y $\mathbb{E}(\cos^2X)$.

\vspace{0.2cm}

Primero calculamos la funci\'on distribuci\'on de $X$:

\[
F_X(t) = \mathbb{P}(X \leq t) = \begin{cases}
0 & t < 0 \\
\frac{t - 0}{\frac{\pi}{4} - 0} = \frac{4}{\pi} & 0 \leq t < \frac{\pi}{4}\\
1 & t \geq \frac{\pi}{4}.
\end{cases}
\]

\vspace{0.2cm}

As\'i, la funci\'on densidad $f$ de $X$ es:

\[
f_X(t) = \begin{cases}
\frac{4}{\pi} & 0 < t  < \frac{\pi}{4}\\
0 & \text{en otros casos}.
\end{cases}
\]

\vspace{0.2cm}

Ahora por (2),

\[
\mathbb{E}(\cos 2X) = \bigints_{0}^{\pi/4}\frac{4}{\pi}\cos 2x dx = \frac{2}{\pi}.
\]

\vspace{0.2cm}

Para calcular $\mathbb{E}(\cos^2 X)$, notamos que $ \cos^2 X = (1 + \cos 2X)/2$. Luego por la proposici\'on 1.3, tenemos:

\vspace{0.2cm}

\[
\mathbb{E}(\cos^2 X)= \mathbb{E}\biggl(\frac{1}{2} + \frac{1}{2}\cos 2X \biggr) = \frac{1}{2} + \frac{1}{2}\mathbb{E}(\cos 2X) = \frac{1}{2} + \frac{1}{2}\cdot \frac{2}{\pi} = \frac{1}{2} + \frac{1}{\pi}.
\]
\end{ejemplo}

\vspace{0.2cm}

\begin{ejemplo}
\normalfont Sea $X$ una variable aleatoria continua con funci\'on densidad de probabilidad dada por:

\[
f_X(x) = \begin{cases}
2x, & 0 \leq x \leq 1, \\
0 & \text{ en otros casos.}
\end{cases}
\]

\vspace{0.2cm}

y sea la variable $Y$, definida como $Y = 4X +2$. La esperanza de $X$ es dada por:


\vspace{0.2cm}


\[
\mathbb{E}(X) = \bigints_{0}^{1}2x^2 dx = \frac{2x^3}{3}\biggl|_{0}^{1} = 2/3.
\]

\vspace{0.2cm}

Ahora calculemos la esperanza de $Y$ de dos maneras: primero integrando la funci\'on de densidad sobre un intervalo apropiado  y segundo usando el enfoque  ofrecido por la ecuaci\'on (2).

\vspace{0.2cm}

Para el primer caso, encontremos la funci\'on de densidad de probabilidad de $Y$ al encontrar primero su funci\'on de distribuci?n acumulativa. Sin embargo, primero necesitamos encontrar la funci\'on de distribuci\'on acumulativa de $X$. Esto es:

\vspace{0.2cm}

\[
F_X(x) = \bigints_{0}^{x}2u du = x^2 \ \ \text{para}\ \ 0 \leq x \leq 1;
\]

\vspace{0.2cm}

es igual a $0$ para $x < 0$ y es igual a $1$ para $x > 1$. Ahora calculemos $F_Y(y)$ como:

\vspace{0.2cm}

\[
F_Y(y) = \mathbb{P}(Y \leq y) = \mathbb{P}(4X + 2\leq y) = \mathbb{P}\biggl(X \leq \frac{y -2}{4}\biggr) = F_X\biggl(\frac{y -2}{4}\biggr).
\]

\vspace{0.2cm}

Para encontrar los l\'imites, observamos que $F_X(x) = 0$ para $x < 0$, lo que nos permite afirmar que $F_Y(y) = 0$ para $( y -2)/4$, esto es, $y < 2$. Tambi\'en $F_X(x) = 1$ para $x > 1$, as\'i $F_Y(y)  =1$ para  $(y -2)/4 > 1$, esto es, para $y >6$. Luego de la ecuaci\'on anterior para $F_Y(y)$, tenemos que:

\vspace{0.2cm} 

\[
F_Y(y) = \biggl(\frac{y -2}{4}\biggr)^2, \ \ \text{para}\ \ 2 \leq y \leq 6.
\]

\vspace{0.2cm}

Finalmente, diferenciando $F_Y(y)$ con respecto a $y$, podemos tener la funci\'on densidad de $Y$ como:

\vspace{0.2cm}

\[
f_Y(y) = \begin{cases}
2(y -2)/16, & 2\leq y \leq 6, \\
0 & \text{en otros casos.}
\end{cases}
\]

\vspace{0.2cm}

\[
\mathbb{E}(Y) = \bigints_{2}^{6}2y(y -2)/16 dy = (2y^3/3 -2y^2)/16\biggl|_{2}^{6} = 4.6667.
\]

\vspace{0.2cm}

Usando la ecuaci\'on (2), obtenemos:

\vspace{0.2cm}

\begin{align*}
\mathbb{E}(Y) = \mathbb{E(h(X))} = \bigints_{-\infty}^{\infty}h(X)f_X(x)dx = \bigints_{0}^{1}(4x +2)2x dx = \bigints_{0}^{1}(8x^2 + 4x)dx \\
= (8x^3/3 + 2x^2)\biggl|_{0}^{1} = 14/3.
\end{align*}

\vspace{0.2cm}

Este resultado coincide con el resultado anterior, pero implica menos c\'alculos.
\end{ejemplo}

\vspace{0.2cm}

La extensi\'on a funciones de $n$ variables aleatorias es inmediata. Si $Y$ es una variable aleatoria  definida como  una funci\'on $h$ de $n$ variables aleatorias $X_1, X_2, \dots, X_n$, la esperanza de $Y$ puede ser encontrada utilizando la f\'ormula:

\vspace{0.2cm}

\[
\mathbb{E}(Y) = \mathbb{E}[h(X_1, X_2, \dots, X_n)] = \begin{cases}
\bigints_{-\infty}^{\infty}\bigints_{-\infty}^{\infty}\cdots \bigints_{-\infty}^{\infty}h(x_1, x_2, \dots, x_n)f_X(x_1,x_2, \dots, x_n)dx_1dx_2\cdots dx_n \\
\sum_{x_1}\sum_{x_2}\cdots \sum_{x_n}h(x_1, x_2, \dots, x_n)p_X(x_1, x_2, \dots, x_n).
\end{cases}
\]

\vspace{0.2cm}

para el caso continuo y discreto, respectivamente, Si la funci\'on $h$ es una funci\'on lineal, esto es:

\vspace{0.2cm}

\[
h(x_1, x_2, \dots, x_n) = \alpha_1x_1+ \alpha_2x_2+ \cdots + \alpha_nx_n,
\]

\vspace{0.2cm}

donde $\alpha_i, \ i=1,2,\dots, n$ son constantes, entonces:

\vspace{0.2cm}

\[
\mathbb{E}(Y) = \mathbb{E}[h(X_1, X_2, \dots, X_n)] = h(\mathbb{E}(X_1), \mathbb{E}(X_2), \dots, \mathbb{E}(X_n)).
\]

\vspace{0.2cm}

Se debe observar que las variables aleatorias $X_1, X_2, \dots X_n$ no se necesitan ser independientes. Sin embargo, este resultado no se aplica generalmente a funciones $h$ que no son lineales.
\end{document}
