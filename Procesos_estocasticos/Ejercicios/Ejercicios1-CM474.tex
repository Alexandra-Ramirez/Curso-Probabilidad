\documentclass[a4paper,11pt]{report}
\usepackage[sc]{mathpazo}
%\usepackage[light,firsttwo,outline,bottomafter]{draftcopy}
%\usepackage{srcltx}
\usepackage{anysize} % Soporte para el comando \marginsize
\marginsize{1.2cm}{1.2cm}{1cm}{1cm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[latin1]{inputenc}
\usepackage[english,spanish]{babel}
\usepackage{amsmath}
\usepackage{multicol} 
\columnsep=7mm
\usepackage{latexsym}
\usepackage{mathrsfs}
\usepackage{bigints}
\usepackage{indentfirst}
\usepackage{graphicx}
\usepackage{enumitem}
%\usepackage{hyperref}

\usepackage{times}



%\usepackage{xcolor}
%\usepackage{pgfplots}
%\usepackage{tikz}

% Define bar chart colors
%
%\definecolor{bblue}{HTML}{4F81BD}
%\definecolor{rred}{HTML}{C0504D}
%\definecolor{ggreen}{HTML}{9BBB59}
%\definecolor{ppurple}{HTML}{9F4C7C}





\setlength{\paperwidth}{216mm} \setlength{\paperheight}{230mm}
\setlength{\textwidth}{39pc} \setlength{\textheight}{57.5pc}
\setlength{\topmargin}{-1.5cm} \setlength{\oddsidemargin}{-0.5cm}
\setlength{\evensidemargin}{0.9cm}
%\setlength{\footskip}{-0.3cm}

\newcommand{\ds}{\displaystyle}
\newcommand{\normal}{\triangleleft \,}
\newcommand{\tx}{\textrm}
% \linespread{1.2} \sloppy


\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\PR}{\mathbb{P}}
\newcommand{\e}{\rightarrow}
\newcommand{\bi}{\Leftrightarrow}
\newcommand{\com}{\mathbb{N} \bi}
\newcommand{\fu}{f:\N \e \R}
\newcommand{\ba}{\backslash}
\newcommand{\Q}{\mathbb{Q}}


\newcommand{\calP}{\mathcal{P}}
\newcommand{\calF}{\mathcal{F}}
\newcommand{\calL}{\mathcal{L}}


\newcommand{\ovl}{\overline}
\newcommand{\ora}{\overrightarrow}
\newcommand{\ola}{\overleftarrow}
\newcommand{\olra}{\overleftrightarrow}
\newcommand{\ula}{\underleftarrow}
\newcommand{\ura}{\underrightarrow}


\newcommand{\inner}[2]{\langle{#1},{#2}\rangle}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\begin{center}
	{\LARGE\textbf {Preguntas de Introducci\'on a los Procesos Estoc\'asticos}}
\end{center}

\setlength{\unitlength}{1in}

\begin{picture}(6,.1) 
\put(0,0) {\line(1,0){6.25}}         
\end{picture}
\vspace{0.8cm}

\vspace{0.2cm}
	
	\renewcommand{\arraystretch}{2}
	
	\vskip.25in
	
	\noindent {\Large \textbf{Lista de Problemas}} 
	
	\vskip.25in

%\begin{multicols}{1}

 \vspace{0.3cm}
 
\begin{enumerate}
\item Sea $p_1, p_2, \dots, p_N$ n\'umeros no negativos tal que $p_1 + p_2 + \cdots p_N = 1$ y sea $\Omega = \{\omega_1, \omega_2, \cdots, \omega_N \}$ y sea $F$ el conjunto potencia de $\Omega$. Muestra que la funci\'on $\mathbb{Q}$ dada por

$$
\mathbb{Q}(A) = \sum_{i:\omega_i \in A}p_i \ \ \ A \in F
$$
es una probabilidad en $(\Omega, F)$. Si $F$ no es el conjunto potencia, $\mathbb{Q}$ es una probabilidad?.
\item Sean $A$ y $B$ dos eventos con probabilidades $\mathbb{P}(A) = \frac{3}{4}$ y $\mathbb{P}(B) = \frac{1}{3}$. Muestra que $\frac{1}{12} \leq \mathbb{P}(A \cap B) \leq \frac{1}{3}$. Encuentra una cota para $\mathbb{P}(A \cup B)$.
\item Sea $A_r, r\geq 1$, los eventos tal que $\mathbb{P}(A_r) = 1$ para todo $r$. Muestra que $\mathbb{P}(\displaystyle\cap_{r =1}^{\infty}A_r) = 1$.
\item El evento $A$ se dice que es \texttt{repelido} por el evento $B$ si $\mathbb{P}(A|B) < \mathbb{P}(A)$ y es \texttt{atraido} por $B$ si $\mathbb{P}(A|B) > \mathbb{P}(A)$. Muestra que si $B$ atrae a $A$, entonces $A$ atrae a $B$ y $B^c$ repele a $A$.
\item Considera una secuencia de lanzamientos infinitos. La probabilidad de un \'exito en el i-\'esimo \mbox{lanzamiento} es alg\'un n\'umero positivo $p_i$. Sea $N$ el evento que no hay \'exitos y sea $I$ el evento donde hay un n\'umero infinito de \'exitos.

\begin{itemize}
	\item Asumiendo que los lanzamientos son independientes y que $\displaystyle\sum_{i=1}^{\infty}p_i = \infty$. Muestra que $\mathbb{P}(N) =0$ y $\mathbb{P}(I) = 1$.
	\item Asumimos que $\displaystyle\sum_{i=1}^{\infty}p_i < \infty$. Muestra que $\mathbb{P}(I) = 0$.
\end{itemize}
\item Si $X$ tiene una distribuci\'on geom\'etrica con par\'ametro $p$, muestra que

\[
\mathbb{P}(X > m + n| X > m) = \mathbb{P}(X > n)
\]

para $m, n = 0,1,2, \dots$.

\item Sea $X$ y $Y$ una variable aleatoria discreta, cada una teniendo una funci\'on de masa dada por

\[
\mathbb{P}(X = k) = \mathbb{P}(Y = k) = pq^k \ \ \text{para}\ \ k =0,1,2,\dots
\]
donde $0 < p = 1 -q < 1$. Muestra que

\[
\mathbb{P}(X =k|X +Y = n) = \dfrac{1}{n +1}\ \ \text{para}\ \ k = 0,1, 2, \dots, n.
\]

\item Sea $\Omega = \{\omega_1, \omega_2, \omega_3 \}$, con $\mathbb{P}(\omega_1) = \mathbb{P}(\omega_2) = \mathbb{P}(\omega_3) = \frac{1}{3}$. Definimos $X, Y, Z: \Omega \rightarrow \mathbb{R}$

\begin{align*}
X(\omega_1)=1,   X(\omega_2)=2,  X(\omega_3)=3 \\
Y(\omega_1)=2,   Y(\omega_2)=3,  Y(\omega_3)=1 \\
Z(\omega_1)=2,   Z(\omega_2)=2,  Z(\omega_3)=1
\end{align*}

Muestra que $X$ e $Y$ tienen la misma funci\'on de masa. Encuentra las funciones de masa de $X +Y, XY $ y $X/Y$. Encuentra la funci\'on de masa condicional $f_{Y|Z}$ y $f_{Z|Y}$.
\item Prueba lo siguiente

\begin{enumerate}
	\item Muestra que para dos constantes $a$ y $b$
	
	\[
	\mathbb{Var}(aX + b) = a^2\mathbb{Var}(X)
	\]
para alguna variable aleatoria $X$.
\item Muestra que para dos variables aleatorias independientes $X$ e $Y$

\[
\mathbb{Var}(X + Y) = \mathbb{Var}(X) + \mathbb{Var}(Y)
\]
\end{enumerate}
\item Una variable aleatoria $X$ se dice que tiene una \texttt{distribuci\'on uniforme} sobre $[a, b]$ si

\[
\mathbb{P}(X \leq t) = \dfrac{t -a}{b -a}, \ \ a \leq t \leq b.
\]

\begin{enumerate}
	\item Calcula $\mathbb{E}(X), \mathbb{Var}(X), \mathbb{E}[(X -a)/(b -a)]$.
	\item Encuentra la distribuci\'on  de $Y = (X- a)/(b -a)$. 
\end{enumerate}

\item Sean $X$ e $Y$ variables aleatorias independientes, con una distribuci\'on de Poisson con par\'ametros $\mu$ y $\lambda$ respectivamente. Prueba que $X +Y$ tiene una distribuci\'on de Poisson y que $\mathbb{V}(X + Y) = \mathbb{V}(X) + \mathbb{V}(Y)$. Encuentra la probabilidad condicional $\mathbb{P}(X= k| X+y = n)$ para $0 \leq k \leq n$ y as\'i muestra que la esperanza condicional de $X$ dado $X +Y = n$, esto es

\[
\mathbb{E}(X| X +Y = n) = \displaystyle\sum_{k=0}^{\infty}k\mathbb{P}(X = k|X +Y =n).
\]
es $n\lambda/(\lambda + \mu)$.
\item Supongamos que $X_1, X_2, \dots$ son variables aleatorias no negativas id\'enticamente distribuidas, independientes con

\[
\mathbb{E}(X_n) = a, \ \ \ \ \mathbb{Var}(X_n) = b^2.
\]

Sea $N$ una variable aleatoria no negativa de valor entero, que es independiente de $\{ X_1, X_2, \dots\}$ y sea 

\[
\mathbb{E}(N) = c, \ \ \ \ \mathbb{Var}(N) = d^2.
\]

\vspace{0.2cm}

Sea $S_0 = 0, S_1 = X_1, S_2 = X_1 + X_2, \dots $ y sea $Y = S_N$

\begin{enumerate}
	\item Calcula $\mathbb{E}(Y|N), \mathbb{E}(Y^2|N)$.
	\item Calcula $\mathbb{E}(Y), \mathbb{E}(Y^2), \mathbb{Var}(Y)$.
	\item Muestra que para alg\'un $\alpha \in \mathbb{R^{+}}$
	
	\[
	\mathbb{E}(e^{-\alpha Y}) = \mathbb{G}(F(\alpha))
	\]
	
\vspace{0.2cm}

donde

\vspace{0.2cm}

\[
F(\alpha) = \mathbb{E}(e^{-\alpha X_n}), \ \ \ \mathbb{G}(\beta) = \mathbb{E}(\beta^N), \ \ \ \alpha \in \mathbb{R^{+}}, \beta \in [0,1]
\]
\end{enumerate}
\item Consideremos sobre $(a ,b)$ una variable $X$, tal que 

$$
F_{X}(x) = \begin{cases}
(x -a)/(b -a ) & x\in (a,b) \\
0				& x\leq a\\
1			& 	x \geq b
\end{cases}
$$

y 

$$
f_{X}(x) = \begin{cases}
1/(b -a) & x \in (a, b)\\
0 & \text{en otros casos}.
\end{cases}
$$

Calcula la esperanza y varianza de $X$.

\item Sea $X_1, X_2, \dots, X_n$ son variables aleatorias independientes y supongamos que $X_k$ es de Bernoulli con par\'ametro $p_k$. Muestra que $Y = X_1 +X_2 + \cdots X_n$ tiene media y varianza dada por

 \[
 \mathbb{E}(Y) = \sum_{1}^{n}p_k, \ \ \ \mathbb{Var(Y)} = \sum_{1}^{n}p_k(1 - p_k)
 \]
\item Sea $\textbf{X} = (X_1, X_2, \dots, X_n)$ un vector de variables aleatorias. La \texttt{matriz de covarianza} $\textbf{V(X)}$ de $\textbf{X}$ es definida como la matriz sim\'etrica $n \times n$ con entradas $(v_{i,j}: 1\leq i, j\leq n)$ dado por $v_{i,j} = \text{cov}(X_i, X_j)$. Muestra que $\det(\textbf{V(X)}) = 0$ si y s\'olo si las $X_i$ son linealmente dependiente con probabilidad $1$, esto es $\mathbb{P}(a_1X_1 + a_2X_2 + \cdots +a_nX_n = b) =1$ para alg\'un $\textbf{a}$ y $b$.
\item Sea $\textbf{X} = (X_1, X_2, \dots, X_n)$ un vector de variables aleatorias cada una con la distribuci\'on de Bernoulli con par\'ametro $p$. Sea $f:\{0, 1\}^n \rightarrow \mathbb{R}$ creciente ($f(\mathbf{x}) \leq f(\mathbf{y})$ siempre que $x_i \leq y_i$ para cada $i$). 

\begin{itemize}
	\item Sea $e(p) = \mathbb{E}(f\textbf{X})$. Muestra que $e(p_1) \leq e(p_2)$ si $p_1 \leq p_2$.
\end{itemize}

\item Sea $X$ y $Y$ variables aleatorias con densidad conjunta $f(x,y) =cx(y -x)e^{-y}, \ 0 \leq x \leq y \infty$.

\begin{enumerate}
	\item Encuentra $c$.
	\item Muestra que
	
	\begin{align*}
	f_{X|Y}(x | y) = 6x(y -x)y^{-3}, \ \ 0 \leq x \leq y, \\
	f_{Y|X}(y|x) = (y -x)e^{x -y}, \ \ \ \ 0 \leq x \leq y \leq \infty
	\end{align*}
\item Calcula $\mathbb{E}(X|Y) =\frac{1}{2}Y$ y  $\mathbb{E}(Y|X) = X +2$.
\end{enumerate}
\item Si $X$ es una variable aleatoria discreta y $g: \mathbb{R} \rightarrow \mathbb{R}$, prueba que 

$$
\mathbb{E}(g(X)) = \sum_{x \in Im\ X}g(x)\mathbb{P}(X =x).
$$

siempre que esta suma converga absolutamente.
\item Prueba lo siguiente

\begin{enumerate}
	\item Para una variable aleatoria no negativa $X$
	
	\[
	\mathbb{E}(X) = \bigints_{0}^{\infty}\mathbb{P}(X >t)dt
	\]
	\item Si $X$ es una variable aleatoria tomando los valores $\mathbb{N} = \{0,1,2, \dots\}$, entonces

\[
\mathbb{E}(X) =\sum_{n = 0}^{\infty}\mathbb{P}(X >n)
\]


\end{enumerate}

\item Sea $\{X_r: r\geq 1 \}$ variables aleatorias indepedientes, uniformemente distribuidas en $[0,1]$. Sea $0 < x < 1$ y definimos

\[
N = \min\{n \geq 1: X_1 +X_2 + \cdots + X_n > x\}
\]

Muestra que $\mathbb{P}(N > n) = x^n/n!$ y as\'i encuentra la media y la varianza de $N$.
\item Sea $X$ una variable aleatoria con una distribuci\'on binomial con par\'ametros $n$ y $p$, muestra que 

\[
\mathbb{E}\Bigl( \dfrac{1}{1 + X}\Bigr) = \dfrac{1 -(1-p)^{n +1}}{(n+1)p}
\] 

Encuentra el l\'imite de esta expresi\'on cuando $n \rightarrow \infty$ y $p \rightarrow \infty$.
\item La variable aleatoria $X$ tiene una funci\'on densidad proporcional a $g(x)$, donde $g$ es una funci\'on satisfaciendo

\[
g(x) = \begin{cases}
\vert x\vert^{-n} & \text{si} \vert x \vert \geq 1 \\
0 & \text{en otros casos}
\end{cases}
\]
y $n \geq 2$ es un entero. Encuentra la funci\'on de densidad de $X$ y determina los valores de $n$ para el cual la media y la varianza de $X$ existe.

\item Sea $\{X_r: r \geq 1\}$ variables aleatorias  id\'enticamente distribuidas e independientes con una funci\'on de distribuci\'on $F$ satisfaciendo $F(y) < 1$ para todo $y$ y sea $Y(y) = \min \{k:X_k > y \}$. Muestra que

\[
\lim_{y \rightarrow \infty}\mathbb{P}\bigr(Y(y)\leq \mathbb{E}Y(y) \bigl) = 1 -e^{-1}.
\]
\item Supongamos que $X_1, X_2, \dots$ son variables aleatorias no negativas id\'enticamente distribuidas, independientes con

\[
\mathbb{E}(X_n) = a, \ \ \ \ \mathbb{Var}(X_n) = b^2.
\]

Sea $N$ una variable aleatoria no negativa de valor entero, que es independiente de $\{ X_1, X_2, \dots\}$ y sea 

\[
\mathbb{E}(N) = c, \ \ \ \ \mathbb{Var}(N) = d^2.
\]

\vspace{0.2cm}

Sea $S_0 = 0, S_1 = X_1, S_2 = X_1 + X_2, \dots $ y sea $Y = S_N$

\begin{enumerate}
	\item Calcula $\mathbb{E}(Y|N), \mathbb{E}(Y^2|N)$.
	\item Calcula $\mathbb{E}(Y), \mathbb{E}(Y^2), \mathbb{Var}(Y)$.
	\item Muestra que para alg\'un $\alpha \in \mathbb{R^{+}}$
	
	\[
	\mathbb{E}(e^{-\alpha Y}) = \mathbb{G}(F(\alpha))
	\]
	
	\vspace{0.2cm}
	
	donde
	
	\vspace{0.2cm}
	
	\[
	F(\alpha) = \mathbb{E}(e^{-\alpha X_n}), \ \ \ \mathbb{G}(\beta) = \mathbb{E}(\beta^N), \ \ \ \alpha \in \mathbb{R^{+}}, \beta \in [0,1]
	\]
\end{enumerate}
\item Sea $X$ con una funci\'on generadora de probabilidad $G_{X}(s)$ y sea $u_n = \mathbb{P}(X > n)$. Muestra que la funci\'on generadora $U(s)$ de la secuencia $u_0, u_1, \dots$ satisface
\[
(1 -s)U(s) = 1 -G_{X}(s)
\]
siempre que la serie definida, de esa funci\'on generadora converge.

\item Sean $G_1$ y $G_2$ funciones generadoras de probabilidad y $0 \leq \alpha \leq 1$. 

Muestra  que $G_1G_2$ y $\alpha G_1 + (1 -\alpha)G_2$ son funciones generadoras de probabilidad. ?` Es $G(\alpha s)/G(\alpha)$ es una funci\'on generadora de momentos?.
\item 
\begin{enumerate}
\item Muestra que $M(t)M(-t)$ es la funci\'on generadora de momentos de $X -Y$, donde $Y$ es independiente de $X$ , pero que tiene la misma distribuci\'on.
\item De manera similar, describe las variables aleatorias que tienen funciones generadoras de momentos
\[
\dfrac{1}{2 -M(t)}, \ \ \ \ \ \bigintss_{0}^{\infty}M(ut)e^{-u}du.
\]
\end{enumerate}
\item Una moneda es lanzada $400$ veces. Usa el \texttt{teorema del l\'imite central} para calcular una aproximaci\'on a la probabilidad de conseguir al menos $250$ caras.
\item Sea $\{X_n: n = 1,2, \dots\}$ un \texttt{proceso de Bernoulli} con probabilidad de \'exito $p$. Sea $X_n$ el n\'umero de \'exitos en el n-\'esimo ensayo. Se define

\[
N_{n}(\omega) = \begin{cases}
0 & \text{si} \ \ n = 0, \\
X_{1}(\omega)+ \cdots +X_{n}(\omega)& \text{si} \ \ n =1, 2, \dots 
\end{cases}
\]

para cada $\omega \in \Omega$. Entonces $N_n$ es el n\'umero de \'exitos en los primeros $n$ ensayos y $N_{n + m} - N_{n}$ es el n\'umero de \'exitos en los ensayos $n +1, n + 2, \dots, n + m$.

\vspace{0.2cm}

Prueba lo siguiente

\vspace{0.2cm}

\begin{enumerate}
	\item $\mathbb{E}(N_n) = np$.
\item $\mathbb{Var}(N_n) = np(1 -p)$
\item  $\mathbb{E}(N_n^2) = n^2p^2 + np(1 -p)$.
\item Para alg\'un $n , k \in \mathbb{N}$, se cumple

\[
\mathbb{P}(N_{k + 1} = k) = p\mathbb{P}(N_{n} = k -1) + (1- p)\mathbb{P}(N_n = k).
\]
\end{enumerate}
\item Prueba que si $n \in \mathbb{N}$

\[
\mathbb{P}(N_n = k) = \frac{n!}{k!(n -k)!}p^k(1 -p)^{n -k}, \ k =0,\dots, n.
\]

\vspace{0.2cm}

Adem\'as, para alg\'un $m, n \in \mathbb{N}$

\[
\mathbb{P}(N_{m + n} - N_{m}= k) = \frac{n!}{k!(n -k)!}p^k(1 -p)^{n -k}, \ k =0,\dots, n.
\]
\end{enumerate}

\begin{flushright}
 %UNI, 27 de Noviembre   del 2015
%\\{\bfseries El profesor.}
\footnote{Hecho en \LaTeX}
\end{flushright}

\end{document}
