\documentclass{article}
\usepackage[sc]{mathpazo}
\usepackage[T1]{fontenc}
\usepackage{bigints}
\usepackage[latin1]{inputenc}
\usepackage{geometry}
\usepackage{amsmath} 
\usepackage{mathtools}
%\usepackage[spanish]{babel}
\usepackage{latexsym}
\geometry{verbose,tmargin=1.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
\usepackage{url}
\usepackage{graphicx}
%\theoremstyle{definition}
\everymath{\displaystyle}
\newtheorem{ejemplo}{{Ejemplo }}[section]
\newtheorem{defi}{{Definici\'on}}[section]
\newtheorem{pros}{{Proposici\'on}}[section]
\newtheorem{cor}{{Corolario}}[section]
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=2,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=false]
 {hyperref}
\hypersetup{
 pdfstartview={XYZ null null 1}}
\begin{document}

<<setup, include=FALSE, cache=FALSE>>=
library(knitr)
library(ggplot2)
library(grid)
#library(animation)
# set global chunk options
opts_chunk$set(fig.path='figure/minimal-', fig.align='center', fig.show='hold')
options(formatR.arrow=TRUE,width=90)
@

%\title{\underline{\textbf{Notas de mat\'ematica}}}
%\date{}
%\maketitle
\hspace*{0.35\linewidth}
\begin{minipage}{0.6\linewidth}
\textbf{Curso: Introducci\'on a la Probabilidad y Estad\'istica CM -274}\par
 \par
\end{minipage}
\vspace{0.5cm}

%\maketitle
\textbf{\Large{Respuestas al examen final}}


%{\normalsize Los c\'odigos, se presentaran impresos,  o como un archivo con extensi\'on $.R$, mostrando ejemplos de su ejecuci\'on.}
\setlength{\unitlength}{1in}

\begin{picture}(6,.1) 
\put(0,0) {\line(1,0){6.25}}         
\end{picture}

\vspace{0.5cm}

\textbf{Soluci\'on 1}

\begin{itemize}
\item  Sea $F$ la funci\'on de distribuci\'on de $\vert X - \mu \vert$. $F(t) =0 $ si $t < 0$, para $t \geq 0$.

\begin{align*}
F(t) &= \mathbb{P}(\vert X - \mu \vert \leq t) = \mathbb{P}(-t \leq X -\mu \leq t) \\
 &= \mathbb{P}(\mu -t \leq X \leq \mu + t)  = \mathbb{P}\biggl(-\frac{t}{\sigma} \leq \frac{X - \mu}{\sigma} \leq \frac{t}{\sigma} \biggr)\\
 &= \Phi\Biggl(\frac{t}{\sigma}\Biggr) - \Phi\Biggl(-\frac{t}{\sigma}\Biggr) = \Phi\biggl(\frac{t}{\sigma}\biggr)- \biggl[1 - \Phi\biggl(\frac{t}{\sigma}\biggr)\Biggr] = 2\Phi\Biggl(\frac{t}{\sigma}\Biggr) -1.
\end{align*}

Por tanto,

\[
F(t) = \begin{cases}
2\Phi\Biggl(\frac{t}{\sigma} \Biggr) -1 &  t\geq 0\\
0 & \text{otros casos}.
\end{cases}
\]

Esto produce,

\[
F^{'}(t) = \frac{2}{\sigma}\Phi^{'}\Biggl(\frac{t}{\sigma} \Biggr) \qquad t \geq 0.
\]

As\'i,

\[
\mathbb{E}(\vert X -\mu\vert ) = \bigints_{0}^{\infty}t\frac{2}{\sigma}\Phi^{'}\Biggl(\frac{t}{\sigma}\Biggr)dt.
\]

Sustituyendo $u = t/\sigma$, obtenemos,

\begin{align*}
\mathbb{E}(\vert X -\mu\vert ) &= \bigints_{0}^{\infty}u\Phi^{'}(u)du = \frac{2\sigma}{\sqrt{2\pi}}\bigints_{0}^{\infty}ue^{-u^2/2}du \\
&= \frac{2\sigma}{\sqrt{2\pi}}\Biggl[-e^{-u^2/2}\Biggr]_{0}^{\infty} = \frac{2\sigma}{\sqrt{2\pi}} = \sigma\sqrt{\frac{2}{\pi}}.
\end{align*}

\item Podemos suponer sin p\'erdida de generalidad, que $\mu = 0$ y $\sigma = 1$. Suponemos que $m > 1$. En este caso al menos la mitad de la masa de probabilidad est\'a a la derecha de $1$, as\'i $\mathbb{E}(XI_{\{X \geq m \}}) \geq \frac{1}{2}$. Ahora $0 = \mathbb{E}(X) = \mathbb{E}(X[I_{\{X \geq m \}}) + I_{\{X < m \}}] )$, implicando que $\mathbb{E}(XI_{\{X < m \}}) \leq -\frac{1}{2}$. Igualmente,

\[
\mathbb{E}(X^2I_{\{X \geq m \}}) \geq \frac{1}{2}, \qquad \mathbb{E}(X^2I_{\{X < m \}}) \leq \frac{1}{2}
\]

Por definici\'on de la mediana  y el hecho de que $X$ es continua,

\[
\mathbb{E}(X| X < m) \leq -1, \qquad \mathbb{E}(X^2|X < m) \leq 1.
\]

Resulta que, $\mathbb{V}(X | X < m) \leq 0$, que implica a su vez que condicionada a $(X | X < m)$, $X$ es concentrada en un s\'olo valor. Esto contradice, la continuidad de $X$ y deducimos que $m \leq 1$. La posibilidad $m < -1$, puede ser considerado con la variable $-X$.

\item Considere la funci\'on $w = x, z = (y - \rho x)/\sqrt{1 - \rho^2}$ con inversa $x = w, y = \rho w + z\sqrt{1 - \rho^2}$ y Jacobiano,

\[
J = \begin{bmatrix}
1 & 0 \\
\rho & \sqrt{1 - \rho^2} 
\end{bmatrix}  = \sqrt{1 -\rho^2}.
\]

La funci\'on es inyectiva y por tanto $W (= X)$ y $Z$ satisfacen,

\[
f_{W,Z}(w,z) = \frac{\sqrt{1 -\rho^2}}{2\pi\sqrt{1 -\rho^2}}\exp\Biggl\{-\frac{1}{2(1 - \rho^2)}(1 - \rho^2)(w^2 + z^2) \Biggr\} = \frac{1}{2\pi}e^{-\frac{1}{2}(w^2 + z^2)},
\]

implicando que $W$ y $Z$ son variables $N(0,1)$ independientes. Ahora,

\[
\{X > 0, Y> 0\} = \Biggl\{W > 0, \ Z > -W\rho/\sqrt{1 - \rho^2}\Biggr\},
\]

Y por tanto moviendo a coordenadas polares,

\[
\mathbb{P}(X > 0, Y > 0) = \bigints_{\theta = \alpha}^{\frac{1}{2}\pi}\bigints_{r = 0}^{\infty}\frac{1}{2\pi}e^{-\frac{1}{2}r^2}rdrd\theta = \bigints_{\alpha}^{\frac{1}{2}\pi}\frac{1}{2\pi}d\theta.
\]

donde $\alpha = -\tan^{-1}(\rho/\sqrt{1 - \rho^2}) = -\sin^{-1}\rho$.

Para la siguiente parte del ejercicio, sea el caso de $\rho \neq 1$. Si escribimos $X= U, Y = \rho U + \sqrt{1 -\rho^2}V$, tenemos que $U$ y $V$ son variables $N(0,1)$ independientes. Se cumple que $Y > X$ si y s\'olo si $(1 - \rho)U < \sqrt{1 - \rho^2}V$. Llevando a coordenadas polares,

\[
\mathbb{E}(\max \{X,Y\}) = \bigints_{0}^{\infty}\frac{re^{-\frac{1}{2}r^2}}{2\pi}\Biggl[\bigints_{\psi}^{\psi + \pi} \Biggl\{ \rho r\cos \theta + r\sqrt{1 - \rho^2}\sin \theta \Biggr\}\Biggr]d\theta
\]

donde $\tan \psi = \sqrt{(1 - \rho)/(1 + \rho)}$, lo que conduce al resultado. Para la segunda parte,

\[
\mathbb{E}(\max \{X,Y\}^2) = \mathbb{E}(X^2I_{\{X > Y\}}) + \mathbb{E}(Y^2I_{\{Y > X\}}) = \mathbb{E}(X^2I_{\{X < Y\}}) + \mathbb{E}(Y^2I_{\{Y > X\}}),
\]

por la simetr\'ia de las marginales de $X$ e $Y$. A\~nadiendo, obtenemos $2\mathbb{E}(\max \{X,Y\}^2) = \mathbb{E}(X^2) + \mathbb{E}(Y^2) = 2$.
\end{itemize}


\textbf{Soluci\'on 2}

\begin{itemize}

\item Colocando $x = yu$, tenemos:

\begin{align*}
f_Y(y) &= ce^{-y}\bigints_{0}^{y}x^{n_1 -1}(y - x)^{n_2 -1}dx = ce^{-y}y^{n_1 + n_2 -1}\bigints_{0}^{1}u^{n_1 + n_2 -1}(1 - u)^{n_2 -1}du \\
&=cy^{n_1 + n_2 -1}e^{-y}\frac{\Gamma(n_1)\Gamma(n_2)}{\Gamma(n_1 + n_2)},
\end{align*}

y desde la ecuaci\'on:

\[
\bigints_{0}^{\infty}f_Y(y)dy =c\Gamma(n_1)\Gamma(n_2),
\]

tenemos, $c^{-1} = \Gamma(n_1)\Gamma(n_2)$, donde $F_Y(y)$ es una distribuci\'on gamma con par\'ametros $n_1 + n_2$. Similarmente, podemos encontrar que $X$ tiene una distribuci\'on gamma con parametro $n_1$:

\[
f_X(x) = cx^{n_1 -1}\bigints_{x}^{\infty}(y -x)^{n_2 -1}e^{-y}dy = \frac{1}{\Gamma(n_1)}x^{n_1 -1}e^{-x}, \quad x >0.
\]


\item (a) La densidad de $Y = X_1 + X_2$ es dado por:
\begin{align*}
f_Y(y) &= \bigints_{-\infty}^{\infty}f_{X_1}(y -x_2)f_{X_2}(x_2)dx_2 \\
&= \begin{cases}
\bigints_{0}^y dx_2 = y \qquad \qquad \qquad \qquad 0 \leq y \leq 1 \\
\bigints_{y -1}^1 dx_2 = 1 - (y -1) = 2 -y  \qquad 1 \leq y \leq 2,
\end{cases}
\end{align*}

esto es,

\[
f_Y(y) = 1 - \vert 1 -y \vert, \qquad  0\leq y \leq 2.
\]

(b) La densidad de $Z = X_1 - X_2$, es dada por:

\[
f_Z(z) = \bigints_{-\infty}^{\infty}f_{X_1}(z + x_2)f_{X_2}(x_2)dx_2 = \begin{cases}
\bigints_{-z}^1 dx_2 = 1 +z \qquad  -1\leq z \leq 0, \\
\bigints_{0}^{1-z} dx_2 = 1 -z  \qquad 0 \leq z \leq 1,
\end{cases}
\]

esto es,

\begin{equation}
f_Z(z) = 1 -\vert z \vert, \qquad  -1 \leq z \leq 1.
\end{equation}

(c) 

\[
W = \vert X_1 -X_2 \vert = \vert Z \vert.
\]

Tenemos,

\[
F_W(w) = \mathbb{P}(W \leq w) = \mathbb{P}(\vert Z \vert \leq w) = \mathbb{P}(- w \leq Z \leq w) = F_Z{w} - F_{Z}(-w),
\]

y as\'i,

\[
f_W(w) = f_Z(w) + f_Z(-w),
\]

Por el resultado (1),tenemos que,

\[
f_W(w) = (1 - \vert w \vert) + (1 - \vert w \vert) = 2(1 - w), \qquad 0 \leq w \leq 1.
\]

(d) La densidad de $V = X_1/X_2$ es dado por:

\[
f_V(v) = \bigints_{-\infty}^{\infty}\vert x_2\vert f_{x_1}(x_2v)f_{x_2}(x_2)dx_2 = \begin{cases}
\bigints_{0}^{1}x_2dx_2 = \frac{1}{2}, \qquad 0 \leq v \leq 1, \\
\bigints_{0}^{1/v}x_2dx_2 = \frac{1}{2v^2}, \qquad   v \geq 1,
\end{cases}
\]
\item Sea $Y = \sum_{j =1}^{n}2 \log X_j = \sum_{j =1}^{n}Y_j$, donde:

\[
Y_j = -2\log X_j, \qquad j =1,2, \dots, n,
\]

variables aleatorias independientes con densidad:

\[
f_{Y_j}(y_j) = f_{X_j}(e^{-y_j/2})\Biggl\vert \frac{d}{dy_j}e^{-y_j/2}\Biggr\vert  = \frac{1}{2}e^{-y_j/2}, \qquad y_j > 0. 
\]

As\'i $Y_j$ tiene la distribuci\'on Gamma con par\'ametros $\lambda = 1/2$ y $s =1$. Pero la distribuci\'on Gamma tiene la propiedad de que $Y$, es la suma de $n$ variables aleatorias independientes e id\'enticamente distribuidas y por tanto tiene una distribuci\'on Gamma con par\'ametros $\lambda = 1/2$ y $s = n$, esto es,

\[
f_Y(y) = \frac{1}{2^n\Gamma(n)}y^{n -1}e^{-y/2}, \qquad y >0,
\]

Esto es, $Y$ tiene una distribuci\'on $\chi^2$ con $2n$ grados de libertad.  Para el siguiente problema, la funci\'on densidad conjunta de variables aleatorias,

\[
\xi = \sqrt{-2 \log X_1}\cos 2\pi X_2, \qquad \eta = \sqrt{-2 \log X_1}\sin 2\pi X_2,
\]


es dado por,

\[
f(\xi, \eta) =f_{(x_1, x_2)}(x_1(\xi, \eta), x_2(\xi,\eta))\vert J(\xi, \eta)\vert.
\]

Pero,

\[
J(\xi, \eta) = \frac{1}{2\pi}e^{-(\xi^2 + \eta^2)/2}
\]

y as\'i,

\[
f(\xi, \eta) = \frac{1}{\sqrt{2\pi}}e^{-\xi^2/2} \frac{1}{\sqrt{2\pi}}e^{-\eta^2/2}, \qquad -\infty < \xi < \infty, \quad -\infty < \eta < \infty.
\]

Por el ejercicio anterior, se deduce que $\xi$ y $\eta$ son variables aleatorias $N(0,1)$ independientes.

\item Desde la definici\'on de funci\'on distribuci\'on, tenemos:

\begin{align*}
F_Z(z) &= \mathbb{P}(Z \leq z) = \mathbb{P}(X \leq z, Y \leq z) = F(z, z), \\
F_W(w) &= \mathbb{P}(W \leq w) = 1 - \mathbb{P}(W > w) = 1 - \mathbb{P}(X > w, Y > w)
\end{align*}

Usando el teorema de adici\'on,

\[
\mathbb{P}(A \cup B) = \mathbb{P}(A) + \mathbb{P}(B) - \mathbb{P}(A\cap B),
\]

 con $A = \{X \leq w \}, B = \{Y \leq w\}$, tenemos:
 
 \begin{align*}
 1 - \mathbb{P}(X > w, Y > w) &= \mathbb{P}(X \leq w) + \mathbb{P}(Y \leq w) - \mathbb{P}(X \leq w, Y\leq w)\\
 &= F_X(w) + F_Y(w) -F(w, w)
 \end{align*}
 
 Si $F$ es continua, obtenemos las densidades de $Z$ y $W$, tomando las derivadas de las correspondientes funciones de distribuci\'on:
 
 \begin{align*}
 f_Z(z) &= \frac{d}{dz}F(z,z) = \frac{d}{dz}\bigints_{-\infty}^{z}\bigints_{-\infty}^{z}f(x,y)dx dy\\
 &= \bigints_{-\infty}^{z}f(x,z)dx + \bigints_{-\infty}^{z}f(z, y)dy,
 \end{align*}
 
 y
 
 \[
 f_W(w) = f_X(w) + f_Y(w) - \bigints_{-\infty}^{w}f(x, w)dx - \bigints_{-\infty}^{w}f(w,y)dy.
 \]
\end{itemize}

\textbf{Soluci\'on 3}

\begin{itemize}
\item De los resultados anteriores $f_1(x)$, la funci\'on densidad de probabilidad de $X_{(1)}$,  est\'a dada por,

\[
f_1(x) = \frac{2!}{(1 - 1)!(2 -1 )!}\lambda e^{-\lambda x}(1 -e^{-\lambda x})^{1 -1}(e^{-\lambda x})^{2 -1}  = 2\lambda e^{-2\lambda x}, \qquad x \geq 0.
\]

Tambi\'en se tiene, que la funci\'on densidad conjunta $f_{12}(x, y)$ de $X_{(1)}$ y $X_{(2}$, es dado por,

\begin{align*}
f_{12}(x, y) &= \frac{2!}{(1 -1)!(2 -1 -1)!(2 - 2)!}\lambda e^{-\lambda x}\lambda e^{-\lambda y}\\
&= (1 -e^{-\lambda x})^{1 -1}(e^{-\lambda x} - e^{-\lambda y})^{2 -1 -1} = 2\lambda^2e^{-\lambda (x + y)}, \quad 0 \leq x < y < \infty.
\end{align*}

Sea $U = X_{(1)}$ y $V = X_{(2)} -X_{(1)}$. Mostraremos que $g(u,v)$, la funci\'on  densidad de probabilidad conjunta de $U$ y $V$, cumplen que $g(u, v) = g_U(u)g_V(v)$. Esto prueba que $U$ y $V$ son independientes. Para encontrar $g(u,v)$, debes notar que el sistema de dos ecuaciones, con dos inc\'ognitas:

\[
\begin{cases}
x = u \\
y - x = v
\end{cases}
\]

Definiendo una transformaci\'on inyectiva desde,

\[
R = \{(x, y ): 0 \leq x  < y < \infty\},
\]

a la regi\'on:

\[
Q = \{(u, v): u \geq 0, v > 0 \}.
\]

El sistema tiene soluci\'on \'unica: $x = u, y = u + v$. As\'i:

\[
J = \begin{bmatrix}
1 & 0 \\
1 & 1
\end{bmatrix} = 1 \neq 0.
\]

Por el teorema de cambio de variable:

\[
g(u,v) =f_{12}(u, u + v)\vert J\vert = 2\lambda^2e^{-\lambda(u + 2v)}, \quad u \geq 0, v \geq 0.
\]

Desde que,

\[
g(u, v) = g_U(u)g_V(v),
\]

donde,

\[
g_U(u) = 2\lambda e^{-2\lambda u}, \qquad u \geq 0,
\]

y

\[
g_V(v) = \lambda e^{-\lambda v}, \qquad v > 0,
\]

tenemos que $U$ y $V$ son independientes. Adem\'as, $U$ es exponencial con par\'ametro $2\lambda$ y $V$ es exponencial con param\'etro $\lambda$.

\item Los estad\'isticos de orden de los $X_i$ tienen funci\'on  densidad conjunta,

\[
h(x_1, x_2,\dots, x_n) = \lambda^nn!\exp\Biggl(-\sum_{i =1}^n \Biggr),
\]

sobre el conjunto $I$ de la sucesiones crecientes de los reales positivos. Definimos una funci\'on inyectiva de $I$ a $(0,\infty)^n$ como,

\[
y_1 = nx_1, y_r = (n + 1-r)(x_r -x_{r -1})\qquad 1 < r \leq n,
\]

con inversa $x_r = \sum_{k =1}^{r}y_k/(n -k +1)$ para $r \geq 1$. El jacobiano es $(n!)^{-1}$, de donde la densidad conjunta de $Y_1, Y_2,\cdots Y_n$ es,


\[
\frac{1}{n!}\exp\Biggl(-\sum_{i =1}^{n}x_i(y)\Biggr) = \lambda^n\exp\Biggl(-\sum_{k =1}^{n}y_k\Biggr).
\]
\end{itemize}

\textbf{Soluci\'on 4}

\begin{itemize}
\item Encontramos la constance $c$ usando la ecuaci\'on,

\[
1 = M_X(0) = c\cdot \frac{3 + 4 + 2}{3 -1},
\]

As\'i obtenemos $c = 2/9$. Luego obtenemos,

\[
\mathbb{E}(X) = \frac{dM_X}{ds}(s)\Biggl\vert_{s = 0} = \frac{2}{9}\cdot\frac{(3 -e^s)(8e^{2s} + 6e^{3s}) + e^s(3  + 4e^{2s} + 2e^{3s})}{(3 -e^s)^2}\Biggl\vert_{s = 0} =\frac{37}{18}.
\]

Ahora usamos la identidad,

\[
\frac{1}{3 -e^s}= \frac{1}{3}\cdot\frac{1}{1 -e^s/3} = \frac{1}{3}\Biggl(1 + \frac{e^s}{3} + \frac{e^{2s}}{9} + \cdots \Biggr),
\]

que  es v\'alido siempre y cuando $s$ sea suficientemente peque\~no para que $e^s < 3$. Se sigue que,


\[
M_X(s) = \frac{2}{9}\cdot\frac{1}{3}\cdot(3 + 4e^{2s} + 2e^{3s})\cdot\Biggl(1 + \frac{e^s}{3} + \frac{e^{2s}}{9} + \cdots \Biggr)
\]

Identificando los coeficientes de $e^{0s}$ y $e^{s}$, obtenemos,

\[
p_X(0) = \frac{2}{9}, \qquad \qquad p_X(1) = \frac{2}{27}.
\]
\item Para cada vector $v$, la funci\'on:

\[
g_{v}(X) = v^{T}G(X)v
\]

es una funci\'on escalar convexa de la matriz $X$ y la desigualdad de Jensen, tenemos:

\[
g_{v}(\mathbb{E}X) \leq \mathbb{E}[g_v(X)],
\]

esto es, para cada $v$,

\[
v^{T}G(\mathbb{E}X)v^{T} \leq Ev^{T}G(X)v.
\]

As\'i la relaci\'on,

\[
G(\mathbb{E}X) \leq \mathbb{E}[G(X)]
\]

Para el siguiente caso, usamos el resultado anterior, mostremos que la funci\'on matricial $G(X) = X^{-1}$ es convexa. Para esto debemos mostrar que las matrices sim\'etricas $X > 0$, $Y > 0$, de orden $r$ y $0 \leq \lambda \leq 1$, la relaci\'on,

\[
[\lambda X + (1 -\lambda)Y]^{-1} \leq \lambda X^{-1} + (1 -\lambda)Y^{-1},
\]

se cumple. Es decir, debemos tener una matriz no singular $A$, tal que $X = AA^{T}, Y = ADA^{T}$, donde $D$ es una matriz diagonal, con elementos $d_1, d_2, \dots, d_r$. As\'i la ecuacion anterior es equivalente a,

\[
[\lambda I + (1 -\lambda)D]^{-1} \leq \lambda I + (1 -\lambda)D^{-1},
\]

que se cumple para matrices diagonales. El resultado se cumple de la convexidad de la funci\'on escalar $y = x^{-1}$. Se debe notar que la desigualdad estricta se cumple si $0 \leq \lambda \leq 1$.
\item Como $(d^2/dy^2)\log y = -1/y^2 < 0$, se sigue que $g(y) = \log y$ es una funci\'on convexa, por tanto por la desigualdad de Jensen, se tiene:

\[
\mathbb{E}[\log Y] \leq \log \mathbb{E}(Y).
\]

Si consideramos la variable aleatoria $Y_n = \sum_{i =1}^n \log X_i$, donde $X_1, X_2,\dots X_n$ son id\'enticamente distribuidas e independientes. Se sigue que $\mathbb{E}(Y_n) = n\mathbb{E}[\log X_i]$ y $\mathbb{V}(Y_n) = n\mathbb{V}(\log X_i)$. Por la desigualdad de Chebyshev, para cada $\epsilon > 0$,

\[
\mathbb{P}[\vert Y_n - \mathbb{E}(Y_n) \vert < n\epsilon] \geq 1 - \frac{\mathbb{V}(Y_n)}{n^2\epsilon^2},
\]

o

\[
\mathbb{P}[n(\mathbb{E}(\log X_i)  -\epsilon)] < \sum_{i =1}^{n}\log X_i < n[\mathbb{E}(\log X_i) + \epsilon] \geq 1 - \frac{\sigma^2}{e\epsilon^2},
\]

o

\[
\mathbb{E}[\exp \{n(\mathbb{E}(\log X_i) - \epsilon )\}] < X_1X_2\dots X_n < \exp\{n(\mathbb{E}(\log X_i)  \epsilon)\} \geq 1 - \frac{\sigma^2}{e\epsilon^2}.
\]

As\'i y por el ejercicio anterior, $\mathbb{E}[\log X] \leq \log \mathbb{E}(X)$, se obtienen  los  resultados pedidos.
\end{itemize}

\textbf{Soluci\'on 5}

\begin{itemize}
\item $X_n$ converge a $0$ en probabilidad, ya que para cada $\epsilon > 0$, $\mathbb{P}(\vert X_n - 0 \vert \geq \epsilon)$ es la probabilidad que el punto aleatorio seleccionado desde $[0,1]$ est\'e en $\Biggl[\frac{i}{2^k}, \frac{i +1}{2^k}\Biggr]$. Ahora $n \rightarrow \infty$ implica que $2^k \rightarrow \infty$ y la longitud del intervalo $\Biggl[\frac{i}{2^k}, \frac{i +1}{2^k}\Biggr] \rightarrow 0$. Por tanto, $\lim_{n \rightarrow \infty}\mathbb{P}(\vert X_n - 0 \vert \geq \epsilon) = 0$.

\vspace{0.2cm}

Sin embargo, $X_n$ no converge a ning\'un punto, ya que para todo entero positivo $N$, existe un $m > N$ y $n > M$ tal que $X_m = 0$ y $X_n = 1$, haciendo imposible para $\vert X_n -X_m\vert $ ser menor que un  $0 < \epsilon < 1$.

\item Desde una de las variantes de la desigualdad de Chebyshev, tenemos:

\begin{align*}
\mathbb{V}(\overline{X}_n) = \frac{1}{n^2}\Biggl \{\sum_{i =1}^{n}\mathbb{V}(X_i) + 2\sum_{k =1}^{n -1}\texttt{Cov}(X_k, X_{k +1}) \Biggr\}\\
\leq \frac{1}{n^2}\{nM + 2(n -1)M\}\rightarrow 0, \qquad \text{si}\ n \rightarrow \infty.
\end{align*}

\item Por la ley de las grandes n\'umeros,

\[
\frac{X_1 + X_2 + \cdots X_n}{n}\rightarrow 2,
\]

con probabilidad $1$ y,

\[
\frac{Y_1 + Y_2 + \cdots Y_n}{n}\rightarrow 3,
\]

con probabilidad $1$, cuando $n \rightarrow \infty$.Se debe notar que si dos eventos $A$ y $B$ tienen ambos probabilidad $1$, entonces el evento $A\cap B$ tambi\'en tiene probabilidad $1$, tanto la convergencia que implica $X_i$ y la que implica $Y_i$ ocurren. Por tanto,

\[
\frac{X_1 +X_2 + \cdots +X_n}{Y_1 +Y_2 + \cdots Y_n} = \frac{(X_1 +X_2 + \cdots +X_n)/n}{(Y_1 +Y_2 + \cdots Y_n)/n} \rightarrow \frac{2}{3},
\]

con probabilidad $1$. Cuando $n \rightarrow \infty$. No era necesario suponer que los $X_i$ son independientes de los $Y_j$ debido a la convergencia puntual con probabilidad $1$.
\end{itemize}

\textbf{Soluci\'on 6}

\begin{itemize}
\item La variable aleatoria $X_i^2$ es gamma con par\'ametros $\lambda = 1/2$  y $r = 1/2$. Por tanto,

\[
\mu = \mathbb{E}(X_i^2) = \frac{r}{\lambda} = 1
\]

y 

\[
\sigma^2 = \mathbb{V}(X_i^2) =  \frac{r}{\lambda^2} = 2 
\]

Por el teorema del l\'imite central,

\begin{align*}
\lim_{n \rightarrow \infty}\mathbb{P}(S_n \leq n + \sqrt{2n}) &= \lim_{n \rightarrow \infty}\mathbb{P}\Biggl(\frac{S_n - n}{\sqrt{2n}}\leq 1\Biggr)\\
&= \lim_{n \rightarrow \infty}\mathbb{P}\Biggl(\frac{S_n - n\mu}{\sigma\sqrt{n}} \leq 1\Biggl) = \Phi(1) = 0.8413.
\end{align*}

\item De acuerdo al teorema del l\'imite central,

\[
\mathbb{P}(-1 < S_{n}^{*} < 1) \rightarrow \Phi(1) - \Phi(-1) = 0.68,
\]

mientras que la desigualdad de Chebyshev, no da informaci\'on, desde que,


\[
\mathbb{P}(\vert S_n \vert \geq \delta) \leq \delta^{-2} = 1, \qquad \delta = 1.
\]

Para $k =2$, la desigualdad de Chebyshev, presenta,

\[
\mathbb{P}(-2 < S_{n}^{*} < 2) \geq 0.75, 
\]

mientras que  el teorema del l\'imite central, produce como un l\'imite $\Phi(2) - \Phi(-2) = 0.9544$. De manera similar para $k = 3$, tenemos $0.8888$ y $0.9974$ respectivamente.

\item El segundo momento de los $X_i$ es,

\[
2\bigints_{0}^{e^{-1}}\frac{x^2}{2x(\log x)^2}dx = \bigints_{-\infty}^{-1}\frac{e^{2u}}{u^2}du,
\]

(Se debe sustituir $x = e^u$), una integral finita. Por tanto, las $X$ tienen una media y varianza finita. La funci\'on densidad es sim\'etrica es alrededor de $0$ y as\'i la media es $0$. Por propiedad, si $0 < x < e^{-1}$,

\[
f_2(x)= \bigints_{-e^{-1}}^{e^{-1}}f(y)f(x -y)dy \geq \bigints_{0}^{x}f(y)f(x - y)dy \geq f(x)\bigints_{0}^{x}f(y)dy,
\]

ya que  $f( x - y)$, vista como una funci\'on de $y$, est\'a aumentando en $[0, x]$. As\'i,

\[
f_2(x) \geq \frac{f(x)}{2\log \vert x\vert} = \frac{1}{4\vert x\vert(\log \vert x \vert)^3},
\]

para $0 < x < e^{-1}$. Continuando este procedimiento, obtenemos,

\[
f_n(x) \geq \frac{k_n}{\vert x \vert (\log \vert x \vert)^{n +1}}, \qquad 0 < x < e^{-1}, 
\]

para alguna constante $k_n$. Por tanto $f_n(x) \rightarrow \infty$ cuando $x \rightarrow \infty$ y en particular la funci\'on densidad conjunta de $(X_1 +X_2 + \cdots +X_n)/\sqrt{n}$ no converge a la apropiada densidad normal en el origen.
\end{itemize}
\end{document} 

