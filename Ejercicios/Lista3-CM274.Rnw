\documentclass{article}
\usepackage[sc]{mathpazo}
\usepackage[T1]{fontenc}
\usepackage{bigints}
\usepackage[latin1]{inputenc}
\usepackage{geometry}
\usepackage{amsmath} 
\usepackage{mathtools}
%\usepackage[spanish]{babel}
\usepackage{latexsym}
\geometry{verbose,tmargin=1.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
\usepackage{url}
\usepackage{graphicx}
%\theoremstyle{definition}
\everymath{\displaystyle}
\newtheorem{ejemplo}{{Ejemplo }}[section]
\newtheorem{defi}{{Definici\'on}}[section]
\newtheorem{pros}{{Proposici\'on}}[section]
\newtheorem{cor}{{Corolario}}[section]
\newcommand{\PR}{\mathbb{P}}
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=2,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=false]
 {hyperref}
\hypersetup{
 pdfstartview={XYZ null null 1}}
\begin{document}

<<setup, include=FALSE, cache=FALSE>>=
library(knitr)
library(ggplot2)
library(grid)
#library(animation)
# set global chunk options
opts_chunk$set(fig.path='figure/minimal-', fig.align='center', fig.show='hold')
options(formatR.arrow=TRUE,width=90)
@

%\title{\underline{\textbf{Notas de mat\'ematica}}}
%\date{}
%\maketitle
\hspace*{0.5\linewidth}
\begin{minipage}{0.6\linewidth}
Curso: Introducci\'on a la Probabilidad y Estad\'istica CM -274\par
Problemas 3 \par
\end{minipage}
\vspace{0.5cm}

%\maketitle
\textbf{\Large{Lista de ejercicios}}
%{\normalsize Los c\'odigos, se presentaran impresos,  o como un archivo con extensi\'on $.R$, mostrando ejemplos de su ejecuci\'on.}
\setlength{\unitlength}{1in}

\begin{picture}(6,.1) 
\put(0,0) {\line(1,0){6.25}}         
\end{picture}
\vspace{0.5cm}

\begin{enumerate}
\item \ Si $X \sim N(\mu, \sigma^2)$ entonces $Y = \alpha X + \beta \sim N(\alpha \mu , \alpha^2\sigma^2)$.
\item 

\begin{enumerate}
	\item Si  $X \sim \text{Bernoulli}(p)$, entonces  $\mathbb{E}(X) = p$.
	\item Si  $X \sim \text{Binomial}(n,p )$ entonces $\mathbb{E}(X) = np$.
	\item Si $X \sim \text{Geometrica}(p)$ entonces  $\mathbb{E}(X) = \frac{1}{p}$.
	\item Si  $X \sim \text{Poisson}(\lambda)$ entonces $\mathbb{E}(X) = \lambda$.
	\item Si $X \sim \text{Uniforme}((\alpha, \beta))$ entonces $\mathbb{E}(X) = \frac{\beta + \alpha}{2}$.
	\item Si $X \sim \text{Uniforme}((\alpha, \beta))$ entonces $\mathbb{E}(X) = \frac{\beta + \alpha}{2}$.
	\item Si $X \sim \text{Exponencial}(\lambda )$ entonces $\mathbb{E}(X) = \frac{1}{\lambda}$.
	\item Si $X \sim N(\mu, \sigma^2)$ entonces  $\mathbb{E}(X) = \mu$.
	\item Si  $X \sim \Gamma(\alpha, \lambda)$ entonces $\mathbb{E}(X) = \frac{\alpha}{\lambda}$.
\end{enumerate}
\item 

\begin{enumerate}
	\item Si $X$ es una variable aleatoria discreta con una funci\'on de masa de probabilidad $p(x)$, entonces para una funci\'on $g$ se cumple:
	
	\[
	\mathbb{E}[g(X)] = \sum_{x:p(x) > 0}g(x)p(x).
	\]
\item Si $X$ es una variable aleatoria continua con una funci\'on de masa de probabilidad $f(x)$, entonces para una funci\'on $g$ de valor real, se cumple:
\[
\mathbb{E}[g(X)] = \bigintss_{-\infty}^{\infty}g(x)f(x) dx.
\]
	\item Si $a$ y $b$ son constantes, entonces:
	
	\[
	\mathbb{E}[aX + b] = a\mathbb{E}(X)  + b.
	\]
\end{enumerate}
\item

\begin{enumerate}
\item  Si $X \sim N(\mu, \sigma^2)$ entonces $\mathbb{Var}(X) = \sigma^2$.
\item Prueba que: 

\[
\mathbb{Var}(X) = \mathbb{E}(X^2) - \mathbb{E}(X)^2
\]
\end{enumerate}
\item 

\begin{enumerate}
\item Si $X$ e $Y$ son variables aleatorias continuas, entonces $\mathbb{E}(X + Y ) = \mathbb{E}(X) + \mathbb{E}(Y)$.
\item Sea $X_i$ una variable aleatoria tal que :

\[
X_i = \begin{cases}
1, & \text{i-\'esimo lanzamiento es un \'exito,}\\
0, & \text{i-\'esimo lanzamiento es una falla.}
\end{cases}
\]

Calcula $\mathbb{E}(X)$ donde $X = X_1 +X_2 + \dots + X_n$. ($X \sim \text{Binom}(n,p )$ ).
\end{enumerate}
\item 

\begin{enumerate}
	\item Si $X$ e $Y$ son independientes, entonces para dos funciones $f$ y $g$ se cumple que:
	
	\[
	\mathbb{E}(g(X)h(Y)) = \mathbb{E}(g(X))\mathbb{E}(h(Y))).
	\]
\end{enumerate}

\item

\begin{enumerate}
	\item \texttt{(Desigualdad de Markov)} Si $X$ es una variable aleatoria de valores positivos, entonces para un $a > 0$, se cumple:
	
	\[
	\PR(X \geq a) \leq \frac{\mathbb{E}(X)}{a}
	\]
\item \texttt{(Desigualdad de Chebyshev)} Si $X$ es una variable aleatoria con media $\mu$ y varianza $\sigma^2$, entonces para alg\'un $k > 0$, tenemos:

\[
\PR\{\vert X - \mu \vert \geq k  \} \leq \frac{\sigma^2}{k^2}
\]
\end{enumerate}

\item Considera la densidad gamma con par\'ametros $\alpha$ y $\lambda$,

\[
\frac{\lambda e^{-\lambda t}(\lambda t)^{\alpha -1}}{\Gamma(\alpha)}.
\]

donde $\Gamma(\alpha)$ es de la forma $\Gamma(\alpha) = \bigintss_{0}^{\infty}e^{-x}x^{\alpha - 1}dx $. Prueba:

\begin{itemize}
	\item $\Gamma(1) =  1$
	\item $\Gamma(\alpha ) = (\alpha - 1)\Gamma(\alpha - 1)$
	\item $\Gamma(k) = (k - 1)!$ si $k$ es entero.
	\item $\Gamma(\frac{1}{2}) = \sqrt{\pi}$.
\end{itemize}
\item Suponiendo que $f:[0,1] \rightarrow [0,1]$ es una funci\'on continua. Muestra un \texttt{m\'etodo probabilistico } para evaluar $\bigintsss_{0}^{1}f(x) dx$, usando la \texttt{Ley Fuerte de los Grandes N\'umeros}.

\item 

\begin{enumerate}
	\item Se lanza una moneda en repetidas ocasiones, las caras  que se producen en cada lanzamiento tienen probabilidad $p$. Encuentra la funci\'on generadora  de probabilidad del n\'umero $T$ de lanzamientos antes de que $n$ caras  hayan  aparecido por primera vez.
	\item Encuentra la funci\'on generadora de la funci\'on de masa binomial negativa:
	
	\[
	f(k) = \binom{k - 1}{r - 1}p^r(1 - p)^{k - r}, \ \  \ k = r, r + 1, \dots
	\]
	
	donde $0 < p < 1$ y $r$ es un entero positivos. Deduce la media y la varianza.
\end{enumerate}
\item 
\begin{enumerate}
	\item Sean $X_2, X_3, \dots $ variables aleatorias independientes tal que:
	\[
	\PR(X_n) = \PR(X_n = -n) = \frac{1}{2n \log n},\ \ \PR(X_n = 0) = 1 - \frac{1}{n\log n}
	\]
	
	Muestra que esta secuencia cumple la ley d\'ebil de los grandes n\'umeros, pero no la ley Fuerte, en el sentido  que $n^{-1}\sum_{1}^{n}X_i$ converge a $0$ en probabilidad.
	\item Sean $X_1, X_2, \dots $ variables aleatorias independientes con una funci\'on densidad:
	
	\[
	f(x) = \begin{cases}
	0 & \text{si}\ \ \ \vert x \vert \leq 2,\\
	\frac{c}{x^2\log \vert x \vert } & \text{si}\ \ \vert x \vert > 2
	\end{cases}
	\]
	
	donde $c$ es una constante. Muestra que $X_i$ no tiene media, pero $n^{-1}\sum_{1}^{n}X_i \rightarrow 0$ en probabilidad cuando $n \rightarrow 0$.
\end{enumerate}

\item 

\begin{enumerate}
	\item Sea $X_1, X_2, \cdots $ una secuencia de variables aleatorias Bernoulli con param\'etro $p$, independientes e id\'enticamente distribuidas. Prueba que para un $\epsilon > 0$, se tiene:
	
	\[
	\PR\Bigl( \Bigl\vert \frac{S_n}{n} - p \Bigr\vert \geq \epsilon\Bigr) \leq \frac{1}{4n\epsilon^2}
	\]
	
	donde $S_n = X_1 + X_2 + \cdots X_n$.
	\item Sea $X$ una variable no negativa. Prueba:
	
	\[
	\mathbb{E}(X) \leq [\mathbb{E}(X^2)]^{\frac{1}{2}} \leq [\mathbb{E}(X^3)]^{\frac{1}{3}}\leq \dots
	\]
	
	\item Sea $X$ e $Y$ variables aleatorias independientes, con media $0$ y varianza $1$ y una funci\'on \mbox{generadora} de momentos $M(t)$. Si $X +Y$ y $X -Y$ son independientes, muestra que:
	
	\[
	M(2t) = M(t)^3M(-t)
	\]
	
	y deduce que $X$ e $Y$ tienen una distribuci\'on normal con  media $0$ y varianza $1$ .
\end{enumerate}
\item 

Considera un Proceso de Poisson, con param\'etro $\lambda$ y sea $N(G_i)$ denota el n\'umero de llegadas del proceso durante el intervalo $(t_i, t_i + c_i ]$. Supongamos que tenemos $n$ intervalos, $i = 1,2, \cdots, n$ mutualmente disjuntos. Denotemos la uni\'on de esos intervalos por $G$ y su longitud total $c = c_1 + c_2 + \cdots c_n$. Dado $k_i \geq 0$ y con $k_1 + k_2 + \cdots + k_n$, determina:

\[
\PR\Bigl( N(G_1)= k_1, N(G_2) = k_2, \dots, N(G_n) = k_n | N(G) = k\Bigr).
\]

\begin{figure}[h]
\centering
\includegraphics[scale=0.8]{p1}
\end{figure}

\item  Sean $X_n$ e $Y_n$ con distribuciones, mostradas en el anterior gr\'afico:

\begin{enumerate}
	\item Encuentra la media y la varianza de $X_n$ y $Y_n$.
	\item ?` Qu\'e nos dice la desigualdad de Chebyshev acerca de la convergencia de $X_n$ e $Y_n$?.
	\item ?` Es $Y_n$ convergente en probabilidad?. ?` Cu\'al es el valor si es que es convergente en \mbox{probabilidad}?.
	\item ?`Si una secuencia de variables aleatorias converge en probabilidad a $a$, entonces la secuencia de valores esperados converge tambi\'en a $a$?. Prueba o da un contraejemplo.
	
	\vspace{0.2cm}
	
	
	Una secuencia de variables aleatorias se dice que converge a un n\'umero $c$ en \textit{media cuadr\'atica}, si se cumple:
	
	\[
	\lim_{n \rightarrow \infty}\mathbb{E}[(X_n - c)^2] = 0
	\]
	
	
	\item Usa la desigualdad de Markov, para probar que la cobvergencia en media cuadr\'atica implica convergencia en probabilidad.
	\item Da un ejemplo que muestra que la convergencia en probabilidad no implica convergencia en media cuadr\'atica.
\end{enumerate}


\item  Resuelve lo siguiente:

\begin{enumerate}
	\item Prueba que se cumple  $n! \sim \sqrt{2\pi n}n^ne^{-n}$ o lo que es lo mismo:
	
	\[
	\lim_{n \rightarrow \infty}\dfrac{n!}{\sqrt{2\pi n}n^ne^{-n}}
	\]
	
	\item Sea $\{X_1, X_2, \dots  \}$ una secuencia de variables aleatorias normal est\'andar independientes. Sea $S_n = X_1^2 + X_2^2 + \cdots X_n^2$. Encuentra:
	
	\[
	\lim_{n \rightarrow \infty}\mathbb{P}(S_n \leq n + \sqrt{2n}).
	\] 
\end{enumerate}

\item  Si $X$ e $Y$ tienen funci\'on generadora de probabilidad conjunta:

\[
G_{X,Y}(s, t) = \mathbb{E}(s^Xt^Y) = \frac{\{1 - (p_1 + p_2)\}^n}{\{1 - (p_1s + p_2t)\}^n}, \ \  \text{donde}\ \ p_1 + p_2 \leq 1
\]

encuentra las funciones de masa marginales de $X$ e $Y$ y la func\'on  de masa de $X + Y$. Encuentra la probabilidad generadora condicional $G_{X|Y}(s | y) = \mathbb{E}(s^X| Y = y)$ de $X$ dado que $Y = y$. 

\item  Sea una secuencia $X_1, X_2, \dots$ de variables aleatoria binarias tomando valores en el \mbox{conjunto} $\{0, 1\}$. Sea $Y$ una variable aleatoria continua que toma valores en $[0, 1]$. Relacionamos $X$ e $Y$ \mbox{asumiendo} que $Y$ es el n\'umero real cuya representaci\'on binaria es $0.X_1X_2X_3\dots$, es decir:

\[
Y = \sum_{k = 1}^{\infty}2^{-k}X_k
\]

\begin{enumerate}
	\item Suponiendo que $X_i$ forman un proceso de Bernoulli con param\'etro $\frac{1}{2}$. Muestra que $Y$ es \mbox{distribuida} uniformemente (considera la probabilidad del evento $(i - 1)/2^k < Y < i/2^k$, donde $i$ y $k$ son enteros positivos).
	\item Suponiendo que $Y$ es distibuida uniformemente. Muestra que las $X_i$ forman un proceso de \mbox{Bernoulli} con param\'etro $\frac{1}{2}$.
\end{enumerate}

\item Consideremos sobre $(a ,b)$ una variable $X$, tal que :

$$
F_{X}(x) = \begin{cases}
(x -a)/(b -a ) & x\in (a,b) \\
0				& x\leq a\\
1			& 	x \geq b
\end{cases}
$$

y 

$$
f_{X}(x) = \begin{cases}
1/(b -a) & x \in (a, b)\\
0 & \text{en otros casos}.
\end{cases}
$$

Calcula la esperanza y varianza de $X$.

\item Sea $X_1, X_2, \dots, X_n$ son variables aleatorias independientes y supongamos que $X_k$ es de Bernoulli con par\'ametro $p_k$. Muestra que $Y = X_1 +X_2 + \cdots X_n$ tiene media y varianza dada por:

 \[
 \mathbb{E}(Y) = \sum_{1}^{n}p_k, \ \ \ \mathbb{Var(Y)} = \sum_{1}^{n}p_k(1 - p_k)
 \]
\item Sea $\textbf{X} = (X_1, X_2, \dots, X_n)$ un vector de variables aleatorias. La \texttt{matriz de covarianza} $\textbf{V(X)}$ de $\textbf{X}$ es definida como la matriz sim\'etrica $n \times n$ con entradas $(v_{i,j}: 1\leq i, j\leq n)$ dado por $v_{i,j} = \text{cov}(X_i, X_j)$. Muestra que $\det(\textbf{V(X)}) = 0$ si y s\'olo si las $X_i$ son linealmente dependiente con probabilidad $1$, esto es $\mathbb{P}(a_1X_1 + a_2X_2 + \cdots +a_nX_n = b) =1$ para alg\'un $\textbf{a}$ y $b$.
\item Sea $\textbf{X} = (X_1, X_2, \dots, X_n)$ un vector de variables aleatorias cada una con la distribuci\'on de Bernoulli con par\'ametro $p$. Sea $f:\{0, 1\}^n \rightarrow \mathbb{R}$ creciente ($f(\mathbf{x}) \leq f(\mathbf{y})$ siempre que $x_i \leq y_i$ para cada $i$). 

\begin{itemize}
	\item Sea $e(p) = \mathbb{E}(f\textbf{X})$. Muestra que $e(p_1) \leq e(p_2)$ si $p_1 \leq p_2$.
\end{itemize}

\item Sea $X$ y $Y$ variables aleatorias con densidad conjunta $f(x,y) =cx(y -x)e^{-y}, \ 0 \leq x \leq y \infty$.

\begin{enumerate}
	\item Encuentra $c$.
	\item Muestra que:
	
	\begin{align*}
	f_{X|Y}(x | y) = 6x(y -x)y^{-3}, \ \ 0 \leq x \leq y, \\
	f_{Y|X}(y|x) = (y -x)e^{x -y}, \ \ \ \ 0 \leq x \leq y \leq \infty
	\end{align*}
\item Calcula $\mathbb{E}(X|Y) =\frac{1}{2}Y$ y  $\mathbb{E}(Y|X) = X +2$.
\end{enumerate}
\item Si $X$ es una variable aleatoria discreta y $g: \mathbb{R} \rightarrow \mathbb{R}$, prueba que: 

$$
\mathbb{E}(g(X)) = \sum_{x \in Im\ X}g(x)\mathbb{P}(X =x).
$$

siempre que esta suma converga absolutamente.
\item Prueba el siguiente resultado:

\begin{enumerate}
	\item Para una variable aleatoria no negativa $X$ se cumple:
	
	\[
	\mathbb{E}(X) = \bigints_{0}^{\infty}\mathbb{P}(X >t)dt
	\]
	\item Si $X$ es una variable aleatoria tomando los valores $\mathbb{N} = \{0,1,2, \dots\}$, entonces:

\[
\mathbb{E}(X) =\sum_{n = 0}^{\infty}\mathbb{P}(X >n)
\]

\end{enumerate}
\item Sea el n\'umero de \'exitos en $n$ pruebas de Bernoulli, donde la probabilidad de \'exito en cada prueba es $p = \frac{1}{2}$. Proporciona un valor n\'umerico para el l\'imite cuando $n$ tiende al infinito para cada una de las siguientes expresiones:

\begin{itemize}
	\item $\PR(\frac{n}{2}- 10 \leq S_n \leq \frac{n}{2} +  10)$
	\item $\PR(\frac{n}{2} - \frac{n}{10} \leq S_n \leq \frac{n}{2} + \frac{n}{10} )$
	\item $\PR(\frac{n}{2} - \frac{\sqrt{n}}{2} \leq S_n \leq \frac{n}{2} + \frac{\sqrt{n}}{2} )$
\end{itemize}

\item 
\begin{enumerate}
	\item Sean $X_1, X_2, \dots, X_n, X_{n + 1}, \dots, X_{2n}$ variables aleatorias id\'enticamente distribuidas e \mbox{independientes}.
	Encuentra:
	
	\[
	\mathbb{E}[X_1|X_1 + X_2 + \dots, X_n = x_0],
	\]
	donde $x_0$ es una constante.
	
	\item Definimos:
	
	\[
	S_k = X_1 + X_2 + \dots + X_{k}, \ \ \  1 \leq k \leq 2n.
	\]
	
	Encuentra:
	
	\[
	\mathbb{E}[X_1| S_n = s_n, S_{n + 1} = s_{n + 1}, \dots S_{2n} = s_{2n}]
	\]
	
	donde $s_n, s_{n + 1}, \dots, s_{2n}$ son constantes.
\end{enumerate}

\item  Sean $X$ e $Y$ variables aleatorias tomando valores enteros positivos, tal que:

\[
\PR(X = k| X + Y = n) = \binom{n}{k}p^k(1 - p)^{n - k}
\]

para alg\'un $p$ y todo $0 \leq k \leq n$. Muestra que $X$ e $Y$ tienes distribuciones de Poisson.

\item 

\begin{enumerate}
	\item Prueba que $\mathbb{E}\{\mathbb{E}(X | Y, Z)| Y  \} = \mathbb{E}(X| Y)$.
	\item Supongamos que $\mathbb{E}\vert X^r \vert <  \infty$, donde $r >0$. Deduce que $x^r\PR(\vert X \vert  \geq x) \rightarrow 0$ cuando $x \rightarrow \infty$. De igual forma si $x^r\PR(\vert X \vert  \geq x) \rightarrow 0$,  cuando $x \rightarrow \infty$  donde $r \geq 0$, muestra que $\mathbb{E}\vert X^s \vert < \infty $ para $0 \leq s <  r $.
	\item Sea $X$ una variable aleatoria que toma valores en el intervalo $[-M, M]$. Muestra que
	
	\[
	\mathbb{P}(\vert X \vert \geq  a) \geq \frac{\mathbb{E}\vert X \vert - a }{M - a}
	\]
	
	si $0 \leq a < M$.
\end{enumerate}
\item   Sea $Y_1, Y_2, \dots$ variables aleatorias independientes, id\'enticamente distribuidas que toma \mbox{valores} en $[0, \infty)$. Si se coloca $Z_0 = 0, Z_1 = Y_1, Z_2 = Y_1 + Y_2, \dots$. Si $Z_n$ es el tiempo del n-\'esima llegada a una tienda (el proceso estoc\'astico $\{ Z_n: n \in \mathbb{N}\}$) es llamado \textit{renewal process}. Sea $N_{t}$ el n\'umero de llegadas durante $(0, t]$.

\begin{enumerate}
	\item Muestra que:
	
	\[
	\PR(N_t \geq n) = \PR(Z_n \leq t)
	\]
	
	para todo $n \in \mathbb{N}$ y $t \in [0, \infty)$.
	
	\item Muestra que para \textit{casi todo } $\omega$,
	
	\[
	\lim_{t \rightarrow \infty}N_t(\omega) = + \infty
	\]
	
	\item Si $Z_{N_{t}}$ es el tiempo de la \'ultima llegada antes del tiempo $t$ y $Z_{N_{t} + 1}$ es el tiempo de la pr\'oxima  llegada despu\'es  del tiempo $t$. Usa la ley fuerte de los grandes n\'umeros y el resultado anterior para probar que:
	
	\[
	\lim_{t \rightarrow \infty}Z_{N_{t}}/N_{t} = a
	\] 
	
	donde $a$ es el valor esperado de $Y$.
\end{enumerate}

\end{enumerate}
\end{document}