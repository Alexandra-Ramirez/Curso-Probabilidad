\documentclass{article}
\usepackage[sc]{mathpazo}
\usepackage[T1]{fontenc}
\usepackage{bigints}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{amsmath} 
\usepackage{mathtools}
\usepackage{latexsym}
\geometry{verbose,tmargin=1.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
\usepackage{url}
\usepackage{bigints}
\usepackage{graphicx}
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=2,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=false]
 {hyperref}
\hypersetup{
 pdfstartview={XYZ null null 1}}
\begin{document}
<<setup, include=FALSE, cache=FALSE>>=
library(knitr)
library(ggplot2)
library(grid)
library(animation)
# set global chunk options
opts_chunk$set(fig.path='figure/minimal-', fig.align='center', fig.show='hold')
options(formatR.arrow=TRUE,width=90)
@

\title{Introducci\'on a la Estad\'istica y Probabilidades CM-274}
\date{}
\maketitle

\vspace{0.3cm}

{\Large Variables aleatorias multivariadas }


\vspace{0.5cm}

Sean un par de variables aleatorias discretas $X$ y $Y$. Considerando el par  $(X,Y)$ como un vector aleatorio con valores en $\mathbb{R}^2$, definamos el  \texttt{jmf} \texttt{(joint mass function)} de $X$ y $Y$  como la funci\'on $p_{X,Y} : \mathbb{R}^2 \rightarrow [0,1]$ definida por


\vspace{0.3cm}

\[
p_{X,Y}(x,y) = \mathbb{P}(\omega \in \Omega :X(\omega) =x , Y(\omega)=y)
\]

\vspace{0.3cm}

o de forma abreviada :

\vspace{0.3cm}

\[
p_{X,Y}(x,y) = \mathbb{P}(X=x , Y=y)
\]


\vspace{0.5cm}


Algunas propiedades que se deducen de la definici\'on:

\vspace{0.3cm}

\begin{itemize}
\item $p_{X,Y}(x,y) = 0$ a menos que $x \in \texttt{Im X}$ y $y \in \texttt{Im Y}$.
\item  $\displaystyle\sum_{x \in \texttt{Im X}}\displaystyle\sum_{y \in \texttt{Im Y}}p_{X,Y}(x,y) = 1$

\end{itemize}


\vspace{0.3cm}

\textbf{Ejemplo} Sea una distribuci\'on bivariada para dos variables aleatorias $X$ y $Y$ tomando los valores $0$ o $1 $:

\vspace{0.3cm}


\begin{table}[h]
\centering
\begin{tabular}{l|ll|l}
      & Y = 0 & Y = 1 &     \\
      \hline
X = 0 & 1/9   & 2/9   & 1/3 \\
X = 1 & 2/9   & 4/9   & 2/3 \\
\hline
      & 1/3   & 2/3   &  1  
\end{tabular}
\end{table}

\vspace{0.2cm}

As\'i, $p(1,1) = \mathbb{P}(X = 1,y =1) = 4/9$.

\vspace{0.5cm}

Los \texttt{pmf} $p_X$ y $p_Y$ de $X$ y $Y$ pueden ser encontrados desde $p_{X,Y}$ de la siguiente manera


\vspace{0.3cm}

\begin{align} 
p_{X}(x) = \mathbb{P}(X=x) = \sum_{y \in \texttt{Im Y}}\mathbb{P}(X = x, Y=y) = \sum_{y}p_{X,Y}(x,y) 
\end{align}


\vspace{0.2cm}

de manera similar

\vspace{0.2cm}

\begin{align}
p_{Y}(y) = \mathbb{P}(Y=y) = \sum_{x \in \texttt{Im X}}\mathbb{P}(X = x, Y=y) = \sum_{x}p_{X,Y}(x,y)
\end{align}


\vspace{0.3cm}

Estas funciones son llamadas \texttt{funciones marginales} de $X$ e $Y$ respectivamente. Si vemos a $(X,Y)$ puntos aleatorios en el plano, entonces $X$ e $Y$ son la proyecci\'on de este punto sobre los ejes coordenados.

\vspace{0.3cm}
\textbf{Ejemplo} Supongamos que $X$ e $Y$ son variables aleatorias que toman los valores $1,2 $ y $3$ y la probabilidad que el par $(X,Y)$ es igual a $(x ,y)$ es dado por la tabla 

\vspace{0.3cm}

\begin{table}[h]
\centering
\begin{tabular}{l|ll l}
      & x = 1 & x = 2 & x=3     \\
      \hline
y = 1 & 1/12   &3/18    & 1/6 \\
y = 2 & 1/18   & 0      & 5/18 \\
y = 3 & 0      &3/18    & 1/12
\end{tabular}
\end{table}

\vspace{0.3cm}

Entonces para este ejemplo

\vspace{0.2cm}

\begin{align*}
\mathbb{P}(X =3) &= \mathbb{P}(X =3, Y =1) + \mathbb{P}(X =3, Y =2) + \mathbb{P}(X =3, Y =3)\\
 &= \frac{1}{6} + \frac{5}{18} + \frac{1}{12} = \frac{19}{36}
\end{align*}

\vspace{0.2cm}


De manera similar

\vspace{0.2cm}

\[
\mathbb{P}(Y = 2) = \frac{1}{18} + 0 + \frac{5}{18} = \frac{1}{3}.
\]


\vspace{0.3cm}

\textbf{Ejemplo} Supongamos que $p_{X,Y}$ es dado por la tabla siguiente. La distribuci\'on marginal para $X$  corresponde a las filas y la distribuci\'on marginal para $Y$ corresponde  a las columnas.


\vspace{0.3cm}

\begin{table}[h]
\centering
\begin{tabular}{l|ll|l}
      & Y = 0 & Y = 1 &     \\
      \hline
X = 0 & 1/10   & 2/10   & 3/10 \\
X = 1 & 3/10   & 4/10   & 7/10 \\
\hline
      & 4/10   & 6/10   &  1  
\end{tabular}
\end{table}

\vspace{0.2cm}

Por ejemplo, $p_{X}(0) = 3/10$ y $p_{X}(1) = 7/10$.


\vspace{0.5cm}


De manera similar, se aplica a $X = (X_1, X_2, \dots,X_n)$ un vector de variables aleatorias discretas. Para este caso el \texttt{jmf} de $X$ es definido como la funci\'on $p_X$ definido por

\vspace{0.3cm}

\[
p_{X}(x) = \mathbb{P}(X_1 = x_1, X_2 = x_2, \dots, X_n= x_n)
\]

\vspace{0.3cm}

para $x = (x_1, x_2, \dots, x_n) \in \mathbb{R}^n$.


\vspace{0.3cm}


\textbf{La esperanza en el caso multivariado}

\vspace{0.3cm}

Si $X$ e $Y$ son variables aleatorias discretas y sea $g : \mathbb{R}^2 \rightarrow \mathbb{R}$ entonces $Z = g(X,Y)$ es una variable aleatoria tambi\'en, definida formalmente como $Z(\omega) = g(X(\omega),Y(\omega)$ para $\omega \in \mathbb{R}$. La esperanza de $Z$ puede ser calculada directamente desde la funci\'on $p_{X,Y}(x,y) = \mathbb{P}(X = X, Y =y)$ como lo indica el siguiente teorema

\vspace{0.3cm}

\textbf{Teorema 1} Tenemos que

\vspace{0.2cm}

\[
\mathbb{E}(g(X,Y)) = \displaystyle\sum_{x \in \texttt{Im X}}\displaystyle\sum_{y \in \texttt{Im Y}}g(x ,y)\mathbb{P}(X =x, Y=y)
\]

\vspace{0.2cm}

siempre que esta suma converga absolutamente.

\vspace{0.3cm}

Una consecuencia importantes de este teorema, es que el operador $\mathbb{E}$ actua de manera lineal sobre el conjunto de las variables discretas. Es decir si $X$ e $Y$ son variables aleatorias discretas y $a, b \in \mathbb{R}$, entonces

\vspace{0.3cm}

\[
\mathbb{E}(aX + bY) = a\mathbb{E}(X) + b\mathbb{E}(Y)
\]

\vspace{0.3cm}

siempre que $\mathbb{E}(X)$ y  $\mathbb{E}(Y)$ existan. 


\vspace{0.5cm}

\textbf{Independencia de variables aleatorias discretas }

\vspace{0.3cm}

Dos variables aleatorias discretas $X$ e $Y$ son independientes si el par de eventos $\{X =x \}$ y $\{Y =y\}$ son independientes para todo $x, y \in \mathbb{R}$ y se escribe esta condici\'on como 

\vspace{0.3cm}

\[
\mathbb{P}(X =x, Y = y) = \mathbb{P}(X =x)\mathbb{P}(Y =y) \ \ \ \text{para}\ \ x, y \in \mathbb{R}.
\]

\vspace{0.2cm}

Las variables aleatorias que no son independientes, se llaman \texttt{dependientes}.

\vspace{0.3cm}

\textbf{Teorema 2} Las variables aleatorias  discretas $X$ e $Y$ son independientes si y s\'olo si existen funciones $f,g : \mathbb{R} \rightarrow \mathbb{R}$ tal que se cumple que el \texttt{jmf} de $X$ e $Y$ satisface

\vspace{0.3cm}

\begin{align}
p_{X,Y}(x,y) = f(x)g(y)\ \ \ \text{para} \ \ x, y \in \mathbb{R}.
\end{align}


\vspace{0.3cm}

Probemos que si se cumple la condici\'on (3) para las funciones $f$ y $g$ y $x \in \texttt{Im X}$ y $y \in \texttt{Im Y}$ tenemos 

\vspace{0.3cm}

\[
p_{X}(x) = f(x)\displaystyle\sum_{y}g(y), \ \ p_{Y}(y) = g(y)\displaystyle\sum_{x}f(x)
\]

\vspace{0.3cm}

por propiedad

\vspace{0.3cm}

\begin{align*}
1 = \displaystyle\sum_{x,y}p_{X,Y}(x,y) &= \displaystyle\sum_{x,y}f(x)g(y)\\
&= \displaystyle\sum_{x}f(x)\displaystyle\sum_{y}g(y)
\end{align*}

\vspace{0.3cm}

Por tanto 

\vspace{0.3cm}

\begin{align*}
p_{X,Y}(x,y) &= f(x)g(y) =f(x)g(y)\displaystyle\sum_{x}f(x)\displaystyle\sum_{y}g(y)\\
&= p_{X}(x)p_{Y}(y)
\end{align*}

\vspace{0.3cm}

\textbf{Teorema 3} Si $X$ e $Y$ son variables aleatorias discretas independientes con esperanza $\mathbb{E}(X)$ y $\mathbb{E}(Y)$ entonces

\vspace{0.2cm}

\[
\mathbb{E}(XY) = \mathbb{E}(X)\mathbb{E}(Y)
\]

\vspace{0.3cm}

Por el teorema 1:

\vspace{0.2cm}

\begin{align*}
\mathbb{E}(XY) &= \displaystyle\sum_{x, y}xy\mathbb{P}(X = x, Y =y) \\
&= \displaystyle\sum_{x,y}xy\mathbb{P}(X =x)\mathbb{P}(Y =y)\\
&= \displaystyle\sum_{x}x\mathbb{P}(X = x)\displaystyle\sum_{y}y\mathbb{P}(Y = y) = \mathbb{E}(X)\mathbb{E}(Y)
\end{align*}

\vspace{0.3cm}

El inverso de este teorema es falso si $\mathbb{E}(XY) = \mathbb{E}(X)\mathbb{E}(Y)$ entonces $X$ e $Y$ son independientes como indica el siguiente ejemplo

\vspace{0.3cm}


\textbf{Ejemplo} Supongamos que $X$ tiene una distribuci\'on dada por

\vspace{0.2cm}

\[
\mathbb{P}(X = -1) = \mathbb{P}(X = 0) = \mathbb{P}(X =1) = \frac{1}{3}
\]

\vspace{0.2cm}

y $Y$ es dado por

\vspace{0.2cm}

\[
Y = \begin{cases}
 0 & \text{si} \ \ X = 0\\
 1 & \text{si} \ \ X \neq 0
\end{cases}
\]

\vspace{0.3cm}

Si tomamos $\Sigma = \{-1, 0, 1 \}$ y $\mathbb{P}$ dado por $\mathbb{P}(-1) = \mathbb{P}(0) = \mathbb{P}(1) = \frac{1}{3} $ y sea $X(\omega) = \omega, Y(\omega) = \vert \omega \vert$. Entonces $X$ e $Y$ son dependientes:

\vspace{0.3cm}


\[
\mathbb{P}(X = 0, Y = 1) = 0
\]

\vspace{0.3cm}

Pero

\vspace{0.3cm}

\[
\mathbb{P}(X = 0)\mathbb{P}(Y =1) = \frac{1}{3}\cdot\frac{2}{3} = \frac{2}{9}
\]

\vspace{0.3cm}

Por otro lado:

\vspace{0.3cm}

\begin{align*}
\mathbb{E}(XY) = \displaystyle\sum_{x, y}xy\mathbb{P}(X =x, Y =y)\\
= (-1)\cdot\frac{1}{3} + 0\cdot\frac{1}{3} + 1\cdot\frac{1}{3} = 0
\end{align*}

\vspace{0.3cm}

\[
\mathbb{E}(X)\mathbb{E}(Y) = 0\cdot\frac{2}{3} = 0
\]


\vspace{0.5cm}


\textbf{Teorema 4} Las variables aleatorias $X$ e $Y$ son independientes si y s\'olo si

\vspace{0.3cm}

\[
\mathbb{E}(g(X)h(Y)) = \mathbb{E}(g(X))\mathbb{E}(h(Y))
\]

\vspace{0.3cm}

para todas las funciones $g,h: \mathbb{R} \rightarrow \mathbb{R}$ en el cual las esperanzas existen.

\vspace{0.5cm}

Para el caso $X = (X_1, X_2, \dots, X_n)$ de variables aleatorias con $n > 2$. Por ejemplo la familia $X$ es llamada independiente si

\vspace{0.2cm}

\[
\mathbb{P}(X_1 =x_1, \dots, X_n =x_n) = \mathbb{P}(X_1 =x_1)\cdots\mathbb{P}(X_n =x_n)
\]

\vspace{0.2cm}

Adem\'as $X_1, X_2, \dots, X_n$ son independientes

\vspace{0.2cm}

\[
\mathbb{E}(X_1, X_2, \cdots, X_n) = \mathbb{E}(X_1)\mathbb{E}(X_2)\cdots \mathbb{E}(X_n)
\]

\vspace{0.2cm}

La familia $X$ es llamada \texttt{independiente componente a componente) si $X_i$ y $Y_j$ son independientes si $i \leq j$.
}


\vspace{0.5cm}

\textbf{Suma de variables aleatorias}

\vspace{0.2cm}

Si $X$ e $Y$ son variables aleatorias discretas, con un \texttt{jmf}.?` Cu\'al es la funci\'on masa de $Z = X + Y$?. Para este caso $Z$ toma valores de $z$ si y s\'olo si $X =x$ y $Y = z -x$ para alg\'un $x$, entonces

\vspace{0.3cm}


\begin{align*}
\mathbb{P}(Z =z) &= \mathbb{P}\Bigl(\bigcup_{x}(\{X =x \} \cap \{Y = z -x \}) \Bigr)\\
 &= \displaystyle\sum_{x \in \texttt{Im X}}\mathbb{P}(X = x, Y = z -x) \ \ \text{para}\ \ z \in \mathbb{R}
\end{align*}

\vspace{0.3cm}


\textbf{F\'ormula de Convoluci\'on} Si $X$ e $Y$ son variables aleatorias discretas independientes, entonces $Z = X +Y$ tiene un funci\'on masa

\vspace{0.3cm}

\[
\mathbb{P}(Z =z) = \displaystyle\sum_{x \in \texttt{Im X}}\mathbb{P}(X =x)\mathbb{P}(Y = z -x)\ \ \text{para}\ \ z \in \mathbb{R}.
\]

\vspace{0.3cm}


La f\'ormula de convoluci\'on dice que la funci\'on de masa de $X + Y$ es la \texttt{convoluci\'on} de la funci\'on de $X$ e $Y$.


\vspace{0.3cm}

\textbf{Funci\'on indicador} La funci\'on indicador de un evento $A$, denotado por $1_A$ y es dado por

\vspace{0.3cm}

\[
1_{A}(\omega) = \begin{cases}

1 & \text{si}\ \ \omega \in A \\
0 & \text{si}\ \  \omega \notin A
\end{cases}
\]

\vspace{0.3cm}

La funci\'on $1_A$ indica si o no ocurre $A$. Para esta variable aleatoria discreta, la esperanza es dada por

\vspace{0.2cm}

\[
\mathbb{E}(1_A) = \mathbb{P}(A)
\]


\vspace{0.3cm}

Algunas propiedades de la funci\'on indicador

\vspace{0.2cm}

\begin{itemize}
\item $1_{A \cap B} = 1_A1_B$
\item $1_A + 1_{A^c} = 1$
\end{itemize}

\vspace{0.3cm}

\textbf{Ejemplo} Sean $A_1, A_2, \dots, A_n$ eventos y $1_{A_i}$ la funci\'on indicador de $A_i$.

\vspace{0.2cm}

Por propiedad, la uni\'on $A = A_1 \cup A_2 \dots, A_n$ tiene como funci\'on indicador

\vspace{0.3cm}

\[
1_A = 1 - \prod_{i =1}^{n}(1 -1_{A_i})
\]

\vspace{0.3cm}

El producto puede ser expandido y agrupado para obtener

\vspace{0.3cm}

\begin{align*}
1_A = \displaystyle\sum_{i}1_{A_i} -  \displaystyle\sum_{i < j}1_{A_i}1_{A_j} +  \displaystyle\sum_{i < j < k}1_{A_i}1_{A_j}1_{A_k} - \cdots \\
+ (-1)^{n +1}1_{A_1}1_{A_2}\cdots 1_{A_n}
\end{align*}

\vspace{0.3cm}

Tomando esperanzas, obtenemos la \texttt{f\'ormula de inclusi\'on-exclusi\'on}

\vspace{0.3cm}

\begin{align*}
\mathbb{P}\Bigl(\bigcup_{i}A_i \Bigr) = \displaystyle\sum_{i}\mathbb{P}(A_i) - \displaystyle\sum_{i < j}\mathbb{P}(A_i \cap A_j) + \displaystyle\sum_{i< j <k}\mathbb{P}(A_i\cap A_j\cap A_k) - \cdots \\
\cdots + (-1)^{n +1}\mathbb{P}\Bigl(\bigcap_{i}A_i\Bigr)
\end{align*}


\vspace{0.8cm}


En el caso de variables aleatorias en general,  estudiamos el \texttt{jdf (joint distribution function)} del par X, Y de variables aleatorias que se define como la funci\'on $F_{X,Y}: \mathbb{R}^2 \rightarrow [0,1]$ es dado por

\vspace{0.3cm}

\[
F_{X,Y}(x,y) = \mathbb{P}(X \leq x, Y \leq y)
\]

\vspace{0.3cm}

Algunas propiedades importantes de \texttt{jfd} es

\vspace{0.2cm}

\begin{align*}
\lim_{x,y \rightarrow -\infty}F_{X,Y}(x,y) = 0 \\
\lim_{x,y \rightarrow \infty}F_{X,Y}(x,y) = 1
\end{align*}

\vspace{0.2cm}

\begin{itemize}
\item $F_{X,Y}(x_1, y_1) \leq F_{X,Y}(x_, y_2)$ si $x_1 \leq x_2$ y $y_1 \leq y_2$.
\item Se define las funciones marginales de $F_{X,Y}$ como  $F_X(x) = \lim_{y \rightarrow \infty}F_{X,Y}(x,y)$ y  $F_Y(y) = \lim_{x \rightarrow \infty}F_{X,Y}(x,y)$.
\end{itemize}

\vspace{0.3cm}

Las variables aleatorias $X$ e $Y$ son independientes, si para todos los $x,y \in \mathbb{R}$, los eventos $\{ X \leq x\}$ y $\{Y \leq y \}$ son independientes. Es decir $X$ e $Y$ son independientes si y s\'olo si

\vspace{0.3cm}

\[
\mathbb{P}(X \leq x, Y \leq y) = \mathbb{P}(X \leq x)\mathbb{P}(Y\leq y) \ \ \text{para}\ \ x, y \in \mathbb{R}.
\]

\vspace{0.3cm}

lo que dice que el \texttt{jdf} se factoriza como el producto de las dos funciones de distribuci\'on marginal

\vspace{0.3cm}

\[
F_{X,Y}(x,y) = F_{X}(x)F_{Y}(y)\ \ \text{si}\ \ x,y \in \mathbb{R}.
\]

\vspace{0.3cm}

Las variables aleatorias que no son independientes, se llaman dependientes.

\vspace{0.3cm}

En general, si $X_1, X_2, \dots X_n$ son variables aleatorias, el \texttt{jdf}  es la funci\'on $F_X:\mathbb{R}^n\rightarrow [0,1]$ es dado por

\vspace{0.3cm}

\[
F_{X}(x) = \mathbb{P}(X_1 \leq x_1, X_2 \leq x_2, \dots, X_n \leq x_n)
\]

\vspace{0.3cm}

para $x = (x_1, x_2, \dots, x_n \in \mathbb{R}^n$. La variables $X_1, X_2, \dots, X_n$ son independientes

\vspace{0.3cm}

\[
\mathbb{P}(X_1 \leq x_1, X_2 \leq x_2, \dots, X_n \leq x_n) = \mathbb{P}(X_1 \leq x_1)\cdots \mathbb{P}(X_n \leq x_n)
\]

\vspace{0.3cm}

o equivalentemente

\vspace{0.3cm}

\[
F_{X}(x) = F_{X_1}(x_1)\cdots F_{X_n}(x_n)\ \ \text{si}\ \ x\in \mathbb{R}.
\]


\vspace{0.3cm}

\textbf{Ejemplo} Sean $X$ e $Y$ dos variables aleatorias, cada una tomando los valores $\{\dots, -1,0, 1, \dots \}$ con \texttt{jmf}


\vspace{0.3cm}

\[
\mathbb{P}(X =i, Y= j) = p(i,j)\ \ \text{para}\ \ i,j =0,\pm 1, \pm 2, \dots
\]

\vspace{0.3cm}

El \texttt{jdf} es dado por

\vspace{0.3cm}

\[
F_{X,Y}(x,y) = \displaystyle\sum_{i\leq x, j\leq y}p(i,j)\ \ \text{para}\ \ (x, y) \in \mathbb{R}^2
\]


\vspace{0.3cm}

\textbf{Ejemplo} Sean $X$ e $Y$ son variables aleatorias con un \texttt{jdf}

\[
F_{X,Y}(x,y) = \begin{cases}
1 -e^{-x} - e^{-y} + e^{x +y} & \text{si}\ \ x, y \geq 0\\
0 & \text{en otros casos}
\end{cases}
\]

\vspace{0.3cm}

La funci\'on de distribuci\'on marginal de $X$ es

\vspace{0.3cm}

\[
F_{X}(x) = \lim_{y \rightarrow \infty}F_{X,Y}(x,y) = \begin{cases}
1 -e^{-x} & \text{si}\ \ x \geq 0 \\
0 & \text{en otro casos}
\end{cases}.
\]

\vspace{0.3cm}

as\'i $X$ tiene una distribuci\'on exponencial con par\'ametro $1$. Un c\'alculo muestra que $Y$ tiene esta distribuci\'on tambi\'en. As\'i

\vspace{0.3cm}

\[
F_{X,Y}(x,y) = F_{X}(x)F_{Y}(y)\ \ \text{para}\ \ x,y \in \mathbb{R}.
\]

\vspace{0.3cm}

por tanto $X$ e $Y$ son independientes.


\vspace{0.8cm}


El par $(X,Y)$ de variables aleatorias es llamada \textbf{continua conjunta } si su \texttt{jdf} es expresable de la forma

\vspace{0.3cm}

\[
F_{X,Y}(x,y) = \mathbb{P}(X \leq x, Y \leq y) = \bigintsss_{u=-\infty}^{x} \bigintsss_{v=-\infty}^{y}f(u,v)dudv
\]


\vspace{0.3cm}

para $x, y \in \mathbb{R}$ para alguna funci\'on $f:\mathbb{R}^2 \rightarrow [0, \infty)$. Si esto se cumple, decimos que $X$ e $Y$ tienen \texttt{pdf} conjunta $f$ que usualmente se identifica como $f_{X,Y}$.

\vspace{0.3cm}

Si $X$ e $Y$ son continuas conjuntas, podemos tener  el \texttt{pdf}  de la siguiente manera

\vspace{0.3cm}

\[
f_{X,Y}(x,y) = \begin{cases}
\dfrac{\partial ^2}{\partial x \partial y}F_{X,Y}(x,y) & \text{si esta derivada existe en $(x, y)$} \\
0 & \texttt{en otros casos}
\end{cases}
\]


\vspace{0.3cm}


Algunas propiedades elementales de $f_{X,Y}$, son

\vspace{0.2cm}

\begin{itemize}
\item $f(x,y) \geq 0$ para todo $(x,y) \in \mathbb{R}$.
\item $\bigintsss_{\infty}^{\infty}\bigintss_{\infty}^{\infty}f(x,y)dxdy = 1$ y
\end{itemize}
.
\vspace{0.3cm}

\textbf{Ejemplo} Sea $(X,Y)$ uniforma sobre el cuadrado unitario. Entonces


\vspace{0.2cm}

\[
f(x,y) = \begin{cases}
1 & \text{si}\ \ 0 \leq x \leq 1, 0 \leq y \leq 1 \\
0 & \text{en otros casos}
\end{cases}
\]

\vspace{0.2cm}

Encontremos $\mathbb{P}(X < 1/2, Y < 1/2)$.

\vspace{0.2cm}

El evento $A = \{ X < 1/2, Y < 1/2 \}$ corresponde a un subconjunto del cuadrado unidad. Integrando $f$ sobre este conjunto, corresponde a calcular el \'area de conjunto $A$ que es $1/4$. As\'i $\mathbb{P}(X < 1/2, Y < 1/2) = 1/4$.

\textbf{Ejemplo} Sea $(X,Y)$ que tienen una funci\'on de densidad

\vspace{0.2cm}

\[
f(x,y)= \begin{cases}
cx^2y & \mbox{si}\ \  x^2 \leq y \leq 1\\
0 & \mbox{ en otro caso}.
\end{cases}
\]

\vspace{0.2cm}

Notemos primero que $-1 \leq x \leq 1$. Encontremos el valor de $c$, para ello fijemos un valor $x$ y dejemos que $y$ varie en su rango, el cu\'al es $x^2 \leq y \leq 1$.

\vspace{0.3cm}

\begin{figure}[h]
\centering
\includegraphics[scale=.45]{m1.png}
\end{figure}
As\'i

\begin{align*}
1 &= \int\int f(x,y)dxdy = c\int_{-1}^{1}\int_{x^2}^{1}x^2ydydx \\
&= c\int_{-1}^{1}x^2\Bigl[\int_{x^2}^{1}ydy\Bigr]dx = c \int_{-1}^{1}x^2\frac{1 -x^4}{2}dx = \frac{4c}{21}.
\end{align*}


\vspace{0.2cm}


Ahora sea $c = 21/4$. Calculemos $\mathbb{P}(X \geq Y)$.

\vspace{0.2cm}

Esto corresponde al conjunto $A = \{ (x,y); 0 \leq x \leq 1, x^2 \leq y \leq x\}$. (Se puede ver esto, en el diagrama de arriba). As\'i


\begin{align*}
\mathbb{P}(X \geq Y) &= \frac{21}{4}\int_{0}^{1}\int_{x^2}^{x}x^2ydydx = \frac{21}{4}\int_{0}^{1}x^2\Bigl[\int_{x^2}^{x}ydy\Bigr]dx\\\ &= \frac{21}{4}\int_{0}^{1}x^2\Bigl( \frac{x^2 - x^4}{2}\Bigr)dx = \frac{3}{20}.
\end{align*}


\vspace{0.5cm}

Para el par $(x,y) \in \mathbb{R}$ y para $\delta x$ y $\delta y$, la probabilidad que el vector aleatorio $(X,Y)$ est\'a en el peque\~no rect\'angulo con los puntos $(x,y)$ en el extremo izquierdo y de lados $\delta x$ y $\delta y$ es

\vspace{0.3cm}

\[
\mathbb{P}(x < X \leq x + \delta x, y < Y \leq y + \delta y) \approx f_{X,Y}(x,y)\delta x\delta y
\]

\vspace{0.3cm}

Este resultado conduce al siguiente teorema

\vspace{0.3cm}

\textbf{Teorema 5} Si $A$ es un subconjunto \textbf{regular} de $\mathbb{R}^2$ y $X$ e $Y$ son variables aleatorias continuas conjuntas con funci\'on densidad $f_{X,Y}$ entonces

\vspace{0.3cm}

\[
\mathbb{P}((X,Y) \in A) = \bigintss \bigintsss_{(x,y) \in A}f_{X,Y}(x,y)dxdy
\]

\vspace{0.3cm}

\textbf{Ejemplo}La funci\'on $f$  definida por

\vspace{0.3cm}

\[
f(x,y) = \begin{cases}
\frac{1}{ab} & \text{si}\ \ 0 < x < b, 0 < y < b \\
0 & \text{en otros casos}
\end{cases}
\]

\vspace{0.3cm}

es una funci\'on densidad conjunta. Si $X$ e $Y$ tienen como densidad $f$, el vector $(X,Y)$ se dice que es \texttt{uniformemente distribuido} sobre el rect\'angulo $B= (0,a) \times (0,b)$. Para alguna regi\'on del plano

\vspace{0.3cm}

\begin{align*}
\mathbb{P}((X,Y) \in A) &= \bigintsss\bigintsss_{A}f(x,y)dxdy \\
&= \bigintsss\bigintsss_{A \cap B}f(x,y)dxdy = \dfrac{\text{area}(A \cap B)}{\text{area}(B)}
\end{align*}

\vspace{0.5cm}

Siempre que el par $X, Y$ tiene  un \texttt{jdf} $f_{X,Y}$ , las densidades marginales de $X$ e $Y$ en los puntos de diferenciabilidad, son 

\vspace{0.2cm}

\begin{align}
f_{X}(x) = \bigintsss_{v = -\infty}^{\infty} f_{X,Y}(x,v)dv \ \ f_{Y}(y) = \bigintsss_{u = -\infty}^{\infty} f_{X,Y}(u,y)du
\end{align}

\vspace{0.2cm}

Las correspondientes funciones de distribuci\'on marginal, de $X$ e $Y$ son denotadas por $F_X$ y $F_{Y}$ y son las proyecciones del vector aleatorio $(X,Y)$ sobre los ejes coordenados. 

\vspace{0.5cm}

\textbf{Ejemplo} Sea $(X,Y)$ con una funci\'on densidad
\[
f(x,y) = \begin{cases}
\frac{21}{4}x^2y & \mbox{si}\ x^2 \leq y \leq 1 \\
0 & \mbox{ en otros casos}.
\end{cases}
\]

\vspace{0.3cm}

As\'i

\vspace{0.2cm}


\[
f_{X}(x) = \int f(x,y)dy = \frac{21}{4}x^2\int_{x^2}^{1}ydy = \frac{21}{8}x^2(1 - x^4)
\]

\vspace{0.2cm}

para $-1 \leq x \leq 1$ y $f_{X}(x) = 0$ en otros casos.

\vspace{0.8cm}


Las variables aleatorias $X$ e $Y$ son independientes si su funci\'on de distribuci\'on satisface

\vspace{0.3cm}

\[
F_{X,Y}(x ,y) = F_{X}(x)F_{Y}(y) \ \ \text{para}\ \ x, y \in \mathbb{R}
\]


\vspace{0.5cm}
 
 Si $X$ e $Y$ son continuas conjuntas, entonces diferenciando esta relaci\'on con respecto $x$ e $y$ se cumple

\vspace{0.3cm}

\[
f_{X,Y}(x ,y) = f_{X}(x)f_{Y}(y) \ \ \text{para}\ \ x, y \in \mathbb{R}
\]

\vspace{0.5cm}


\textbf{Ejemplo} Sean $X$ y $Y$ tienen la siguiente distribuci\'on

\vspace{0.3cm}

\begin{table}[h]
\centering
\begin{tabular}{l|ll|l}
      & Y = 0 & Y = 1 &     \\
      \hline
X = 0 & 1/4   & 1/4   & 1/2 \\
X = 1 & 1/4   & 1/4   & 1/2 \\
\hline
      & 1/2   & 1/2   &  1  
\end{tabular}
\end{table}

\vspace{0.2cm}

Entonces, $f_{X} (0) = f_{X}(1) = 1/2$ y $f_{Y}(0) = f_{Y}(1) = 1/2$. $X$ y $Y$ son independientes, ya que $f_{X}(0)f_{Y}(0) = f(0,0), f_{X}(0)f_{Y}(1) = f(0,1),  f_{X}(1)f_{Y}(0) = f(1,0), f_{X}(1)f_{Y}(1) = f(1,1)$. Pero si $X$ y $Y$ tienen la misma distribuci\'on

\vspace{0.2cm}

\begin{table}[h]
\centering
\begin{tabular}{l|ll|l}
      & Y = 0 & Y = 1 &     \\
      \hline
X = 0 & 1/2   & 0    & 1/2 \\
X = 1 & 0     & 1/2   & 1/2 \\
\hline
      & 1/2   & 1/2   &  1  
\end{tabular}
\end{table}

\vspace{0.3cm}

ellas no son independientes, ya que $f_{X}(0)f_{Y}(1) = (1/2)(1/2) = 1/4$, pero $f(0,1) = 0$.

\vspace{0.5cm}

\textbf{Ejemplo} Supongamos que $X$ y $Y$ son indepedientes y tienen la misma densidad

\[
f(x) = \begin{cases}
2x & \mbox{si}\  \ 0 \leq x \leq 1\\
0  & \mbox{en otros casos}
\end{cases}
\]

\vspace{0.2cm}

Encontremos $\mathbb{P}(X + Y \leq 1)$. Usando independencia, tenemos que  la \texttt{jdf} es

\[
f(x,y) = f_{X}(x)f_{Y}(y) = \begin{cases}
4xy & \mbox{si}\ \  0 \leq x \leq 1,\ \ 0 \leq y \leq 1\\
0 & \mbox{en otros casos}.
\end{cases}
\]

\vspace{0.5cm}

Ahora,

\vspace{0.3cm}

\begin{align*}
\mathbb{P}(X +  Y \leq 1) &= \bigintsss \bigintsss_{x + y \leq 1}f(x,y)dydx \\
&= 4\bigintsss_{0}^{1}x\Bigl[\bigintsss_{0}^{1-x}ydy \Bigr]dx \\
&=4 \bigintsss_{0}^{1}x\frac{(1 - x)^2}{2}dx = \frac{1}{6}.
\end{align*}

\vspace{0.5cm}

\textbf{Teorema 6}  Supongamos que  $X$ y $Y$ son continuas conjuntas, son independientes si y s\'olo si el \texttt{jdf} puede ser expresado en la forma

\vspace{0.3cm}

\[
f_{X, Y}(x,y) = g(x)h(y) \ \text{para} \ \ x, y \in \mathbb{R}
\]

\vspace{0.3cm}

\textbf{Ejemplo} Sean $X$ y $Y$ con densidad

\vspace{0.3cm}

\[
f(x,y) =\begin{cases}
2e^{-(x + 2y)} & \mbox{si} \ \ x > 0 \ \ y \ \ y > 0\\
0 & \mbox{en otros casos}.
\end{cases}
\]

\vspace{0.3cm}

El rango de $X$ y $Y$ es el rect\'angulo $(0, \infty) \times (0, \infty)$. Podemos escribir $f(x, y) = g(x)h(y)$, donde $g(x) = 2e^{-x}$ y $h(y) = e^{-2y}$. As\'i, $X$ y $Y$ son independientes.

\vspace{0.8cm}

\textbf{Suma de variables aleatorias continuas}

\vspace{0.3cm}

Supongamos que $X$ e $Y$ tienen una densidad $f_{X,Y}$, entonces la densidad de $Z = X +Y$ se puede calcular

\vspace{0.5cm}

\begin{align*}
\mathbb{P}(Z \leq z) = \mathbb{P}(X + Y \leq z)\\
= \bigintsss\bigintsss_{A}f_{X,Y}(x,y)dxdy
\end{align*}

\vspace{0.3cm}

por el \texttt{teorema 5} donde $A = \{(x,y)\in \mathbb{R}^2: x +y \leq z \}$. Escribiendo en los l\'imites de integraci\'on encontramos 

\vspace{0.3cm}


\begin{align*}
\mathbb{P}(Z \leq z) &=\bigintsss_{x = -\infty}^{\infty}\bigintsss_{y = -\infty}^{z -x}f_{X,Y}(x,y)dxdy\\
&= \bigintsss_{v = -\infty}^{z}\bigintsss_{u = -\infty}^{\infty}f_{X,Y}(u,v -u)dudv
\end{align*}

\vspace{0.3cm}

con la sustituci\'on $u = x, v = x^2 + y$. Diferenciando esta ecuaci\'on con respecto a $z$ , se tiene

\vspace{0.3cm}

\[
f_{Z}(z) = \bigintsss_{-\infty}^{\infty}f_{X,Y}(u, z -u)du
\]

Si $X$ y $Y$ son discretas, entonces podemos calcular la distribuci\'on condicional de $X$ dado que hemos observado $Y = y$. Especificamente, $\mathbb{P}(X =x| Y=y) = \mathbb{P}(X = x,Y=y)/\mathbb{P}(Y=y)$. Esto conduce a la definici\'on \texttt{cpmf} o \textit{Conditional probability mass function}.

\vspace{0.4cm}

\textbf{Definici\'on} La \texttt{cpmf} es

\[
f_{X|Y}(x|y) = \mathbb{P}(X = x| Y=y) = \frac{\mathbb{P}(X = x, Y=y)}{\mathbb{P}(Y = y)} = \frac{f_{X,Y}(x,y)}{f_{Y}(y)}
\]

\vspace{0.2cm}

si $f_{Y} > 0$.

\vspace{0.2cm}

\textbf{Definici\'on} Para variables aleatorias continuas, la \texttt{cpdf}, \textit{Conditional probability density function} es

\[
f_{X|Y}(x|y) = \frac{f_{X|Y}(x,y)}{f_{Y}(y)}
\]

\vspace{0.3cm}

asumiendo que $f_{Y}(y) > 0$. Entonces,

\[
\mathbb{P}(X \in A| Y=y) = \int_{A}f_{X|Y}(x|y)dx.
\]

\vspace{0.5cm}

\textbf{Ejemplo} Sean $X$ y $Y$, que tienen una distribuci\'on uniforme sobre un cuadrado unidad. As\'i $f_{X|Y}(x|y) = 1$, para $0 \leq x \leq 1$ y $0$ en otros casos. Dado $Y =y$, $X$ es $\mbox{Uniform}(0.1)$. Podemos escribir esto como $X|Y = y \sim \mbox{Uniform}(0,1)$.

\vspace{0.5cm}

\textbf{Ejemplo} Supongamos que $X \sim \mbox{Uniform}(0,1)$. Despu\'es de obtener un valor de $X$, generamos $Y|X = x \sim \mbox{Uniform}(x,1)$. La distribuci\'on marginal de $Y$ se puede calcular a partir de

\[
f_{X}(x) = \begin{cases}
1 & \mbox{si}\ \ 0 \leq x \leq 1\\
0 & \mbox{ en otros casos}.
\end{cases}
\]

y 

\[
f_{Y|X}(y|x) = \begin{cases}
\frac{1}{1 -x}& \mbox{si}\ \ 0 < x < y < 1 \\
0 & \mbox{en otros casos}.
\end{cases}
\]

\vspace{0.3cm}

As\'i

\[
f_{X,Y}(x,y) = f_{Y|X}(y|x)f_{X}(x) = \begin{cases}
\frac{1}{1 -x}& \mbox{si}\ \ 0 < x < y < 1 \\
0 & \mbox{en otros casos}.
\end{cases}
\]

\vspace{0.3cm}

La distribuci\'on marginal para $Y$ es

\[
f_{Y}(y) = \int_{0}^{y}f_{X,Y}(x,y)dx = \int_{0}^{y}\frac{dx}{1 -x} = -\int_{1}^{1- y}\frac{du}{u} = -\log(1-y)
\]

\vspace{0.3cm}

para $0 < y < 1$.

\vspace{0.8cm}

{\Large Distribuciones Multivariadas y Muestras Id\'enticamente Distribuidas e Independientes}

\vspace{0.5cm}

Sea $X = (X_1,\dots, X_n)$ donde $X_1, \dots, X_n$ son variables aleatorias. Llamamos a $X$ \textbf{Vector Aleatorio}. $f(x_1,\dots,x_n)$ denota el \texttt{PDF}. Es posible definir los mismos conceptos, de la misma manera que las distribuciones multivariadas. Decimos que $X_1,\dots, X_n$ son independientes si, para cada $A_1, \dots, A_n$

\begin{align}
\mathbb{P}(X_1 \in A_1,  \dots, X_n \in A_n) = \prod_{i=1}^{n}\mathbb{P}(X_i \in A_i).
\end{align}

\vspace{0.3cm}

Es suficiente verificar que $f(x_1,\dots, x_n) = \prod_{i = 1}^{n}f_{X_{i}}(x_i) $.


\vspace{0.5cm}

\textbf{Definici\'on} Si $X_1,\dots X_n$ son independientes y cada uno de ellos tiene la misma distribuci\'on marginal con \texttt{CDF} $F$, decimos que $X_1, \dots, X_n$ son id\'enticamente distribuidas e independientes y escribimos como

\vspace{0.3cm}

\[
X_1, \dots, X_n \sim F
\]

\vspace{0.3cm}

Si $F$ tiene densidad $f$, escribimos tambi\'en $X_1, \dots, X_n \sim f$. Llamamos a $X_1, \dots, X_n$ una muestra aleatoria de tama\~no $n$ desde $F$.


\vspace{0.8cm}


{\Large Dos Importantes Distribuciones Multivariadas}

\vspace{0.5cm}

\textbf{Multinomial} La versi\'on multivariada de la Binomial, es llamada \texttt{multinomial}. Considere la posibilidad de extraer una bola de una urna que tiene bolas con $k$ colores, etiquetados como \texttt{'color 1', 'color 2', \dots 'color k'}. Sea $p = (p1, \dots, p_k)$ donde $p_j \geq 0$ y $\sum_{j = 1}^{k}p_j = 1$ y supongamos que $p_j$ es la probabilidad de encontrar una bola de color $j$. Lanzamos $n$ veces (independientes lanzamientos con reemplazamiento) y sea $X = (X_1, \dots, X_k)$ donde $X_j$ es el n\'umero de veces que el color $j$ aparece. Por lo tanto $n = \sum_{j= 1}^{k}X_j$, as\'i decimos que $X$ tiene una distribuci\'on Multinomial $\mbox{Multinomial}(n,p)$. La funci\'on probabilidad es

\begin{align}
f(x) = \binom{n}{x_1\dots x_k}p_{1}^{x_1}\cdots p_{k}^{x_k}
\end{align}

\vspace{0.2cm}

donde 

\vspace{0.2cm}

\begin{align*}
\binom{n}{x_1\dots x_k} = \frac{n!}{x_1!\cdots x_k!}.
\end{align*}

\vspace{0.2cm}

\textbf{Propiedad} Supongamos que $X \sim \mbox{Multinomial(n, p)}$, donde $X = (X_1,\dots, X_k$ y $p = (p_1, \dots, p_k)$. La distribuci\'on marginal de $X_j$ es $\mbox{Binomial}(n, p_j)$.


\vspace{0.5cm}

\textbf{Normal Multivariada} La distribuci\'on normal univariada tiene dos par\'ametros  $\mu$ y $\sigma$. En la versi\'on multivariada, $\mu$ es un vector y $\sigma$ es reemplazada por una matriz $\Sigma$. Para empezar

\[
Z = \begin{pmatrix}
Z_1 \\
\vdots\\
Z_k
\end{pmatrix}
\]

\vspace{0.3cm}

donde $Z_1, \dots, Z_k \sim N(0,1)$ son independientes. La densidad de $Z$ es

\begin{align*}
f(z) = \prod_{i = 1}^{k}f(z_i) = \frac{1}{(2\pi)^{k/2}}\exp\Bigl\{-\frac{1}{2}\sum_{j = 1}^{k}z_j^{2}\Bigr\}\\
= \frac{1}{(2\pi)^{k/2}}\exp\Bigl\{-\frac{1}{2}z^{T}z\Bigr\}.
\end{align*}

\vspace{0.3cm}

Decimos que $Z$ tiene una distribuci\'on multivariada est\'andar escrita como $Z \sim N(0,I)$, donde $0$ representa el vector de $K$ ceros y $I$ es la matriz identidad  de orden $k \times k$. De manera m\'as general, un vector $X$ tiene una distribuci\'on Normal Multivariada, denotada por $X \sim N(\mu, \Sigma)$, si tiene una densidad

\vspace{0.3cm}

\begin{align}
f(x; \mu, \Sigma) = \frac{1}{(2\pi)^{k/2}\vert (\Sigma)\vert^{1/2}}\exp \Bigl\{-\frac{1}{2}(x - \mu)^{T}\Sigma^{-1}(x - \mu) \Bigr\}
\end{align}

\vspace{0.3cm}


donde $\vert \Sigma \vert$, denota el determinante de $\Sigma$, $\mu$ es un vector de longitud $k$ y $\Sigma$ es una matriz sim\'etrica, definida positiva. Colocando $\mu = 0$ y $\Sigma = I$ tenemos la distribuci\'on Normal Est\'andar.

\vspace{0.3cm}

desde que  $\vert \Sigma \vert$ es sim\'etrica y definida positiva, existe una matriz $\Sigma^{1/2}$ llamada la ra\'iz cuadrada de $\Sigma$ con las siguientes propiedades: 

\begin{enumerate}
\item  La matriz $\Sigma^{1/2}$ es sim\'etrica.
\item $\Sigma = \Sigma^{1/2}\Sigma^{1/2}$.
\item $\Sigma^{1/2}\Sigma^{-1/2} = \Sigma^{-1/2}\Sigma^{1/2} = I$ donde $\Sigma^{-1/2} = (\Sigma^{1/2})^{-1}$
\end{enumerate}

\vspace{0.5cm}

\textbf{Teorema} Si $Z \sim N(0,I)$ y $X = \mu + \Sigma^{1/2}Z$ entonces $X \sim N(\mu, \Sigma)$. De otro lado, si $X \sim N(\mu, \Sigma)$ entonces $\Sigma^{-1/2}(X -\mu) \sim N(0,I)$.


\vspace{0.5cm}

Supongamos que particionamos un vector Normal aleatorio $X$, como $X = (X_a, X_b)$. De manera similar tenemos la partici\'on $\mu = (\mu_a, \mu_b)$ y

\vspace{0.3cm}


\[
\Sigma = \begin{pmatrix}
\Sigma_{aa} & \Sigma_{ab} \\
\Sigma_{ba} & \Sigma_{bb}
\end{pmatrix}
\]


\vspace{0.5cm}

\textbf{Teorema} Sea $X \sim N(\mu, \Sigma)$. Entonces

\begin{enumerate}
\item La distribuci\'on marginal de $X_a$ es $X_a \sim N(\mu_a, \Sigma_{aa})$.
\item La distribuci\'on condicional de $X_b$ dado $X_a = x_a$ es

\[
X_b|X_a = x_a \sim N(\mu_b + \Sigma_{ba}\Sigma_{aa}^{-1}(x_a - \mu_a), \Sigma_{bb} -\Sigma_{ba}\Sigma_{aa}^{-1}\Sigma_{ab}).
\]
\item Si $a$ es un vector entonces $a^{T}X \sim N(a^{T}\mu, a^{T}\sum a)$.
\item $V = (X -\mu)^{T}\Sigma^{-1}(X - \mu) \sim \chi_{k}^{2}$.
\end{enumerate}

\vspace{0.8cm}


{\Large  Transformaci\'on de  Variables Aleatorias}

\vspace{0.5cm}

Supongamos que $X$ es una variable aleatoria con \texttt{PDF} $f_X$ y \texttt{CDF} $F_X$ . Sea $Y = r(X)$ una funci\'on de $X$, llamada una \textbf{transformaci\'on de X}. Calcular el  \texttt{PDF}  y \texttt{CDF} de $Y$ en el caso discreto, por ejemplo tenemos

\begin{align*}
f_{Y}(y) &= \mathbb{P}(Y=y) = \mathbb{P}(r(X) =y)\\
& = \mathbb{P}(\{x; r(x) = y\}) = \mathbb{P}(X \in r^{-1}(y))
\end{align*}

\vspace{0.3cm}

\textbf{Ejemplo} Sea $\mathbb{P}(X = -1) = \mathbb{P}(X = 1) = 1/4$ y $\mathbb{P}(X = 0) = 1/2$. Sea $Y = X^2$. Entonces, $\mathbb{P}(Y = 0) = \mathbb{P}(X = 0) = 1/2$ y $\mathbb{P}(Y = 1) = \mathbb{P}(X = 1) + \mathbb{P}(X = -1) = 1/2$. Resumiendo


\vspace{0.5cm}



\begin{minipage}{2.5in}
%\begin{table}[h]
\centering
\begin{tabular}{l l }
$x$ & $f_{X}(x)$ \\
\hline
-1     & 1/4   \\                     
0     & 1/2   \\                
1     & 1/4                        
\end{tabular}
%\end{table} 
\end{minipage}
\begin{minipage}{2.5in}
%\begin{table}[h]
\centering
\begin{tabular}{l l}
y & $f_Y(y)$ \\
\hline
0 & 1/2                 \\
1 & 1/2                 
\end{tabular}
%\end{table}
\end{minipage}

\vspace{0.5cm}


El caso es continuo, es m\'as dificil. Aqu\'i tres pasos para encontrar $f_Y$:

\vspace{0.3cm}

\begin{enumerate}
\item Para cada $y$, encuentra el conjunto $A_y =  \{x: r(x) \leq y \}$.
\item Encontrar el \texttt{CDF}

\begin{align*}
F_{Y}(y) = \mathbb{P}(Y \leq y) = \mathbb{P}(r(X) \leq y)\\
   = \mathbb{P}(\{x : r(x) \leq y \})\\
   = \int_{A_y}f_{X}(x)dx 
\end{align*}
\item El \texttt{PDF} es $f_{Y}(y) = F_{Y}^{'}(y)$.
\end{enumerate}


\vspace{0.5cm}

\textbf{Ejemplo} Sea $f_{X}(x) = e^{-x}$ para $x > 0$. As\'i, $F_{X}(x) = \int_{0}^{x}f_{X}(s)ds = 1 -e^{-x}$. Sea $Y = r(X) = \log(X)$. Entonces $A_y = \{x: x\leq e^{y} \}$ y

\begin{align*}
F_{Y}(y) = &\mathbb{P}(Y\leq y) = \mathbb{P}(\log X \leq Y)\\
= & \mathbb{P}(X \leq e^y) = F_{X}(e^y) =  1- e^{-e^{y}}.
\end{align*}

\vspace{0.3cm}

Por tanto, $f_{Y}(y) = e^ye^{-e^{y}}$ para $y \in \mathbb{R}$.


\vspace{0.5cm}

Si tenemos por ejemplo, $X$ y $Y$ variables aleatorias, veamos como conocer la distribuci\'on de $X/Y, X +Y, \max\{X,Y \} $ o $\min\{X,Y \}$. Sea $Z = r(X,Y)$, los pasos para encontrar $f_{Z}$ son los mismo de antes

\begin{enumerate}
\item Para cada $z$, encuentra el conjunto $A_z =  \{(x,y): r(x,y) \leq z \}$.
\item Encontrar el \texttt{CDF}

\begin{align*}
F_{Z}(z) = \mathbb{P}(Z \leq z) = \mathbb{P}(r(X,Y) \leq z)\\
   = \mathbb{P}(\{(x,y) : r(x,y) \leq z \})\\
   = \int \int_{A_z}f_{X,Y}(x,y)dxdy 
\end{align*}
\item El \texttt{PDF} es $f_{Z}(z) = F_{Z}^{'}(z)$.
\end{enumerate}

\vspace{0.3cm}

\textbf{Ejemplo} Sea $X_1, X_2 \sim \mbox{Uniform}(0.1)$ independientes. Encontremos la densidad de $Y = X_1 + X_2$ en efecto la  \texttt{jdf} de $(X_1,X_2)$ es

\vspace{0.2cm}

\[
f(x_1, x_2) = \begin{cases}
1 & 0< x_1 < 1, 0 < x_2 < 1\\
0 & \mbox{en otros casos}
\end{cases}
\]


\vspace{0.3cm}

Sea $r(x_1, x_2) = x_1 + x_2$. Ahora

\vspace{0.3cm}

\begin{align*}
F_{Y}(y) &= \mathbb{P}(Y \leq y)= \mathbb{P}(r(X_1,X_2) \leq y)\\
& = \mathbb{P}( \{(x_1, x_2): r(x_1, x_2)\leq y \}) = \int \int_{A_y}f(x_1, x_2)dx_1dx_2.
\end{align*}

\vspace{0.2cm}

Para encontrar $A_y$, suponemos primero que $0 < y \leq 1$. Entonces $A_y$ es el tri\'angulo con v\'ertices $(0,0), (y,0)$ y $(0.y)$.. En este caso $\int \int_{A_y}f(x_1, x_2)dx_1dx_2$ es el \'area del tri\'angulo, es decir $y^2/2$. Si $1 < y < 2$, entonces $A_y$ es todo el cuadrado unitario, menos el tri\'angulo con v\'ertices $(1, y-1),(1,1), (y -1,1)$. Este conjunto tiene \'area $1 - (2 -y)^2/2$ 

\vspace{0.2cm}

\begin{figure}[h]
\centering
\includegraphics[scale=.65]{m2.png}
%\caption{Modelo del problema}
\end{figure}

\vspace{0.2cm}

Por tanto

\vspace{0.2cm}

\[
F_{Y}(y) = \begin{cases}
0 & y < 0\\
\frac{y^2}{2} & 0 \leq y < 1 \\
1 -\frac{(2 -y)^2}{2} & 1 \leq y < 2\\
1 & y \geq 2
\end{cases}
\]

\vspace{0.2cm}

Por diferenciaci\'on el \texttt{PDF} es

\[
f_{Y}(y)=\begin{cases}
y & 0 \leq y \leq 1 \\
2 -y & 1\leq y \leq 2\\
0 & \mbox{ en otros casos}.
\end{cases}
\]
\end{document}