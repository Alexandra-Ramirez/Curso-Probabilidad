\documentclass{article}
\usepackage[sc]{mathpazo}
\usepackage[T1]{fontenc}
\usepackage{bigints}
\usepackage[latin1]{inputenc}
\usepackage{geometry}
\usepackage{amssymb}
\usepackage{amsmath} 
\usepackage{mathtools}
%\usepackage[spanish]{babel}
\usepackage{latexsym}
\geometry{verbose,tmargin=1.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
\usepackage{url}
\usepackage{graphicx}
\newcommand{\R}{\mathbb{R}}
\newcommand{\bb}[1]{\textbf{#1}}
\DeclareMathOperator{\rank}{\textbf{rango}}
\DeclareMathOperator{\proy}{\textbf{proy}}
\DeclareMathOperator{\nulll}{\textbf{nul}}
\DeclareMathOperator{\diag}{\textbf{diag}}
\DeclareMathOperator{\col}{\textbf{col}}
\DeclareMathOperator{\fila}{\textbf{fila}}
\DeclareMathOperator{\dimm}{dim}
\DeclareMathOperator{\Traz}{Tr}
%\theoremstyle{definition}
\everymath{\displaystyle}
\newtheorem{ejemplo}{{Ejemplo }}[section]
\newtheorem{teo}{{Teorema}}[section]
\newtheorem{defi}{{Definici\'on}}[section]
\newtheorem{pros}{{Proposici\'on}}[section]
\newtheorem{cor}{{Corolario}}[section]
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=2,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=false]
 {hyperref}
\hypersetup{
 pdfstartview={XYZ null null 1}}
\begin{document}

<<setup, include=FALSE, cache=FALSE>>=
library(knitr)
library(ggplot2)
library(grid)
#library(animation)
# set global chunk options
opts_chunk$set(fig.path='figure/minimal-', fig.align='center', fig.show='hold')
options(formatR.arrow=TRUE,width=90)
@

%\title{\underline{\textbf{Notas de mat\'ematica}}}
%\date{}
%\maketitle
\hspace*{0.5\linewidth}
\begin{minipage}{0.6\linewidth}
Curso: Introducci\'on a la Probabilidad y Estad\'istica CM -274\par
Desigualdades y teoremas l\'imites \par
\end{minipage}


\vspace {0.5cm}


\section{Desigualdades y teoremas l\'imites}

Las desigualdades y el comportamiento asint\'otico de secuencias de variables aleatorias es un parte importante de la teoria de las probabilidades. El principal contexto  involucra una secuencia $X_1, X_2, \dots $ de variables aleatorias independientes id\'enticamente distribuidas con media $\mu$ y varianza $\sigma^2$.  

\vspace{0.2cm}

Existen importantes desigualdades que se pueden aplicarse al caso especial de variables de Bernoulli id\'enticamente distribuidas, que es la manera en que se  aplica frecuentemente en combinatoria y en inform\'atica o que proporcionan conceptualmente una interpretaci\'on de la probabilidad y esperanza, en t\'erminos de secuencias independiente y id\'enticas. La desigualdad de Markov, utiliza s\'olo la esperanza de una variable aleatoria y proporciona l\'imites que suelen ser bastante sueltos.

La desigualdad de Chebychev, utiliza tanto la esperanza como la varianza de una variable aleatoria y normalmente genera l\'imites m\'as estrictos.

El  l\'imite de Chernoff, requiere un conocimiento de la funci\'on generadora de momentos de una variable aleatoria. Puesto que el conocimiento de una funci\'on de generaci\'on de momentos implica un conocimiento de los momentos de todos los \'ordenes, debemos esperar que el l\'imite de Chernoff proporcione l\'imites m\'as estrictos que los obtenidos de las desigualdades de Markov o de Chebychev.

\subsection{Desigualdad de Markov} 

\begin{teo}
\normalfont Sea $X$ es una variable no negativa y supongamos que  $\mathbb{E}(X)$ existe. Entonces se cumple:

\[
\mathbb{P}(X > a)\leq \frac{\mathbb{E}(X)}{a}.
\]

Para alg\'un $a > 0$.
\end{teo}

Fijemos un n\'umero positivo $a$ y consideremos una variable aleatoria $Y_a$ definida como:

\[
Y_a = \begin{cases}
0, & \text{si}\ \ X  < a\\
a, & \text{si}\ \ X \leq a
\end{cases}
\]

Luego: 

\[
Y_a \leq X
\]

Lo que implica, si tomamos esperanzas en ambos lados: 

\vspace{0.2cm}

\[
\mathbb{E}(Y_a) \leq \mathbb{E}(X)
\]

\vspace{0.2cm}

\[
\mathbb{E}(X) \geq \mathbb{E}(Y_a) = a\mathbb{P}(Y_a = a) = a\mathbb{P}(X \geq a)
\]

\vspace{0.2cm}

Por tanto : 

\vspace {0.2cm}

\[
a\mathbb{P}(X \leq a) \leq \mathbb{E}(X).
\]

\begin{ejemplo}
\normalfont Sea $X$ la variable aleatoria que denota la edad  (en a\~nos) de un ni\~no de un determinado colegio. Si la edad promedio de edad de ese colegio es $12.5$ a\~nos, entonces usando la desigualdad de Markov, la probabilidad que un ni\~no es tiene por lo menos 20 a\~nos satisface la desigualdad 

\vspace{0.2cm}

\[
\mathbb{P}(X \geq 20) \leq 12.5/20 = 0.6250.
\]

\vspace{0.2cm}

Este resultado es ilustrado mediante el siguiente gr\'afico:


\begin{figure}[h]
\centering
\includegraphics[scale=.55]{m1.png}
\end{figure}

\vspace{0.3cm}


Donde  la parte a) muestra el \texttt{funci\'on densidad de probabilidad} de una variable aleatoria no negativa $X$. La parte b) muestra la \texttt{funci\'on de masa de probabilidad } de una variable $Y_a$, que se construye de la siguiente manera: Toda la masa de probabilidad en la \texttt{funci\'on de densidad de probabilidad} de $X$ que se encuentra entre $0$ y $a$ se asigna a $0$  y toda la masa que se encuentra por encima de $a$ es asignada a $a $, puesto que la masa se desplaza a la izquierda, la esperanza s\'olo puede disminuir.


\vspace{0.3cm}

En general si $X$ es una variable aleatoria y $h$ una funci\'on no decreciente  y no negativa. La esperanza $h(X)$ asumiendo que existe, es dada por:


\[
\mathbb{E}(h(X)) = \bigintsss_{-\infty}^{\infty}h(u)f_{X}(u)du,
\]

\vspace{0.2cm}

y escribimos:


\[
\bigintsss_{-\infty}^{\infty}h(u)f_{X}(u)du \geq \bigintsss_{a}^{\infty}h(u)f_{X}(u)du \geq h(a)\bigintsss_{t}^{\infty}f_{X}(u)du = h(a)\mathbb{P}(X \geq a).
\]

\vspace{0.2cm}

Esto conduce a la desigualdad de Markov: $\mathbb{P}(X \geq a) \leq \dfrac{\mathbb{E}(h(X))}{h(a)} $ para $a > 0$.
\end{ejemplo}

\subsection{Desigualdad de Chebyshev}

\vspace{0.2cm}

La desigualdad de Chebyshev, dice que si una variable aleatoria tiene una peque\~na varianza, entonces la probabilidad que toma un valor lejos de la media es tambi\'en peque\~na.

\begin{teo}
\normalfont  Sea $\mu = \mathbb{E}(X)$ y $\sigma^2 = \mathbb{V}(X)$. Entonces para un $t > 0$

\[
\mathbb{P}(\vert X- \mu \vert \geq t) \leq \frac{\sigma^2}{t^2}\qquad y \qquad \mathbb{P}(\vert Z \vert \geq k) \leq \frac{1}{k^2}.
\]

\vspace{0.2cm}

donde $Z = (X - \mu)/\sigma$. En particular $\mathbb{P}(\vert Z \vert > 2) \leq 1/4$ \qquad y \qquad  $\mathbb{P}(\vert Z \vert > 3) \leq 1/9$.


\vspace{0.2cm}

Aplicando la desigualdad de Markov, podemos concluir que:

\vspace{0.2cm}

\[
\mathbb{P}((X -\mu)^2 \leq t^2) \leq \dfrac{\mathbb{E}(X -\mu)^2}{t^2} = \dfrac{\sigma^2}{t^2}.
\]


Para completar el evento $(X - \mu)^2 \geq t^2$ es id\'entico al evento $\vert X - \mu\vert \geq t$, as\'i:

\vspace{0.2cm}

\[
\mathbb{P}(\vert X - \mu \vert \leq t) = \mathbb{P}((X -\mu)^2 \leq t^2) \leq \dfrac{\sigma^2}{t^2}.
\]

\vspace{0.2cm}

Para la segunda parte se coloca $t = k\sigma$.
\end{teo}

\begin{ejemplo}
\normalfont Supongamos que probamos un modelo de predicci\'on sobre un conjunto de $n$  casos de prueba. Sea $X_i = 1$ si el predictor es incorrecto y $X_i = 0$ si el predictor es correcto. Entonces $\overline{X}_n = n^{-1}\displaystyle\sum_{i =1}^{n}X_i$ es la taza de error observado. Cada $X_i$ puede ser considerado una variable de Bernoulli con media desconocida $p$. Intuitivamente $\overline{X}_n$ debe ser cercano a $p$, pero que tan probable es que $\overline{X}_n$ est\'e fuera de un entorno de tama\~no $\epsilon$, sabiendo que $\mathbb{V}(\overline{X}_n) =\mathbb{V}(X_1)/n = p(1 -p)/n$. En efecto: 

\[
\mathbb{P}(\vert \overline{X}_n -p \vert > \epsilon) \leq \dfrac{\mathbb{V}(\overline{X}_n)}{\epsilon^{2}} = \dfrac{p(1 -p)}{n\epsilon^2} \leq \dfrac{1}{4n\epsilon^2}
\]

\vspace{0.2cm}

desde que $p(1 -p) \leq \frac{1}{4}$ para todo $p$. Para $\epsilon = .2$ y $n = 100$, la cota es $.0625$.
\end{ejemplo}

\begin{ejemplo}
\normalfont Volvamos al  ejemplo (1.1) de los ni\~no de la escuela media y recalculamos el l\'imite usando la desigualdad de Chebychev. En este caso tambi\'en necesitamos la varianza de las edades de los ni\~nos, que tomamos como $3$. Buscamos la probabilidad de que un ni\~no en la escuela pueda ser mayor  que $20$. Primero debemos poner esto en la forma requerida por La ecuaci\'on de Chebychev:

\[
\mathbb{P}(X \geq 20) = \mathbb{P}[(X -\mathbb{E}(X)) \geq (20 - \mathbb{E}(X))] =  \mathbb{P}[(X -\mathbb{E}(X)) \geq 7.5)].
\]

Sin embargo:

\[
 \mathbb{P}[(X - 12.5) \geq 7.5] \neq   \mathbb{P}[\vert X -12.5\vert \geq 7.5)] = \mathbb{P}(5 \leq X \geq 20).
\]


As\'i no podemos aplicar la desigualdad de Chebyshev para calcular directamente $\mathbb{P}(X \leq 20)$. En efecto, calculamos, una cota para $\mathbb{P}(5 \leq X \geq 20)$ y obtenemos:

\[
 \mathbb{P}[\vert X - \mathbb{E}(X) \vert \geq 7.5)] \leq \frac{3}{(7.5)^2} = 0.0533, 
\]

que sigue siendo un l\'imite mucho m\'as estricto que el obtenido anteriormente.
\end{ejemplo}

\vspace{0.3cm}

\subsection{Desigualdad de Hoeffding}

Empecemos  con un resultado importante:

\vspace{0.2cm}

\begin{pros}
\normalfont Supongamos que $\mathbb{E}(X) = 0$ y que $a \leq x \leq b$. Entonces:

\[
\mathbb{E}(e^{tX}) \leq e^{t^2(b -a)^2/8}.
\]
\end{pros}

\vspace{0.2cm}

Para desarrollar la desigualdad de  Hoeffding, necesitamos el \texttt{m\'etodo de Chernoff.}

\vspace{0.2cm}

\begin{pros}
\normalfont \textbf{(M\'etodo de Chernoff)} Sea $X$ una variable aleatoria. Entonces:

\[
\mathbb{P}(X > \epsilon) \leq \inf_{t \geq 0}e^{-t\epsilon}\mathbb{E}(e^{tX}).
\]

\end{pros}

\vspace{0.2cm}

Este m\'etodo puede ser derivado de la desigualdad de Markov con  la funci\'on $h(x) = e^{tx}$ que  es la funci\'on generadora de momentos  $\mathbb{E}(e^{tX})$.

\vspace{0.2cm}


Para alg\'un $t > 0$

\vspace{0.2cm}

\[
\mathbb{P}(X >\epsilon) = \mathbb{P}(e^ X > e^{\epsilon}) = \mathbb{P}(e^{tX} > e^{t\epsilon}) \leq e^{-t\epsilon}\mathbb{E}(e^{tX}).
\]

\vspace{0.2cm}

Desde que esto es cierto para $t \geq 0$ el resultado sigue.


\vspace{0.3cm}

\begin{teo}
\normalfont (\textbf{Desigualdad de Hoeffding}) Sean $Y_1, Y_2, \dots, Y_n$ observaciones indepedientes, tal que $\mathbb{E}(Y_i)=0 $ y $a_i \leq Y_i \leq b_i$. Sea $\epsilon > 0$. Entonces, para alg\'un $ t > 0$

\vspace{0.2cm}

\[
\mathbb{P}(\vert \overline{Y}_n - \mu \vert \geq \epsilon) \leq 2e^{-2n\epsilon^2/(b -a)^2}.
\]

\end{teo}

\vspace{0.2cm}


Sin p\'erdida de generalidad, supongamos $\mu = 0$. Entonces se tiene:

\vspace{0.2cm}

\begin{align*}
\mathbb{P}(\vert \overline{Y}_n\vert \geq \epsilon) = \mathbb{P}( \overline{Y}_n \geq \epsilon) + \mathbb{P}( \overline{Y}_n\vert \leq -\epsilon) \\
= \mathbb{P}( \overline{Y}_n \geq \epsilon) + \mathbb{P}( -\overline{Y}_n \geq \epsilon)
\end{align*}

\vspace{0.2cm}

Usamos el m\'etodo de chernoff. Para alg\'un $t > 0$, desde la desigualdad de Markov, que:

\vspace{0.2cm}

\begin{align*}
\mathbb{P}( \overline{Y}_n \geq \epsilon) = \mathbb{P}\Bigl(\displaystyle\sum_{i =1}^{n}Y_i \geq n\epsilon\Bigr) = \mathbb{P}(e^{\sum_{i =1}^{n}y_i}\geq e^{n\epsilon})\\
= \mathbb{P} \Bigl(e^{t\sum_{i =1}^{n}y_i}\geq e^{tn\epsilon}\Bigr) \leq e^{-tn\epsilon}\mathbb{E}\Bigl(e^{t\sum_{i =1}^{n}y_i} \Bigr)\\
=e^{-tn\epsilon}\displaystyle\prod_{i}(e^{tY_i}) = e^{-tn\epsilon}(\mathbb{E}(e^{tY_i}))^{n}
\end{align*}


\vspace{0.2cm}

Por la proposici\'on (1.1): $\mathbb{E}(e^{tY_i}) \leq e^{t^2(b -a)^2/8}$. As\'i:


\[
\mathbb{P}( \overline{Y}_n \geq \epsilon) \leq e^{-tn\epsilon}e^{t^2n(b -a)^2/8}
\]

\vspace{0.2cm}

Esto se reduce al m\'inimo cuando $t = 4\epsilon/(b -a)^2$ dando el resultado: 

\vspace{0.2cm}

\[
\mathbb{P}( \overline{Y}_n \geq \epsilon) \leq e^{-2n\epsilon^2/(b -a)^2}
\]

\vspace{0.2cm}

Aplicando el mismo argumento a $\mathbb{P}( - \overline{Y}_n \geq \epsilon)$ se produce el resultado.

\vspace{0.2cm}

\begin{cor}
\normalfont Si $X_1, X_2, \cdots, X_n$ son independientes con $\mathbb{P}(a \leq X_i \leq b) = 1$   y una media com\'un $\mu$, entonces con probabilidad de al menos $1 -\delta$, se cumple:

\vspace{0.2cm}

\[
\vert \overline{X}_n - \mu \vert \leq \sqrt{\dfrac{(b -a)^2}{2n}\log\Bigl(\dfrac{2}{\delta}\Bigr)}.
\]
\end{cor}

\begin{ejemplo}
\normalfont Sea $X_1, X_2, \dots, X_n \sim \mbox{Bernoulli}(p)$. Desde la desigualdad Hoeffding se tiene,


\[
\mathbb{P}(\vert \overline{X}_n -p \vert > \epsilon) \leq 2\epsilon^{-2n\epsilon^2}.
\]

\vspace{0.2cm}

donde $\overline{X}_n = n^{-1}\displaystyle\sum_{i = 1}^{n}X_i$

\vspace{0.2cm}

Sea $X_1, X_2, \dots, X_n \sim \mbox{Bernoulli}(p)$. Sea $n = 100$ y $\epsilon= .2$. Por la desigualdad de Chebyshev, tenemos

\vspace{0.2cm}

\[
\mathbb{P}(\vert \overline{X}_n - p\vert > \epsilon) \leq .0625
\]

\vspace{0.2cm}

De acuerdo a la desigualdad de Hoeffding:

\vspace{0.2cm}

\[
\mathbb{P}(\vert \overline{X}_n - p\vert > .2) \leq 2e^{-2(100)(.2)^2} = .00067
\]

\vspace{0.2cm}

que es mucho m\'as peque\~no que el resultado obtenido por la desigualdad de Chebyshev $.0625$.
\end{ejemplo}

\vspace{0.3cm}

La desigualdad de Hoffding proporciona una manera de crear un \texttt{intervalo de confianza} para un par\'ametro binomial $p$. Sea $\alpha > 0 $ y sea

\vspace{0.3cm}

\[
\epsilon_n = \sqrt{\dfrac{1}{2n}\log\Bigl(\dfrac{2}{\alpha} \Bigr)}
\]

\vspace{0.3cm}

Por la desigualdad de Hofffing:

\[
\mathbb{P}(\vert \overline{X}_n - p \vert > \epsilon_n) \leq 2e^{-2n\epsilon^2_n} = \alpha.
\]

\vspace{0.3cm}

Sea $C = (\overline{X}_n -\epsilon_n, \overline{X}_n + \epsilon_n )$. Entonces $\mathbb{P}(p \notin C) = \mathbb{P}(\vert \overline{X}_n - p\vert > \epsilon_n) \leq \alpha$. As\'i, $\mathbb{P}(p \in C) \geq 1 -\alpha$, esto es, el intervalo aleatorio
$C$ contiene el param\'etro $p$ con probabilidad $1 - \alpha$.

\vspace{0.2cm}

Llamamos a $C$ un \texttt{intervalo de confianza $1 -\alpha$}.


\vspace{0.8cm}

El siguiente resultado extiende la desigualdad de Hoeffding a funciones m\'as generales $g(x_1, \dots, x_n)$. Consideramos la desigualdad de McDiarmind.

\subsection{Desigualdad de McDiarmind}

\begin{teo}
\normalfont \textbf{Teorema de McDiarmid} Sea $X_1, X_2, \dots X_n$ variables aleatorias independientes. Suponganse que

\[
\sup_{x_1,\dots, x_n, x^{'}_{i}}\Bigl\vert g(x_1,\dots, x_{i - 1}, x_i, x_{i + 1}, \dots, x_n ) - g(x_1,\dots, x_{i - 1}, x^{'}_i, x_{i + 1}, \dots, x_n )\Bigr\vert \leq c_i
\]

\vspace{0.2cm}

para $i = 1,\dots, n$. Entonces

\vspace{0.2cm}

\[
\mathbb{P}\Bigl(g(X_1,\dots, X_n) -\mathbb{E}(g(X_1,\dots, X_n))\geq \epsilon \Bigr) \leq \exp\Bigl\{-\frac{2\epsilon^2 }{\sum_{i = 1}^{n}c^{2}_i}    \Bigr\}.
\]

\end{teo}
\vspace{0.3cm}

Si tomamos $g(x_1,\dots, x_n) = n^{-1}\displaystyle\sum_{i = 1}^{n}x_i$, es la desigualdad de Hoeffding.


\begin{ejemplo}
\normalfont Supongamos que lanzamos $m$ pelotas en $n$ recipientes. ?` Qu\'e fracci\'on de los recipientes est\'an vacios?.

\vspace{0.2cm}

Sea $Z$ el n\'umero de recipientes vacios y sea $F = Z/n$ la fracci\'on de recipientes vacios. Podemos escribir $Z = \sum_{i =1}^{n}Z_i$, donde $Z_i = 1$ de un recipiente $i$ est\'a vac\'io y $Z_i = 0$ en otro caso. Entonces:


\[
\mu = \mathbb{E}(Z) = \displaystyle\sum_{i =1}^{n}\mathbb{E}(Z_i) = n(1 -1/n)^m = ne^{m\log(1 -1/n)} \approx ne^{-m/n}.
\]

\vspace{0.2cm}

y  $\theta =\mathbb{E}(F) = \mu/n \approx e^{-m/n}$. ?` Cu\'an cerca est\'a Z de $\mu$?. Se debe notar que las variables $Z_i$ no son independientes, por lo que no se puede aplicar la desigualdad de Hoeffding. En este caso procedemos de otra forma:

\vspace{0.2cm}


Definamos las variables $X_1, \dots, X_m$ donde $X_s = i$ si la pelota $s$ cae en el recipiente $i$. Entonces $Z = g(X_1,\dots, X_m)$. Si movemos una pelota a un recipiente diferente, entonces $Z$ puede cambiar a lo m\'as $1$. As\'i del Teorema de McDiarmind con $c_i = 1$ tenemos:

\vspace{0.3cm}

\[
\mathbb{P}(\vert Z - \mu\vert  > t) \leq 2e^{-2t^2/m}.
\]

\vspace{0.3cm}

Como las fracci\'on de recipientes vacios es $F=Z/m$ con media $\theta = \mu/n$. Tenemos:


\[
\mathbb{P}(\vert F - \theta\vert > t) = \mathbb{P}(\vert Z - \mu\vert> nt) \leq 2e^{-2n^2t^2/m}.
\]

\end{ejemplo}



\subsection{Cotas para los valores esperados}


\begin{teo}
\normalfont \textbf{Desigualdad de Cauchy -Schwartz} Si $X$ y $Y$ tienen varianza finita, entonces:

\[
\mathbb{E}\vert XY\vert \leq \sqrt{\mathbb{E}(X^2)\mathbb{E}(Y^2)}.
\]

\vspace{0.2cm}


La desigualdad de Cauchy-Schwarz puede ser escrita como:

\[
\text{Cov}^2(X,Y) \leq \delta_{X}^{2}\delta_{Y}^{2}.
\]

\end{teo}

\begin{teo}
\normalfont\textbf{Desigualdad de Jensen} Si $g$ es convexa, entonces:
\[
\mathbb{E}g(X) \geq g(\mathbb{E}X)
\]

Si $g$ es c\'oncava

\[
\mathbb{E}g(X) \leq g(\mathbb{E}X).
\]


\vspace{0.2cm}

En efecto, sea $L(x)= a +bx$ una l\'inea tangente a $g(x)$ en el punto $\mathbb{E}(X)$. Desde que $g$ es convexa, est\'a por encima de la l\'inea $L(X)$. As\'i:

\vspace{0.2cm}

\[
\mathbb{E}g(X) \geq \mathbb{E}L(X) = \mathbb{E}(a +bX) = a + b\mathbb{E}(X) = L(\mathbb{E}(X)) = g(\mathbb{E}X).
\]

\vspace{0.2cm}

De la desigualdad de Jensen, se cumple $\mathbb{E}(X^2) \geq (\mathbb{E}X)^2$ y si $X$ es positiva, entonces $\mathbb{E}(1/X) \geq 1/\mathbb{E}(X)$. Desde que $\log$ es c\'oncava, $\mathbb{E}(\log X) \leq \log\mathbb{E}(X)$.

\end{teo}



\subsection{Distancia de Kullback-Leibler} 

\vspace{0.2cm}

Definamos la distancia de \textbf{Kullback-Leibler} entre dos densidades $f_1$ y $f_2$ como:


\[
D(f_1, f_2) = \bigintsss{f_1(x)\log\Bigl(\frac{f_1(x)}{f_2(x)}\Bigr)dx}
\]

\vspace{0.2cm}

Debemos notar que  $D(f_1,f_1) = 0$ y usando la desigualdad de Jensen se cumple que  $D(f_1, f_2) \geq 0$. En efecto sea $X \sim f_1$. Entonces:

\vspace{0.2cm}

\[
-D(f_1, f_2) = \mathbb{E}\log\Bigl(\dfrac{f_1(X)}{f_2(X)} \Bigr) \leq \log\mathbb{E}\Bigl(\dfrac{f_1(X)}{f_2(X)} \Bigr) = \log\bigintsss f_1(x)\dfrac{f_2(x)}{f_1(x)}dx = \log\bigintsss f_2(x)dx = \log(1) = 0.
\]

\vspace{0.2cm}

As\'i $D(f_1, f_2) \geq 0$, es decir $D(f_1, f_2) \geq 0$.

\vspace{0.2cm}

La distancia de Kullbacker-Leibler es una medida de la informaci\'on que se pierde cuando $f_1$ se aproxima a $f_2$. Un ejemplo en R.

\vspace{0.2cm}

<<fooq, comment = NA, prompt =TRUE, eval= FALSE>>=
set.seed(1000)
X<- rexp(10000, rate=0.2)
Y<- rexp(10000, rate=0.4)
KL.dist(X, Y, k=5)
KLx.dist(X, Y, k=5)
@

\vspace{0.2cm}

El anterior script usa el paquete \textbf{FNN } de $R$. 


\vspace{0.5cm}


La distancia de Kullbacker-Leibler entre una distribuci\'on Gaussiana $f_1$ con media $\mu_1$ y varianza $\sigma^2_1$ y una distribuci\'on Gaussiana $f_2$ con media $\mu_2$ y varianza $\sigma^2_2$ es dado por:

\vspace{0.2cm}


\[
D(f_1,f_2) = \log \frac{\sigma_2}{\sigma_1} + \frac{\sigma^2_1 + (\mu_1 - \mu_2)^2}{2\sigma^2_2} - \frac{1}{2}.
\]

\vspace{0.2cm}

Si tenemos una cota exponencial sobre $\mathbb{P}(X_n > \epsilon$, podemos acotar $\mathbb{E}(X_n)$ como sigue:

\begin{teo}
\normalfont Supongamos que $X_n \geq 0$ y para cada $\epsilon >0$:

\vspace{0.2cm}

\begin{equation}
\mathbb{P}(X_n > \epsilon) \leq c_1e^{-c_2n\epsilon^2}
\end{equation}

\vspace{0.2cm}

para alg\'un $c_2 >0$ y $c_1 > 1/e$. Entonces:

\vspace{0.2cm}

\[
\mathbb{E}(X_n) \leq \sqrt{\dfrac{C}{n}}.
\]


\vspace{0.2cm}

donde $C = (1 + \log(c_1))/c_2$
\end{teo}


En efecto para una variable aleatoria $Y$ no negativa, $\mathbb{E}(Y) = \bigints_{0}^{\infty}\mathbb{P}(Y \geq t) dt$. As\'i, para alg\'un $a > 0$,

\[
\mathbb{E}(X_n^2) =  \bigints_{0}^{\infty}\mathbb{P}(X_n^2 \geq t) dt =  \bigints_{0}^{a}\mathbb{P}(X_n^2 \geq t) dt +  \bigints_{a}^{\infty}\mathbb{P}(X_n^2 \geq t) dt \leq a  +  \bigints_{a}^{\infty}\mathbb{P}(X_n^2 \geq t) dt.  
\]

La ecuaci\'on (1) implica que $\mathbb{P}(X_n > \sqrt{t}) \leq c_1e^{-c_2nt}$. As\'i:

\[
\mathbb{E}(X_n^2) \leq a  +  \bigints_{a}^{\infty}\mathbb{P}(X_n^2 \geq t) dt =  a  +  \bigints_{a}^{\infty}\mathbb{P}(X_n \geq \sqrt{t}) dt \leq a  +  c_1\bigints_{a}^{\infty}e^{-c_2nt} dt = a + \frac{c_1e^{-c_2na}}{c_2n}.
\]

Si $a = \log(c_1)/(nc_2)$, se concluye que:

\[
\mathbb{E}(X_n^2) \leq \frac{\log(c_1)}{nc_2} + \frac{1}{nc_2} = \frac{1 + \log(c_1)}{nc_2}.
\]

Finalmente, tenemos:

\[
\mathbb{E}(X_n)  = \sqrt{\mathbb{E}(X_n^2)} \leq \sqrt{\frac{1 + \log(c_1)}{nc_2}}. 
\]
\vspace{0.2cm}

Ahora consideramos acotar el m\'aximo de un conjunto  de variables aleatorios

\vspace{0.2cm}

\begin{teo}
\normalfont Sean $X_1,\dots, X_n$ variables aleatorias. Supongamos que existe un $\delta >0$ tal que $\mathbb{E}(e^{tX_i}) \leq e^{t^2\delta^2/2}$ para todo $t > 0$. Entonces: 

\vspace{0.2cm}

\[
\mathbb{E}\Bigl( \max_{1 \leq i \leq n}X_i \Bigr) \leq \delta \sqrt{2 \log n}.
\]


\vspace{0.2cm}

En efecto por la desigualdad de Jensen,

\vspace{0.3cm}

\begin{align*}
\exp\Bigl\{t\mathbb{E}\Bigl(\max_{1 \leq i \leq n}X_i \Bigr)\Bigr\} &\leq \mathbb{E}\Bigl(\exp\Bigl\{t\max_{1 \leq i \leq n} X_i\Bigr\}\Bigr)\\
&= \mathbb{E}\Bigl(\max_{1 \leq i \leq n}\exp\{tX_i\}\Bigr) \leq \displaystyle\sum_{i =1}^{n}\mathbb{E}(\exp\{tX_i \}) \leq ne^{t^2\sigma^2/2}.
\end{align*}


\vspace{0.2cm}

As\'i,

\vspace{0.2cm}

\[
\mathbb{E}\Bigl(\max_{1 \leq i \leq n}X_i \Bigr)\leq \dfrac{\log n}{t} + \dfrac{t\sigma^2}{2},
\]

\vspace{0.2cm}

El resultado se sigue colocando $t = \sqrt{2\log n}/\sigma$.
\end{teo}
\vspace{0.5cm}

La siguiente desigualdad es \'util para cotas de probabilidad para variables aleatorias normales

\vspace{0.3cm}

\begin{teo}
\normalfont Sea $Z \sim N(0,1)$. Entonces:

\vspace{0.2cm}

\[
\mathbb{P}(\vert Z \vert > t) \leq \sqrt{\dfrac{2}{\pi}}\dfrac{e^{-t^2/2}}{t}
\]
\end{teo}

\vspace{0.3cm}

\subsection{Notaci\'on $O_p$ y $o_p$}

En estad\'istica, probabilidades, algoritmos, se usa  la notaci\'on $o_{P}$ y $O_{P}$. Por ejemplo, $a_n = o(1)$ significa que $a_n \rightarrow 0$ cuando $n \rightarrow \infty$. $a_n = o(b_n)$ significa $a_n/b_n = o(1)$.


\vspace{0.2cm}

$a_n = O(1)$ significa eventualmente acotado. Es decir que para un $n$ una cantidad muy grande, $\vert a_n \vert \leq C$ para alg\'un $C >0$. $a_n = O(b_n)$, significa que $a_n/b_n = O(1)$.

\vspace{0.2cm}

Escribimos $a_n \sim b_n$ si ambos $a/b$ y  $b/a$ son eventualmente acotados. En ciencia de la computaci\'on esto se escribe $a_n = \Theta(b_n)$. Se prefiere usar $a_n \sim b_n$, ya que en estad\'istica $\Theta$ denota un espacio de par\'ametros.

\vspace{0.2cm}

Veamos estas definiciones  en el terreno de las probabilidades, es decir $Y_n = o_P(1)$ significa que para un  $\epsilon > 0$,


\[
\mathbb{P}(\vert Y_n \vert > \epsilon ) \rightarrow 0.
\]

\vspace{0.2cm}

Decimos que $Y_n = o_P(a_n)$ si, $Y_n/a_n = o_{P}(1)$. 

\vspace{0.2cm}

Decimos $Y_n = O_P(1)$ si, para cada $\epsilon > 0$, existe un $C > 0$ tal que:

\vspace{0.2cm}

\[
\mathbb{P}(\vert Y_n \vert > C) \leq \epsilon.
\]

\vspace{0.2cm}
Se dice que $Y_n = O_P(a_n)$ si $Y_n/a_n = O_p(1)$. 


\vspace{0.3cm}

Usamosla desigualdad Hoeffding, para mostrar que las proporciones muestrales son $O_{p}(1/\sqrt{n})$ alrededor de la media.  Sea $Y_1, Y_2, \dots, Y_n$ lanzamientos de monedas, esto es $Y_i \in \{0,1 \}$. Sea $p = \mathbb{P}(Y_i = 1)$. Sea:

\[
\overline{p}_n = \frac{1}{n}\sum_{i = 1}^{n}Y_i
\]

\vspace{0.2cm}

Mostremos que : $\overline{p}_n - p = o_P(1)$ y   $\overline{p}_n - p = O_P(1/\sqrt{n})$.

\vspace{0.5cm}

En efecto tenemos que:

\vspace{0.2cm}

\[
\mathbb{P}(\vert \overline{p}_n - p\vert > \epsilon) \leq 2e^{-2n\epsilon^2}\rightarrow 0
\]

\vspace{0.2cm}

y as\'i $\overline{p}_n - p = o_P(1)$. Tambi\'en:

\vspace{0.2cm}

\begin{align*}
\mathbb{P}(\sqrt{n}(\overline{p}_n - p) > C) &= \mathbb{P}\Bigl(\vert \overline{p}_n - p \vert > \dfrac{C}{\sqrt{n}}\Bigr)\\
& \leq 2e^{-2C^2} < \epsilon. 
\end{align*}

\vspace{0.2cm}

si escogemos un $C$ grande. As\'i $\sqrt{n}(\overline{p}_{n} - p) = O_{P}(1)$, lo que resulta  que:

\vspace{0.2cm}

\[
\overline{p}_{n} - p = O_p\Bigl(\dfrac{1}{\sqrt{n}} \Bigr)
\]

\vspace{0.3cm}

\section{La ley de promedios}

Si lanzamos un dado $5$ millones de veces y se mantiene un registro de los datos. El promedio de los n\'umeros  los cuales fueron lanzados es $3.500867$. Desde que la media de cada lanzamiento es $\frac{1}{6}(1 + 2 + \cdots +6) = 3\frac{1}{2}$, este resultado muestra  que para un $x_i$ el $i$ \'esimo lanzamiento, el promedio

\vspace{0.2cm}

\[
a_n = \dfrac{1}{n}(x_i + x_2 + \cdots + x_n)
\]

\vspace{0.2cm}

se acerca a la media $3\dfrac{1}{2}$, cuando $n \rightarrow \infty$.

\vspace{0.2cm}

En general si tenemos una secuencia $X_1, X_2, \dots$ de variables aleatorias identicamente distribuidas e independientes cada una teniendo una media $\mu$, deberiamos probar que el promedio:

\[
\dfrac{1}{n}(X_1 + X_2 +\cdots +X_n)
\]

\vspace{0.2cm}

converge cuando $n \rightarrow \infty$ a la media $\mu$.

\vspace{0.2cm}
\begin{defi}
\normalfont Decimos que la secuencia $x_1, x_2, \dots$ de variables aleatorias \texttt{converge en media  cuadrada a la variables x} si,

\vspace{0.2cm}

\[
\mathbb{E}([X_n - x]^2)\rightarrow 0 \ \ \text{cuando}\ \ n\rightarrow \infty
\]

\vspace{0.2cm}

y se escribe \texttt{$X_n \rightarrow x$ en media cuadrada cuando $n \rightarrow \infty$}.
\end{defi}

\vspace{0.2cm}

\begin{ejemplo}
\normalfont Sea $X_n$ una secuencia de variables aleatorias discretas con una funci\'on de masa de probabilidad:

\vspace{0.2cm}

\[
\mathbb{P}(X_n = 1) =  \dfrac{1}{n},  \ \ \ \ \mathbb{P}(X_n = 2) = 1 - \dfrac{1}{n}.
\]

\vspace{0.2cm}

Entonces $X_n$ converge a la variable aleatoria $2$ en media cuadrada cuando $n \rightarrow \infty$, desde que

\vspace{0.2cm}

\begin{align*}
\mathbb{E}([X_n - 2]^2) = (1 -2)\dfrac{1}{n} + (2 -2)^2\Bigl(1 - \dfrac{1}{n}\Bigr)\\
= \dfrac{1}{n} \rightarrow 0 \ \ \ \text{cuando}\ \ n \rightarrow \infty.
\end{align*}
\end{ejemplo}

\vspace{0.3cm}

\begin{teo}
\normalfont \textbf{(Ley de los grandes n\'umeros(1))} Sea una secuencia $X_1, X_2, \dots $ una secuencia de variables independientes cada una con media $\mu$ y varianza $\sigma^2$. El promedio de los primeros $n$ de las $X_i$ satisfacen cuando $n \rightarrow \infty$.

\vspace{0.2cm}

\[
\dfrac{1}{n}(X_1 +X_2 + \cdots + X_n) \rightarrow \mu \ \ \ \text{en media cuadrada}.
\]


\vspace{0.2cm}

Sea $S_n = X_1 + X_2 + \cdots X_n$, la n-\'esima suma parcial de los $X_i$. Entonces

\vspace{0.2cm}

\[
\mathbb{E}\Bigl(\dfrac{1}{n}S_n\Bigr) = \dfrac{1}{n}\mathbb{E}(X_1 +X_2 + \cdots + X_n) = \dfrac{1}{n}n\mu = \mu.
\]

\vspace{0.2cm}

y as\'i:

\vspace{0.2cm}

\begin{align*}
\mathbb{E}\Bigl(\Bigl[\dfrac{1}{n}S_n - \mu \Bigr]^2\Bigr) &= \mathbb{V}\Bigl(\dfrac{1}{n}S_n\Bigr)\\
  &= \dfrac{1}{n^2}\mathbb{V}(X_1 +X_2 + \cdots + X_n)\\
  &= \dfrac{1}{n^2}(\mathbb{V}X_1 + \cdots  + \mathbb{V}X_n)\\
  &=\dfrac{1}n{n^2}\sigma^2 = \dfrac{1}{n}\sigma^2 \rightarrow 0 \ \ \ \text{cuando}\ \ n \rightarrow \infty
\end{align*}

\vspace{0.2cm}

y as\'i, $n^{-1}S_n \rightarrow \mu$ en media cuadr\'atica cuando $n \rightarrow \infty$.
\end{teo}

\begin{defi}
\normalfont Decimos que la secuencia de variables aleatoria $X_1, X_2, \dots$ de variables aleatorias \texttt{converge en probabilidad } a $X$ si,

\vspace{0.2cm}

\[
\texttt{Para todo} \ \ \epsilon >0 \ \ \ \mathbb{P}(\vert X_n - X\vert > \epsilon) \rightarrow \ \ \text{cuando}\ \ n \rightarrow \infty
\]

\vspace{0.2cm}

Si esto se cumple, escribimos \texttt{$X_n \rightarrow X$ en probabilidad cuando $n \rightarrow \infty$ }

\end{defi}

\vspace{0.3cm}

\begin{teo}
\normalfont Si $X_1, X_2, \dots $ es una secuencia de variables aleatorias y $X_n \rightarrow X$ en media cuadrada cuando $n \rightarrow \infty$ entonces $X_n \rightarrow X$ en probabilidad.

\vspace{0.2cm} 

Para hacer la prueba de este teorema, escribamos la \texttt{la desigualdad de Chebyshev} de la siguiente manera

\vspace{0.2cm}

Si $Y$ es una variable aleatoria y $\mathbb{E}(Y^2) < \infty$, entonces:

\vspace{0.2cm}

\[
\mathbb{P}(\vert Y \vert \geq t) \leq \dfrac{1}{t^2}\mathbb{E}(Y^2)\ \ \ \text{para}\ \ \ t >0
\]


Luego aplicando la desigualdad de Chebyshev a la variable aleatoria $Y = X_n - X$ para encontrar que:

\vspace{0.2cm}

\[
\mathbb{P}(\vert X_n -X\vert > \epsilon) \leq \dfrac{1}{\epsilon^2}\mathbb{E}([X_n -X]^2)\ \ \ \epsilon > 0.
\]

\vspace{0.2cm}

Si $X_n \rightarrow X$ en media cuadrada cuando $n \rightarrow \infty$, el lado derecho de la desigualdad tiende a $0$ cuando $n \rightarrow \infty$ y as\'i tiende a $0$ para todo $\epsilon > 0$, como es requerido. El caso contrario no se cumple.
\end{teo}

\vspace{0.3cm}

\begin{teo}
\normalfont \textbf{(Ley d\'ebil de los grandes n\'umeros(2))} Sea $X_1, X_2, \dots$ una secuencia de variables aleatorias, cada una con media $\mu$ y varianza $\sigma^2$. El promedio de los primeros $n$ de los $X_i$ satisface, cuando $n \rightarrow \infty$,

\vspace{0.2cm}

\[
\dfrac{1}{n}(X_1 +X_2 + \cdots + X_n) \rightarrow \mu \ \ \ \text{en probabilidad}.
\]
\end{teo}
\vspace{0.2cm}

\begin{ejemplo}
\normalfont Sea $X_n$ una secuencia de variables aleatorias que convergen en probabilidad, pero  no en media cuadrada con una funci\'on de masa de probabilidad:

\vspace{0.2cm}

\[
\mathbb{P}(X_n = 0) =  1 - \dfrac{1}{n},  \ \ \ \ \mathbb{P}(X_n = n) = \dfrac{1}{n}.
\]


\vspace{0.2cm}

Entonces, para $\epsilon > 0$ y para un $n$ dado, tenemos: 

\vspace{0.2cm}


\[
\mathbb{P}(\vert X_n\vert > \epsilon) = \mathbb{P}(X_n = n) = \dfrac{1}{n} \rightarrow 0 \ \ \ \text{cuando}\ \ n \rightarrow \infty
\]

\vspace{0.2cm}

dando que $X_n \rightarrow \infty $ en probabilidad. Por otro lado, 

\vspace{0.2cm}

\begin{align*}
\mathbb{E}([X_n - 0]^{2}) = \mathbb{E}(X_n^2) = 0\Bigl(1 -\dfrac{1}{n}\Bigr) + n^2\dfrac{1}{n}\\
= n \rightarrow \infty \ \  \text{cuando} \ \ n \rightarrow \infty.
\end{align*}

\vspace{0.2cm}

as\'i $X_n$ no converge a $0$ en media cuadrada.
\end{ejemplo}

\vspace{0.2cm}

\begin{ejemplo}
\normalfont Considere el lanzamiento de  una moneda donde la probabilidad de obtener cara es $p$. Sea $X_i$  el resultado de un lanzamiento $(0-1)$. Por lo tanto, $p = \mathbb{P}(X_i = 1) = \mathbb{E}(X_i)$. La fracci\'on de caras  despu\'es de $n$ lanzamientos  es  $\overline{X}_n$ . De acuerdo con la ley de los grandes n\'umeros, $\overline{X}_n$ converge en probabilidad a $p$. Esto no quiere decir que $\overline{X}_n$ se num\'ericamente igual $p$. Esto significa que, cuando $n$ es grande, la distribuci\'on de $\overline{X}_n$ est\'a  'concentrado' alrededor $p$.

\vspace{0.2cm}

Supongamos que $p = 1/2$, encontremos el valor de $n$ para que $\mathbb{P}(.4\leq \overline{X}_n \leq .6) \geq .7$. Para ello tenemos que $\mathbb{P}(\overline{X}_n) = p = 1/2$ y $\mathbb{V}(\overline{X}_n) = \sigma^2/n = p(1 -p)/n = 1/n$. Por la desigualdad de Chebyshev, se tiene:

\vspace{0.2cm}

\begin{align*}
\mathbb{P}(.4\leq \overline{X}_n \leq .6) = \mathbb{P}(\vert \overline{X}_n - \mu) \leq .1)\\
= 1 - \mathbb{P}(\vert \overline{X}_n - \mu\vert > .1)\\
\geq 1 - \dfrac{1}{4n(.1)^2} = 1 - \frac{25}{n}
\end{align*}

\vspace{0.2cm}

Esta expresi\'on es mayor que $.7$ si $n = 84$.
\end{ejemplo}

\vspace{0.3cm}

\begin{teo}
\normalfont \textbf{(Teorema del l\'imite central)} Sea $X_1, X_2, \dots $ variables aleatorias identicamente distribuidas e independientes cada una con media $\mu$ y varianza distinta de cero $\sigma^2$. Por la ley de los grandes n\'umeros, la suma $S_n = X_1 +X_2+ \cdots +X_n$ es casi tan grande que $n\mu$ para valores de $n$ muy grandes. Un siguiente paso es determinar el orden de la diferencia $S_n - n\mu$, que resulta ser de orden $\sqrt{n}$.

\vspace{0.2cm}

Se define la versi\'on est\'andar de $S_n$

\vspace{0.2cm}

\[
Z_n = \dfrac{S_n -\mathbb{E}(S_n)}{\sqrt{\mathbb{V}(S_n)}}
\]

\vspace{0.2cm}

Esta es una funci\'on lineal $Z_n = a_nS_n + b_n$ de $S_n$, donde $a_n$ y $b_n$ tienen que ser elegida de manera que $E(Z_n) = 0$ y $var(Z_n) = 1$. Adem\'as:

\vspace{0.2cm}

\begin{align*}
\mathbb{E}(S_n ) &= \mathbb{E}(X_1) + \mathbb{E}(X_2) + \cdots + \mathbb{E}(X_n )\\
&= n\mu
\end{align*}


Tambi\'en, 

\vspace{0.2cm}

\begin{align*}
\mathbb{V}(S_n) &= \mathbb{V}(X_1) + \mathbb{V}(X_2) + \cdots +\mathbb{V}(X_n)\\
&= n\sigma^2
\end{align*}

y as\'i,

\vspace{0.2cm}

\[
Z_n = \dfrac{S_n - n\mu}{\sigma\sqrt{n}}
\]
\end{teo}


\vspace{0.5cm}


Muchas de las propiedades de $Z_n$ se pueden sintetizar en el teorema de l\'imite central:

\vspace{0.3cm}

Sean $X_1, X_2, \dots $ variables aleatorias identicamente distribuidas e independiente, cada una con media $\mu$ y varianza distinta de cero $\sigma^2$. La versi\'on est\'andar: 

\vspace{0.2cm}

\[
Z_n = \dfrac{S_n - n\mu}{\sigma\sqrt{n}}
\]

\vspace{0.2cm}

de la suma $S_n = X_1 +X_2 + \cdots + X_n$, satisface cuando $n \rightarrow \infty $:

\vspace{0.2cm}

\[
\mathbb{P}(Z_n \leq x) \rightarrow \bigintss_{-\infty}^{x}\dfrac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}u^2}du = \Phi(x)\ \ \ \text{para}\ \ x\in \mathbb{R}.
\]

\vspace{0.2cm}

El lado derecho de la \'ultima de la ecuaci\'on es s\'olo la funci\'on de distribuci\'on de la distribuci\'on normal con media $0$ y varianza $1$, as\'i que esta expresi\'on se puede escribir como:

\vspace{0.2cm}

\[
\mathbb{P}(Z_n \leq x) \rightarrow \mathbb{P}(Y \leq x) \ \ \ \text{para}\ \ x \in \mathbb{R}.
\]


\vspace{0.2cm}


donde $Y$ es la variable aleatoria con esta distribuci\'on normal est\'andar. Tambi\'en podemos  escribir el teorema  de la siguiente forma: 

\vspace{0.2cm}

\[
\lim_{n \rightarrow \infty}\mathbb{P}(Z_n \leq x) = \Phi(x)\ \ \text{para cada $x$}.
\]

\vspace{0.2cm}


El teorema del l\'imite central, nos permite calcular probabilidades relacionadas a $Z_n$ como si $Z_n$ fuese normal. Desde que la normalidad es preservada bajo transformaciones lineales, esto es equivalente  a tratar $S_n$ como una distribuci\'on normal con media $n\mu$ y varianza $n\sigma^2$. 

\vspace{0.2cm}


Sea $S_n = X_1 +X_2+ \cdots +X_n$ donde las $X_i$ son variables aleatorias id\'enticamente distribuidas e independientes cada una con media $\mu$ y varianza distinta de cero $\sigma^2$. Si $n$ es muy grande, la probabilidad $\mathbb{P}(S_n \leq c)$ puede ser aproximada considerando $S_n$ como si fuese normal de acuerdo  al siguiente procedimiento:

\vspace{0.2cm}

\begin{itemize}
\item Calculamos la media $n\mu$ y la varianza $n\sigma^2$ de $S_n$.
\item Calculamos el valor normalizado $z = (c - n\mu)/\sigma\sqrt{n}$.
\item Usamos la aproximaci\'on, 

\[
\mathbb{P}(S_n \leq c) \approx \Phi(z),
\]

\vspace{0.2cm}

donde $\Phi(z)$ es  disponible desde la tabla de funci\'on densidad acumulativa   de la distribuci\'on est\'andar. 
\end{itemize}


\vspace{0.2cm}

\begin{ejemplo}
\normalfont  Cargamos en un avi\'on $100$ paquetes cuyos pesos son  variables aleatorias independientes  que se distribuyen uniformemente entre $5$ y $50$ libras. ?`Cu\'al es la probabilidad de que el peso total es superior a 3000 libras?.

\vspace{0.2cm}

No es f\'acil calcular la funci\'on densidad acumulativa  del peso total y la probabilidad deseada, pero una  respuesta aproximada se puede obtener usando el teorema del l\'imite central:

\vspace{0.2cm}

Calculemos $\mathbb{P}(S_{100} > 3000)$, donde $S_{100}$ es la suma de los pesos de los $100$ paquetes. En este caso la media y la varianza de los pesos de un \'unico paquete es: 

\vspace{0.2cm}

\[
\mu = \dfrac{5 + 50}{2} = 27.5 \ \ \ \ \sigma^{2} = \dfrac{(50 -5)^2}{12} = 168.75.
\]


\vspace{0.3cm}

basados en la f\'ormulas para la media y la varianza de la funci\'on densidad de una distribuci\'on  uniforme, calculemos el valor normalizado :

\vspace{0.2cm}

\[
z = \dfrac{3000 - 100\cdot27.5}{\sqrt{168.75 -100}} = \dfrac{250}{129.9} = 1.92
\]

\vspace{0.3cm}

y usando  las tablas normal est\'andar para obtener la aproximaci\'on tenemos:

\vspace{0.2cm}

\[
\mathbb{P}(S_{100} \leq 3000) \approx \Phi(1.92) = 0.9726.
\]

\vspace{0.2cm}

y as\'i la probabilidad pedida es:

\vspace{0.2cm}

\[
\mathbb{P}(S_{100} > 3000) = 1 - \mathbb{P}(S_{100} \leq 3000) \approx 1 - 0.97626 = 0.0274.
\]

\end{ejemplo}

\vspace{0.5cm}


Para la prueba(parcial) de este teorema usaremos el m\'etodo de las funciones generadoras de momentos.

\begin{teo}

\normalfont \textbf{(Teorema de continuidad)} Sea $X_1, X_2, \dots$ una secuencia de variables aleatorias con funciones generadores de momentos $M_1, M_2, \dots$ y suponiendo que para $n \rightarrow \infty$

\vspace{0.2cm}

\[
\mathbb{M}_n(t) \rightarrow e^{\frac{1}{2}t^2}\ \  \text{para } \ \ t\in \mathbb{R}
\]

\vspace{0.2cm}

Entonces:

\vspace{0.2cm}

\[
\mathbb{P}(Z_n \leq x) \rightarrow \bigintss_{-\infty}^{x}\dfrac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}u^2}du \ \ \ \text{para}\ \ x\in \mathbb{R}.
\]

\vspace{0.2cm}

En otras palabras, la funci\'on distribuci\'on de $Z_n$ converge a la funci\'on distribuci\'on de la distribuci\'on  normal si la funci\'on generadora de momentos de $Z_n$ converge a la funci\'on generadora de momentos de la distribuci\'on normal. 

\vspace{0.2cm}

Usemos estas propiedades  para probar el \texttt{teorema del l\'imite central} en el caso que las $X_i$ tienen una funci\'on generadora de momentos:

\vspace{0.2cm}


\[
\mathbb{M}_{X}(t) = \mathbb{E}(\exp(tX_i))\ \ \ \text{para}\ \  i=1,2 \dots
\]

\vspace{0.2cm}


haciendo hincapi\'e en que el teorema del l\'imite central es v\'alido  incluso cuando esta esperanza no existe, siempre que la media y la varianza de la $X_i$ sean finitas.

\vspace{0.3cm}

Sea $U_i = X_i - \mu$. Entonces $U_1, U_2, \dots$ son variables aleatorias id\'enticamente distribuidas e independientes con media y varianza dado por:

\vspace{0.2cm}

\[
\mathbb{E}(U_i) = 0, \ \ \ \ \mathbb{E}(U_i^2) = \mathbb{V}(U_i) = \sigma^2,
\]

\vspace{0.2cm}

y una funci\'on generadora de momentos:

\vspace{0.2cm}

\[
\mathbb{M}_{U}(t) = \mathbb{M}_{X}(t)e^{-\mu t}
\]

\vspace{0.2cm}

Ahora, tenemos:

\vspace{0.2cm}

\[
Z_n = \dfrac{S_n - n\mu}{\sigma \sqrt{n}} = \dfrac{1}{\sigma\sqrt{n}}\displaystyle\sum_{i = 1}^{n}U_i
\]

\vspace{0.2cm}

La funci\'on generadora de momentos de $Z_n$ es:

\vspace{0.2cm}

\begin{align*}
\mathbb{M}_n(t) = \mathbb{E}(\exp(tZ_n)) &= \mathbb{E}\Bigl(\exp \Bigl(\dfrac{t}{\sigma\sqrt{n}}\displaystyle\sum_{i = 1}^{n}U_i\Bigr) \Bigr)\\
&= \Bigl[\mathbb{M}_{U}\Bigl( \dfrac{1}{\sigma\sqrt{n}}\Bigr) \Bigr]^{n}
\end{align*}

\vspace{0.2cm}

Expandamos $\mathbb{M}_U(x)$ como una serie de potencia alrededor de $x = 0$, para conocer el comportamiento de $\mathbb{M}_U(t/(\sigma\sqrt{n})$ para $n \rightarrow \infty$,

\vspace{0.2cm}

\begin{align*}
\mathbb{M}_U(x) &= 1 + x\mathbb{E}(U_1) +\frac{1}{2}x^2\mathbb{E}(U_1^2) + o(x^2)\\
 &= 1 + \frac{1}{2}\sigma^2x^2 + o(x^2)
\end{align*}

\vspace{0.2cm}

Reemplazando este resultado con $x = t/(\sigma\sqrt{n}$ y $t$ fijo.

\vspace{0.2cm}

\[
\mathbb{M}_n(t) = \Bigl[1 + \dfrac{t^2}{2n} +o\Bigl(\dfrac{1}{n} \Bigr) \Bigr]^{n} \rightarrow e^{\frac{1}{2}t^2} \ \ \ \text{cuando}\ \ n\rightarrow \infty .
\]

\vspace{0.2cm}

y por el teorema de continuidad tenemos el resultado del teorema del l\'imite central. La prueba requiere la existencia de $\mathbb{M}_X(t)$ para valores de $t$ cerca de $0$.
\end{teo}


\vspace{0.3cm}
\begin{ejemplo}
\normalfont \textbf{(Muestreo estad\'istico)} Una fracci\'on $p$ desconocida de la poblaci\'on son caballeros jedi. Se desea estimar $p$ con un error no superior a $0.005$ sobre   una muestra de individuos (se supone que contestan con la verdad). ?`Qu\'e tan grande es esa muestra?

\vspace{0.2cm}

Supongamos que se elige una muestra de $N$ individuos. Sea $X_i$ la funci\'on indicador del evento de que la i-\'esima persona admite ser un caballero jedi, y suponiendo  que las $X_i$ son variables aleatorias independientes, de Bernoulli con par\'ametro $p$, escribimos entonces:

\vspace{0.2cm}

\[
S_n = \displaystyle\sum_{i = 1}^{n}X_i,
\]


\vspace{0.2cm}

Elegimos para estimar $p$ con la media muestral $n^{-1}S_n$,  al siguiente operador $\overline{p}$ seg\'un la notaci\'on estad\'istica.

\vspace{0.2cm}

Deseamos elegir $n$ suficientemente grande para que $\vert \overline{p} - p \vert \leq 0,005$. Esto no se puede hacer, ya que $\vert \overline{p} - p \vert$ es una variable aleatoria que puede (aunque con una peque\~na probabilidad) tomar un valor mayor que $0.005$ para un $n$ dado. El enfoque aceptado es establecer un nivel m\'aximo de la probabilidad en la que se permite la ocurrencia de un error. Por convenci\'on, tomamos este como  $0.05$ que nos lleva al siguiente problema: encontrar un $n$ tal que,

\vspace{0.2cm}

\[
\mathbb{P}(\vert \overline{p} - p \vert \leq 0.005) \geq 0.95.
\]

\vspace{0.2cm}

Como $S_n$ es la suma de variables aleatorias id\'enticamente distribuidas con media $p$ y varianza $p(1 -p)$. La probabilidad de arriba puede ser escrita como:

\vspace{0.2cm}

\begin{align*}
\mathbb{P}\Bigl(\Bigl\vert \dfrac{S_n}{n} - p\Bigr\vert  \leq 0.005\Bigr) &= \mathbb{P}\Bigl( \dfrac{\vert S_n - np\vert}{\sqrt{np(1 -p)}} \leq 0.005\sqrt{\dfrac{n}{p(1 - p)}}\Bigr)\\
&= \mathbb{P}\Bigl( \dfrac{\vert S_n - np\vert}{\sqrt{\mathbb{V}(S_n)}} \leq 0.005\sqrt{\dfrac{n}{p(1 - p)}}\Bigr)
\end{align*}

\vspace{0.2cm}

Por el teorema del l\'imite central, $(S_n - \mathbb{E}(S_n))/\sqrt{\mathbb{V}(s_n)}$ converge  a la distribuci\'on normal y as\'i la probabilidad final puede ser aproximada por una integral de funci\'on densidad normal. Un inconveniente  a este resultado es que el rango de esta integral depende del valor de $p$ que es desconocido.

\vspace{0.2cm}

Desde que $p(1 -p) \leq \frac{1}{4}$ para $p \in [0,1]$, se tiene:

\vspace{0.2cm}

\[
\mathbb{P}\Bigl( \dfrac{\vert S_n - \mathbb{E}(S_n)\vert}{\sqrt{\mathbb{V}(S_n)}} \leq 0.005\sqrt{\dfrac{n}{p(1 - p)}}\Bigr)\geq \mathbb{P}\Bigl( \dfrac{\vert S_n - \mathbb{E}(S_n)\vert}{\sqrt{\mathbb{V}(S_n)}} \leq 0.005\sqrt{4n}\Bigr)
\]

\vspace{0.2cm}

y el lado derecho es aproximadamente $\mathbb{P}(\vert N \vert \leq 0.005\sqrt{4n}$, donde $N$ es normal con media $0$ y varianza $1$. Por tanto:

\vspace{0.2cm}

\begin{align*}
\mathbb{P}(\vert \hat{p} - p \vert \leq 0.005) &\gtrapprox \bigintsss_{-0.005\sqrt{4n}}^{0.005\sqrt{4n}}\dfrac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}u^2}du \\
& = 2\Phi(0.005\sqrt{4n}) -1
\end{align*}

\vspace{0.2cm}

donde $\Phi$ es la funci\'on de distribuci\'on de $N$. Consultando a las tablas, encontramos que es mayor que $0.95$ si $0.005\sqrt{4n}\geq 1.96$, es decir cuando $n \gtrapprox 40.000$.

\end{ejemplo}

\vspace{0.5cm}

En el caso multivariarado el teorema del l\'imite central, se escribe de la siguiente manera: Para $X_1, \dots X_n$ vectores aleatorios id\'enticamente distribuidos independientes, donde:

\vspace{0.3cm}

\[
X_i = \begin{pmatrix}
X_{1i}\\
X_{2i}\\
\vdots \\
X{ki}
\end{pmatrix}
\]

\vspace{0.2cm}

con media :

\vspace{0.2cm}

\[
\mu = \begin{pmatrix}
\mu_{1}\\
\mu_{2}\\
\vdots \\
\mu_{k}
\end{pmatrix}
 = \begin{pmatrix}
\mathbb{E}(X_{1i})\\
\mathbb{E}(X_{2i})\\
\vdots\\
\mathbb{E}(X_{ki})
\end{pmatrix}
\]

\vspace{0.2cm}

y matriz varianza $\Sigma$. Sea :

\vspace{0.2cm}

\[
\overline{X} = \begin{pmatrix}
\overline{X}_1\\
\overline{X}_2\\
\vdots \\
\overline{X}_{k}
\end{pmatrix}
\]

\vspace{0.2cm}

donde $\overline{X}_j = n^{-1}\displaystyle\sum_{i =1}^{n}X_{ji}$. Entonces:

\vspace{0.2cm}

\[
\sqrt{n}(\overline{X} - \mu) \rightarrow N(0, \Sigma).
\]

\vspace{0.2cm}

\subsection{Distribuciones que no cumplen el teorema del l\'imite central}

No todas las distribuciones obedecen al teorema del l\'imite central. Un ejemplo es la variable aleatoria de Cauchy, que tiene como  funci\'on  densidad de probabilidad:

\[
f(x) = \frac{1}{\pi(1 + x)^2}.
\]

La esperanza y todos los momentos superiores de una variable aleatoria de Cauchy no est\'an definidos. En particular, no tiene una varianza finita y por lo tanto no satisface las condiciones del teorema del l\'imite central.
\end{document}

