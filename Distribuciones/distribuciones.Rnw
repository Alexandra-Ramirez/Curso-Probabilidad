\documentclass{article}
\usepackage[sc]{mathpazo}
\usepackage[T1]{fontenc}
\usepackage{bigints}
\usepackage[latin1]{inputenc}
\usepackage{geometry}
\usepackage{amsmath} 
\usepackage{mathtools}
%\usepackage[spanish]{babel}
\usepackage{latexsym}
\geometry{verbose,tmargin=1.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
\usepackage{url}
\usepackage{graphicx}
\newcommand{\R}{\mathbb{R}}
\newcommand{\bb}[1]{\textbf{#1}}
\DeclareMathOperator{\rank}{\textbf{rango}}
\DeclareMathOperator{\proy}{\textbf{proy}}
\DeclareMathOperator{\nulll}{\textbf{nul}}
\DeclareMathOperator{\diag}{\textbf{diag}}
\DeclareMathOperator{\col}{\textbf{col}}
\DeclareMathOperator{\fila}{\textbf{fila}}
\DeclareMathOperator{\dimm}{dim}
\DeclareMathOperator{\Traz}{Tr}
%\theoremstyle{definition}
\everymath{\displaystyle}
\newtheorem{ejemplo}{{Ejemplo }}[section]
\newtheorem{defi}{{Definici\'on}}[section]
\newtheorem{pros}{{Proposici\'on}}[section]
\newtheorem{cor}{{Corolario}}[section]
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=2,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=false]
 {hyperref}
\hypersetup{
 pdfstartview={XYZ null null 1}}
\begin{document}

<<setup, include=FALSE, cache=FALSE>>=
library(knitr)
library(ggplot2)
library(grid)
#library(animation)
# set global chunk options
opts_chunk$set(fig.path='figure/minimal-', fig.align='center', fig.show='hold')
options(formatR.arrow=TRUE,width=90)
@

%\title{\underline{\textbf{Notas de mat\'ematica}}}
%\date{}
%\maketitle
\hspace*{0.5\linewidth}
\begin{minipage}{0.6\linewidth}
Curso: Introducci\'on a la Probabilidad y Estad\'istica CM -274\par
Distribuciones importantes \par
\end{minipage}


\vspace {0.5cm}


\section{Distribuciones discretas }
 
 
 Algunas de la m\'as importantes variables aleatorias discretas, se listan a continuaci\'on:


\subsection{Distribuci\'on de masa puntual} $X$ tiene una distribuci\'on de masa puntual en $a$, $X \sim \delta_a$, si $\mathbb{P}(X =a) =1$, en el caso que 

\[
F_X(x) = \begin{cases}
0 & x < a \\
1 & x \geq a.
\end{cases}
\]

La funci\'on de masa de probabilidad es $p_X(x) =1$ para $x = a$ y $0$ en otros casos.


\subsection{Distribuci\'on uniforme discreta} Sea $k > 1$ un n\'umero entero. Supongamos que $X$ tiene \texttt{PMF} dado por

\[
p_X(x) = \begin{cases}
1/k & \mbox{para} \ 1, \dots ,k\\
0 & \mbox{ en otros casos.}
\end{cases}
\]

\vspace{0.2cm}

Decimos que $X$ tiene una distribuci\'on uniforme sobre $\{1, \dots, k \}$.

\vspace{0.3cm}

\subsection{Distribuci\'on de Bernoulli} Sea $X$ que representa una lanzamiento de una moneda. Entonces $\mathbb{P}(X = 1) = p$ y $\mathbb{P}(X= 0) = 1 - p$ para alg\'un $p \in [0,1]$, entonces decimos que $X$ tiene una distribuci\'on de Bernoulli, escrita como $X \sim\mbox{Bernoulli}(p)$. La funci\'on \mbox{probabilidad} es $p_X(x) = p^{x}(1 - p)^{1-x}$ para $x \in \{0,1\}$.

\vspace{0.3cm}

\begin{figure}[ht]
\centering
\includegraphics[scale=.45]{p1.png}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[scale=.45]{p2.png}
\end{figure}


\vspace{0.2cm}

La distribuci\'on de \texttt{Bernoulli(p)} tiene una media:

\[
\mathbb{E}(X) = \sum_{x = 0,1}xp(X =x) = 0(1 -p) + 1p = p.
\]

\vspace{0.2cm}

Adem\'as:

\vspace{0.2cm}

\[
\mathbb{E}(X)^2 = \sum_{x = 0,1}x^2p(X =x) = 0^2(1 -p) + 1^2p = p
\]

y as\'i la varianza:

\[
\mathbb{V}(X) = \mathbb{E}(X^2) - [\mathbb{E}(X)]^2 = p - p^2 = p(1 -p).
\]

\vspace{0.2cm}

La varianza  es maximizada para el valor de $p = 1/2$ y cada momento es el mismo $\mathbb{E}(X^{\alpha}) = 0^{\alpha}(1 -p) + 1^{\alpha}p = p$.
\vspace{0.3cm}

\subsection{Distribuci\'on binomial} Supongamos que tenemos una moneda, que cae cara con una probabilidad $p$ para alg\'un $0 \leq p \leq 1$. Lanzamos la moneda $n$ veces y sea $X$ el n\'umero de caras. Asumimos que estos lanzamientos son independientes. El PMF  de X, $p_X(x) = \mathbb{P}(X = x)$  para esta distribuci\'on es:

\vspace{0.2cm}


\[
p_X(x) = \begin{cases}
\binom{n}{x}p^{x}(1 - p)^{x} & \mbox{para} \ x =0, \dots ,n\\
0 & \mbox{ en otros casos.}
\end{cases}
\]

Esta es la suma de $n$ variables aleatorias independientes de Bernoulli. Se denota como $X \sim\mbox{Binomial}(n, p)$.


\vspace{0.2cm}

Desde que la esperanza es un operador lineal, la esperanza de una distribuci\'on Binomial, es la suma de las esperanzas de las distribuciones de Bernoulli involucradas, as\'i si $X$ es una distribuci\'on \texttt{Binomial(n,p)}:

\[
\mathbb{E}(X) = np,
\]

y desde que la varianza de la suma de variables independientes es la suma de varianza,

\[
\mathbb{V}(X) = np(1 -p).
\]


\begin{figure}[ht]
\centering
\includegraphics[scale=.45]{p3.png}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[scale=.45]{p4.png}
\end{figure}


\vspace{0.3cm}


\subsection{Distribuci\'on binomial negativa} Replicar un experimento  de \texttt{Bernoulli(p)} de forma independiente hasta que ocurre el r-\'esimo  \'exito $(r \geq 1)$ Sea $X$  el n\'umero de pruebas  en el cual se  produce el r-\'esimo \'exito. Entonces se dice que $X$ tiene una distribuci\'on  \texttt{binomial negativa(r,p)}.

\vspace{0.2cm}

Existe otra definici\'on de distribuci\'on binomial negativa, a saber, la distribuci\'on del n\'umero de fallos antes del r-\'esimo \'exito. (Esta es la definici\'on empleada por R, en los c\'alculos).

La relaci\'on entre las dos definiciones es bastante simple. Si $X$ es binomial negativo en el sentido usual  y $F$ es binomial negativo en el sentido de R, entonces:

\[
F = X -r
\]

?` Cu\'al es la probabilidad de que el r-\'esimo  \'exito ocurre en la prueba  $t$, para $t \geq r$ ?.  Para que esto suceda, debe haber $t- r$ fracasos y $r -1$ \'exitos, en las primeras $t - 1$ pruebas, con un \'exito en la prueba $t$.

Por independencia, esto sucede con la probabilidad binomial para $r- 1$ \'exitos en las primeras $t -1$ pruebas, por  la probabilidad $p$ de \'exito en la pruena $t$:

\[
\mathbb{P}(X =t) = \binom{t -1}{r -1}p^r(1 -p)^{t -r}\ \ (t \geq r).
\]

de esta manera la probabilidad es cero, cuando $t < r$. El caso especial $r = 1$ es llamada \texttt{distribuci\'on geom\'etrica}. Es decir, $X$ tiene una distribuci\'on geom\'etrica con par\'ametro $p \in (0,1)$, denotada  como $X \sim \mbox{Geom}(p)$, si

\[
\mathbb{P}(X = k) = p(1 -p)^{k -1}, \ \ k \geq 1.
\]

\vspace{0.2cm}

Tenemos a partir de este resultado que 

\vspace{0.2cm}

\[
\sum_{k = 1}^{\infty}\mathbb{P}(X= k) = p\sum_{k = 1}^{\infty}(1 - p)^{k} = \frac{p}{ 1 - (1 - p)} = 1.
\]

\vspace{0.2cm}

$X$ es el n\'umero de lanzamientos necesarios hasta que  la primera cara salga, cuando una moneda es lanzada. La media de una distribuci\'on geom\'etrica, tiene el valor de:

\[
\mathbb{E}(X) = \sum_{t =1}^{\infty}tp(1 - p)^{t -1} = \frac{1}{p}.
\]

Adem\'as que se tiene:

\[
\mathbb{V}(X) = \mathbb{E}(X^2) - [\mathbb{E}(X)]^2 = \frac{1 - p}{p}.
\]
 
 
\vspace{0.2cm}

La distribuci\'on \texttt{binomial negativa(r,p)} es la suma de $r$ de variables aleatorias  geom\'etricas independientes \texttt{Geom(p)}. Como la esperanza es un operador lineal positivo, se concluye que la media de \texttt{binomial negativa(r,p)} es $r$ veces la media que \texttt{Geom(p)} y la varianza de una suma independiente es la suma de varianza, as\'i:

\vspace{0.2cm}

Si $X \sim \texttt{binomial negativa(r,p)}$, entonces:

\[
\mathbb{E}(X) = \frac{r}{p}\ \ \text{y}\ \ \mathbb{V}(X) = \frac{r(1 -p)}{p^2}.
\]
 
 
\vspace{0.3cm}


\subsection{Distribuci\'on multinomial} En un sentido, la distribuci\'on multinominal generaliza la distribuci\'on binomial a experimentos aleatorios independientes con m\'as de dos resultados. Es la distribuci\'on de un vector que cuenta cu\'antas veces se produce cada resultado. Se denota como $X \sim\mbox{Multinomial}(n, \textbf{p})$.

\vspace{0.2cm}

Un vector aleatorio multinomial es un $m$ -vector \textbf{X}  de resultados en una secuencia de $n$ repeticiones independientes de un experimento aleatorio con $m$ resultados distintos.

\vspace{0.2cm}

Si el experimento tiene $m$ resultados posibles y el i-\'esimo resultado tiene probabilidad $p_i$, entonces la funci\'on de masa de probabilidad \texttt{Multinomial(n, \textbf{p})} viene dada por:

\[
\mathbb{P}(\textbf{X} = (k_1, \dots, k_m)) = \frac{n!}{k_1!k_2!\cdots k_m!}p_1^{k_1}\cdot p_2^{k_2}\cdots p_m^{k_m}.
\]

donde $k_1 + k_2 + \cdots + k_m = n$.

\vspace{0.2cm}

Si contamos el resultado $k$ como \texttt{\'exito}, entonces es obvio que cada $X_k$ es simplemente una variable aleatoria  binomial (n, pk). Pero los componentes no son independientes, ya que suman a n.


\vspace{0.3cm}

\subsection{Distribuci\'on de Rademacher} La distribuci\'on \texttt{Rademacher(p)} es una grabaci\'on de la distribuci\'on de Bernoulli, $1$ todav\'ia indica \'exito, pero el fracaso se codifica como $-1$. Si $Y$ es una variable aleatoria \texttt{Bernoulli(p)}, entonces $X = 2Y -1$ es una variable aleatoria \texttt{Rademacher(p)}. La funci\'on de masa de probabilidad es:

\[
\mathbb{P}(X =x) = \begin{cases}
p & x =1 \\
1 -p & x = -1
\end{cases}
\]

\vspace{0.2cm}

Una variable aleatoria $X$ \texttt{Rademacher(p)} tiene una media:

\[
\mathbb{E}(X) = \sum_{x = -1,1}xp(X =x) = -1(1 -p) + 1p = 2p -1.
\]

Adem\'as  $X^2 = 1$, as\'i:

\[
\mathbb{E}(X^2) = 1,
\]

por tanto la varianza es:

\[
\mathbb{V}(X) = \mathbb{E}(X^2) -[\mathbb{E}(X)]^2 = 1 - (2p -1)^2 = 4p(1 - p).
\]

Una secuencia de sucesivas sumas de variables aleatoria independientes de \texttt{Rademacher(p)}  es llamado \textbf{camino aleatorio}. Esto es, si $X_i$ son id\'enticamente distribuidas a  variables aleatorias \texttt{Rademacher(1/2)}, la secuencia $S_1, S_2, \dots$ es un camino aleatorio, donde:

\[
S_n = X_1 + \cdots X_n.
\]

Desde que el operador esperanza es un operador lineal:

\[
\mathbb{E}(S_n)  = 0,\ \ \text{para todo}\ \  n,
\]

\vspace{0.2cm}

y desde que la varianza  de la suma de variables aleatorias independientes es la suma de varianzas:

\[
\mathbb{V}(S_n) = n, \ \ \text{para todo}\ \ n.
\]
\vspace{0.3cm}

\subsection{Distribuci\'on de Poisson} Una variable aleatoria de Poisson $N$ modela el recuento de  \'exitos cuando la probabilidad de \'exitos es peque\~na y el n\'umero de pruebas independientes  es grande, de modo que la tasa de \'exito promedio $\mu$. Se denota esta variable aleatoria  como $X \sim \mbox{Poisson} (\mu)$ si:

\[
\mathbb{P}(N = k) = e^{-\mu}\frac{\mu^{k}}{k!} \ \ k = 0,1, \dots.
\]


As\'i :

\[
\mathbb{E}(N) = \mu \ \ \text{y}\ \ \mathbb{V}(N) = \mu.
\]


\vspace{0.2cm}


La distribuci\'on de Poisson es usado a menudo como un modelo para eventos, como decaimiento radiactivo y accidentes de tr\'afico.


\vspace{0.2cm}

Ladislaus von Bortkiewicz  se refiri\'o a la distribuci\'on de Poisson como \texttt{La Ley de los n\'umeros peque\~nos}. Se puede pensar esto, como un l\'imite peculiar de las distribuciones binomiales. Consideremos una secuencia de variables \texttt{binomiales(n, p)}, donde la probabilidad $p$ de \'exito va a cero, pero el n\'umero $n$ de pruebascrece de tal manera que $np = \mu $ permanece fijo. Entonces tenemos la siguiente proposici\'on:

\vspace{0.2cm}


\begin{pros}

\normalfont Para cada $k$:

\[
\lim_{n \rightarrow \infty}\text{Binomial}(n, \mu/k)(k) = \text{Poisson}(\mu)(k).
\]
\end{pros}

\vspace{0.3cm}



\begin{figure}[ht]
\centering
\includegraphics[scale=.5]{p5.png}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[scale=.5]{p6.png}
\end{figure}

\vspace{0.3cm}


\section{Distribuciones continuas }
 
 
 Algunas de la m\'as importantes variables aleatorias continuas, se listan a continuaci\'on:
 
 \subsection{La familia Normal}
 
 De acuerdo con el teorema del l\'imite central, la distribuci\'on limitante de la suma estandarizada un gran n\'umero de variables aleatorias independientes,es la distribuci\'on normal.
 
 \vspace{0.2cm}
 
 
 La densidad de $N(\mu, \sigma^2)$ es:
 
 \[
 f_{\mu, \sigma^2}(x) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x -\mu)^2}{2\sigma^2}}
 \]
 
 La media de una variable aleatoria  $N(\mu, \sigma^2)$ es:
 
 \[
 \mathbb{E}(X) = \mu
 \]
 
 y la varianza es:
 
 \[
 \mathbb{V}(X) = \sigma^2.
 \]
 
La \texttt{distribuci\'on normal est\'andar} tiene media $\mu = 0$ y $\sigma^2 = 1$, as\'i la funci\'on densidad:

\[
f(x ) = \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}.
\]

La media es: $\mathbb{E}(X) = 0$ y la varianza $\mathbb{V}(X) = 1$.

\vspace{0.4cm}

El CDF de esta variable aleatoria, se denota como $\Phi$ y se define como:

\[
\Phi(t) = \frac{1}{\sqrt{2\pi}}\bigints_{-\infty}^{t}e^{-\frac{x^2}{2}} dx.
\]

\vspace{0.3cm}

\textbf{(Familia Normal)} Si $Z$ es una variable normal est\'andar, entonces:

\[
\sigma Z + \mu  \sim N(\mu, \sigma^2)
\]

y si 

\[
X \sim N(\mu, \sigma^2), \ \ \text{entonces}\  \frac{x - \mu}{\sigma} \sim N(0,1)
\]

\vspace{0.2cm}

Si $X$ y $Z$ son normales e independientes, entonces $aX + bZ$ es tambi\'en normal.


\begin{figure}[ht]
\centering
\includegraphics[scale=.5]{p7.png}
\end{figure}

\vspace{0.2cm}

\begin{figure}[ht]
\centering
\includegraphics[scale=.5]{p8.png}
\end{figure}

\vspace{0.2cm}

\begin{figure}[ht]
\centering
\includegraphics[scale=.5]{p9.png}
\end{figure}

\vspace{0.2cm}

\begin{figure}[ht]
\centering
\includegraphics[scale=.5]{p10.png}
\end{figure}

\subsection{La familia Exponencial}

La familia Exponencial se usa para modelar tiempos de espera. Una variable aleatoria exponencial \texttt{Exponencial ($\lambda$)} tiene una densidad:

\[
f(t) = \lambda e^{-\lambda t}
\]

y un CDF: 

\[
F(t) = 1 -e^{-\lambda t}.
\]

\vspace{0.2cm}

Si definimos la funci\'on (Survival function):

\[
G(t) = 1 -F(t) = e^{-\lambda t}.
\]

Entonces la tasa de Hazard $f(t)/G(t)$ es igual a $\lambda$.

\vspace{0.2cm}

La media de la \texttt{Exponencial ($\lambda$)} es:

\[
\mathbb{E}(T) = \frac{1}{\lambda}
\]

y la varianza es:

\[
\mathbb{V}(T) = \frac{1}{\lambda^2}.
\]

\vspace{0.2cm}

La familia Exponencial tiene la \textbf{Propiedad de Markov} o \texttt{memoryless}:

\[
\mathbb{P}(T < t +s | T > t) = \mathbb{P}(T > s).
\]

\vspace{0.2cm}


\begin{figure}[ht]
\centering
\includegraphics[scale=.5]{p11.png}
\end{figure}

\vspace{0.2cm}

\begin{figure}[ht]
\centering
\includegraphics[scale=.5]{p12.png}
\end{figure}

\subsection{La familia Gamma}

La familia  de distribuciones \texttt{Gamma(r, $\lambda$)} es vers\'atil. La distribuci\'on de la suma de $r$  variables aleatorias \texttt{Exponencial($\lambda$)} independientes, la distribuci\'on del $r$- \'esimo tiempo de llegada  en un proceso de Poisson con tasa de llegada $\lambda$ es la distribuci\'on  \texttt{Gamma(r, $\lambda$)}.

La distribuci\'on de la suma de cuadrados de $n$ distribuciones  normales est\'andar independientes, la distribuci\'on $\chi^2(n)$ es la distribuci\'on  \texttt{Gamma(1/2, 1/2)}.

\vspace{0.3cm}

La distribuci\'on  \texttt{Gamma(r, $\lambda$)}  en general ($r > 0, \lambda > 0$), tiene una densidad dado por:

\[
f(t) = \frac{\lambda^r}{\Gamma(r)}t^{r -1}e^{-\lambda t}\ \ (t > 0).
\]

\vspace{0.2cm}

La media y la varianza de un variable aleatoria \texttt{Gamma(r, $\lambda$)} es dado por:

\[
\mathbb{E}(X) = \frac{r}{\lambda}\ \ \ \mathbb{V}(X) = \frac{r}{\lambda^2}.
\]

\vspace{0.2cm}


\begin{figure}[ht]
\centering
\includegraphics[scale=.5]{p13.png}
\end{figure}

\vspace{0.2cm}

\begin{figure}[ht]
\centering
\includegraphics[scale=.5]{p14.png}
\end{figure}

\vspace{0.2cm}


\begin{figure}[ht]
\centering
\includegraphics[scale=.5]{p15.png}
\end{figure}

\vspace{0.2cm}

\begin{figure}[ht]
\centering
\includegraphics[scale=.5]{p16.png}
\end{figure}


\subsection{Distribuci\'on de Cauchy}

Si $X$ e $Y$ son normales est\'andar independientes, entonces $Y/X$ tiene una distribuci\'on de Cauchy. La distribuci\'on de Cauchy, es la distribuci\'on de la tangente de un \'angulo aleatoriamente seleccionado desde $[-\pi, \pi]$.

\vspace{0.2cm}

La densidad es:

\[
f(x) = \frac{1}{\pi(1 + x^2)},
\]


\vspace{0.3cm}

y el CDF:

\vspace{0.2cm}


\[
F(t) = \frac{1}{\pi}\arctan(t) + \frac{1}{2}.
\]

\vspace{0.2cm}


\begin{align*}
\int_{-\infty}^{\infty}f(x)dx = \frac{1}{\pi}\int_{-\infty}^{\infty}\frac{dx}{(1 + x^2)} =  \frac{1}{\pi}\int_{-\infty}^{\infty}\frac{d\tan^{-1}(x)}{dx}\\ =
\frac{1}{\pi}[\tan^{-1}(\infty) - \tan^{-1}(-\infty)] 
&=  \frac{1}{\pi}[\frac{\pi}{2} - (-\frac{\pi}{2})] = 1.
\end{align*}

\vspace{0.2cm}

La esperanza de una distribuci\'on de Cauchy no existe :


\vspace{0.3cm}

\begin{align*}
\int_{0}^{t}\frac{x }{\pi(1 + x^2)}dx = \frac{\ln (1 + t^2)}{2 \pi} \rightarrow  \infty\ \ \text{si}\ \ t \rightarrow \infty. \\
\int_{-t}^{0}\frac{x }{\pi(1 + x^2)}dx = \frac{-\ln (1 + t^2)}{2 \pi} \rightarrow  -\infty\ \ \text{si}\ \ t \rightarrow \infty.
\end{align*}


\vspace{0.2cm}

As\'i

\[
\int_{-\infty}^{\infty}\frac{x }{\pi(1 + x^2)}dx \ \ \ \text{es divergente y sin sentido}.
\]

\vspace{0.2cm}

\begin{figure}[ht]
\centering
\includegraphics[scale=.6]{p17.png}
\end{figure}

\vspace{0.2cm}

\begin{figure}[ht]
\centering
\includegraphics[scale=.6]{p18.png}
\end{figure}


\subsection{Distribuci\'on uniforme}

Una variable aleatoria continua $X$ que es igualmente probable que tome  cualquier valor en un rango de valores $(a, b)$, con $a <b$, da lugar a la distribuci\'on uniforme. Dicha distribuci\'on est\'a uniformemente distribuida en su rango. Algunas veces se denota como $X \sim \text{Uniforme}(a,b)$.

\vspace{0.3cm}

La funci\'on densidad de la distribuci\'on es:

\[
f(x) = \begin{cases}
\frac{1}{b -a} & x \in [a,b] \\
0 & \text{en otros casos}.
\end{cases}
\]

\vspace{0.2cm}

El CDF es:

\vspace{0.2cm}


\[
F(x) = \begin{cases}
\frac{x- a}{b -a} & x \in [a,b] \\
0 & \text{en otros casos}.
\end{cases}
\]

\vspace{0.2cm}

La media y la varianza de la variable aleatoria uniforme es:

\[
\mathbb{E}(X) = \frac{a + b}{2}\ \ \ \mathbb{V}(X) = \frac{(b -a)^2}{12}.
\]


\vspace{0.2cm}

\begin{figure}[ht]
\centering
\includegraphics[scale=.6]{p19.png}
\end{figure}

\vspace{0.2cm}

\begin{figure}[ht]
\centering
\includegraphics[scale=.6]{p20.png}
\end{figure}

\subsection{Distribuci\'on beta}

$X$ tiene una distribuci\'on $\mbox{Beta}$ con par\'ametros $\alpha > 0$  y $\beta > 0$, denotado por $X \sim \mbox{Beta}(\alpha, \beta)$, si


\vspace{0.2cm}

\[
f(x) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}x^{\alpha -1}(1 -x)^{\beta -1}, \ \ 0< x < 1.
\]

\vspace{0.2cm}

La media y la varianza de una distribuci\'on Beta es:

\[
\mathbb{E}(X) = \frac{\alpha }{\alpha + \beta}\ \ \ \mathbb{V}(X) = \frac{\alpha \beta}{(\alpha + \beta)^2(1 + \alpha + \beta)}.
\]

\vspace{0.2cm}

\begin{figure}[ht]
\centering
\includegraphics[scale=.5]{p21.png}
\end{figure}

\vspace{0.2cm}

\begin{figure}[ht]
\centering
\includegraphics[scale=.5]{p22.png}
\end{figure}


\vspace{0.2cm}

\begin{figure}[ht]
\centering
\includegraphics[scale=.5]{p23.png}
\end{figure}

\subsection{La distribuci\'on $\chi^2(n)$}

Sea $Z_1, \dots, Z_n$ variables aleatorias normales est\'andar. La distribuci\'on Chi-cuadrado con $n$ grados de libertad, $\chi^2(n)$ es la distribuci\'on de:

\[
R_n^2 =Z_1^2 +\cdots + Z_n^2.
\]

\vspace{0.2cm}

La funci\'on densidad de esta distribuci\'on, est\'a dado por:

\[
f_{R_n^2}(t) = \frac{1}{2^{n/2}\Gamma(n/2)}t^{(n/2) -1}e^{-t/2}, \ \ (t > 0), 
\]

y escribimos:

\[
R_n^2 \sim \chi^2(n).
\]

\vspace{0.2cm}

La media y varianza son caracterizadas por:

\[
\mathbb{E}(R_n^2) =n, \ \ \mathbb{V}(R_n^2) = 2n.
\]

\vspace{0.2cm}

\begin{figure}[ht]
\centering
\includegraphics[scale=.5]{p24.png}
\end{figure}

\vspace{0.3cm}

Se sigue de la definici\'on  que si $X$ e $Y$ son independientes y $X \sim \chi^2(n)$ y $Y \sim \chi^2(m)$, entonces:

\[
(X + Y) \sim \chi^2(n + m).
\]

\vspace{0.2cm}

\begin{figure}[ht]
\centering
\includegraphics[scale=.5]{p25.png}
\end{figure}

\vspace{0.3cm}

Esta es una distribuci\'on importante en estad\'istica inferencial y es la base de la prueba de ajustes chi-cuadrado y el m\'etodo de estimaci\'on de  m\'inimos de  chi-cuadrado.

\vspace{0.2cm}

Si hay $m$ resultados posibles de un experimento y sea $p_i$ la probabilidad de que el resultado $i$ ocurre. Si el experimento se repite independientemente $N$ veces, sea $N_i$ el n\'umero de veces que se observa el resultado $i$, por lo que $N = N_1 + \cdots + N_m$. Entonces la estad\'istica del chi-cuadrado:

\vspace{0.2cm}

\[
\sum_{i =1}^{n}\frac{(N_i - Np_i)^2}{Np_i},
\]

\vspace{0.2cm}

converge en distribuci\'on cuando $N \rightarrow$ a  una distribuci\'on $\chi^2(m -1)$ con $ m -1$ grados de libertad.
\end{document}
