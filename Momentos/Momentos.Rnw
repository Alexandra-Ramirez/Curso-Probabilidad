\documentclass{article}
\usepackage[sc]{mathpazo}
\usepackage[T1]{fontenc}
\usepackage{bigints}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{amsmath} 
\usepackage{mathtools}
\usepackage{latexsym}
\geometry{verbose,tmargin=1.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
\usepackage{url}
\usepackage{graphicx}
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=2,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=false]
 {hyperref}
\hypersetup{
 pdfstartview={XYZ null null 1}}
\begin{document}
<<setup, include=FALSE, cache=FALSE>>=
library(knitr)
library(ggplot2)
library(grid)
library(animation)
# set global chunk options
opts_chunk$set(fig.path='figure/minimal-', fig.align='center', fig.show='hold')
options(formatR.arrow=TRUE,width=90)
@

\title{Introducci\'on a la Estad\'istica y Probabilidades CM-274}
\date{}
\maketitle

\vspace{0.3cm}

{\Large Momentos }

\vspace{0.3cm}

Para una variable aleatoria $X$ el \texttt{k-\'esimo momento} de $X$ es definido para $k = 0, 1,2, \cdots$ al n\'umero $E(X^k)$ que es la esperanza de la \texttt{k-\'esima potencia } de $X$ siempre que la esperanza  existe.

La secuencia $E(X), E(X^2), \dots$ contiene mucha informaci\'on acerca de la distribuci\'on de $X$.

\vspace{0.3cm}

Dada la funci\'on de distribuci\'on $F_X$ de la variable aleatoria $X$, podemos calcular su momento siempre que esta exista, ya sea X discreta o continua. Lo contrario es decir dada una secuencia $E(X), E(X^2), \dots$ de momentos(finitos) de $X$ es posible reconstruir $X$ es en general no se cumple, salvo algunas condiciones extras.

\vspace{0.3cm}

\textbf{Teorema 1} Supongamos que todos los momentos $E(X), E(X^2)\dots$ de la variable aleatoria$X$ existen y que la serie

\[
\sum_{k = 0}^{\infty}\dfrac{1}{k!}t^kE(X^k)
\]
 
 es absolutamente convergente para alg\'un $t > 0$. Entonces la secuencia de momentos determina unicamente la distribuci\'on de $X$.

\vspace{0.3cm}


\textbf{Varianza y Covarianza}

\vspace{0.3cm}

La varianza de una variable aleatoria $X$ es definida como

\[
var(X) = E([X- \mu]^2), 
\]

\vspace{0.2cm}

donde $\mu = E(X)$. La varianza es una medida de dispersi\'on con respecto a $\mu$, en el sentido de que si $X$ toma valores que difieren considerablemente de $\mu$, entonces el valor de $\vert X- \mu \vert$ es grande y as\'i  $E([X- \mu]^2)$ tiene un valor grande, mientras que si $X$ es cercano al valor de $\mu$, entonces $\vert X- \mu \vert$ tiene un valor que usualmente peque\~no de la misma forma que $E([X- \mu]^2)$.

\vspace{0.2cm}

En un caso extremo cuando $X$ es concentrado en alg\'un punto, para una variable $Y$ 

\[
E(Y^2) = 0 \ \ \leftrightarrow \ \ P(Y = 0) =1 
\]

\vspace{0.2cm}

si aplicamos la ecuaci\'on anterior a $Y = X - \mu$ nos da

\[
var(X) \ \ \leftrightarrow \ \ P(X = \mu) =1 
\]

\vspace{0.2cm}

de esta forma la varianza cero significa que \texttt{no hay dispersi\'on en lo absoluto}. Algunas propiedades ya vista de la varianza son las siguientes:

\begin{itemize}
\item $var(X) = E([X- \mu]^2) = E(X^2) - \mu^2$ donde $E(X) = \mu$.
\item $var(aX + b) = a^2var(X)$.
\end{itemize}

\vspace{0.3cm}

La varianza como medida de dispersi\'on tiene una propiedad no deseable: no es lineal en el sentido de que la varianza de $aX$ es $a^2$ veces la varianza de $X$, por esta raz\'on es preferible trabajar con la \texttt{desviaci\'on est\'andar} de $X$, definido como $\sqrt{var (X)}$.

\vspace{0.2cm}

En t\'erminos de $var(X)$ y $var(Y)$, la expresi\'on de $var(X + Y) $ es de la forma:

\[
var(X + Y) = var(X) + 2E([X -E(X)][Y -E(Y)]) +var(Y)
\]


\vspace{0.5cm}


La \texttt{covarianza} de las variables aleatorias $X$ y $Y$ es la cantidad denotada por $cov(X,Y)$ y es dada por 

\[
cov(X,Y) = E([X -E(X)][Y -E(Y)]) = E(XY) -E(X)E(Y)
\]

\vspace{0.3cm}
siempre que esas esperanzas existan.

\vspace{0.3cm}

Podemos agregar algunas propiedades m\'as

\begin{itemize}
\item $var(X +Y) = var(X) + 2cov(X,Y) + var(Y)$
\item Si $X$ y $Y$ son independientes : $cov(X,Y) = 0$ y $var(X + Y) = var(X) + var(Y)$.
\end{itemize}


\vspace{0.3cm}

La principal desventaja de la covarianza es una medida de dependencia es que no es invariante en la escala: Si $X$ y $Y$ esta\'an medidas en pulgadas y $U$ y $V$ tienen las misma medida en cent\'imetros (tal que $U = \alpha X$ y $V = \alpha Y$, donde $\alpha \sim 2.54$), entonces $cov(U,V) \sim 6cov(X,Y)$ a pesar del hecho de que el par $(X,Y)$ y $(U,V)$ miden la misma cantidad. Para tratar con esta \texttt{re-escala} de la covarianza  se define lo siguiente:


\vspace{0.5cm}

La \texttt{correlaci\'on} de las variables aleatorias $X$ y $Y$ es la cantidad $\rho(X,Y)$ dado por:

\vspace{0.3cm}

\[
\rho(X,Y) = \dfrac{cov(X ,Y)}{\sqrt{var(X)var(Y)}}
\]

\vspace{0.2cm}

siempre que la \'ultima cantidad exista y $var(X)var(Y) \neq 0$. Algunas propiedades de la correlaci\'on que se pueden deducir de la definici\'on son:


\vspace{0.3cm}

\begin{itemize}
\item $\rho(aX +b, cY + d) = \rho(X,Y)$, para $a,b, c, d \in \mathbb{R}$ tal que $ac \neq 0$ (la correlaci\'on es invariante a escala).
\item (Teorema de Cauchy-Schwarz) Si $U$ y $V$ son variables aleatorias, entonces 
\[
[E(UV)]^2 \leq E(U^2)E(V^2)
\]
siempre que las esperanzas existan.
\item Si $X$ y $Y$ son variables aleatorias, entonces $-1 \leq \rho(X,Y) \leq 1$, siempre que a correlaci\'on exista. 
\end{itemize}

\vspace{0.3cm}

Sea $a = var(X)$, $b = 2cov(X,Y$, $c = var(Y)$  y supongamos que $\rho(X,Y) \pm 1$. Entonces tenemos que  $var(X)var(Y) \neq 0$ y 

\[
b^2 - 4ac = 4var(X)var(Y)[\rho(X,Y)^2 - 1] = 0
\]

\vspace{0.2cm}

y as\'i la ecuaci\'on cuadr\'atica: $as^2 + bs + c = 0$ tiene dos ra\'ices iguales en $s = \alpha$. Por tanto, $W = \alpha[X -E(X)] + [Y -E(Y)]$ satisface

\[
E(W^2) = a\alpha^2 + b\alpha + c =0
\]

\vspace{0.2cm}

con $P(W =0) = 1 $ y probando que $Y = -\alpha X + \beta$, donde $\beta = \alpha E(X) + E(Y)$.

\vspace{0.3cm}

Un tratamiento m\'as exahustivo distingue los valores de $\pm 1$:

\begin{itemize}
\item $\rho(X,Y) = 1$ si y s\'olo si $P(Y = \alpha X + \beta$) = 1 para alg\'un real $\alpha$ y $\beta$ con $\alpha > 0$.
\item $\rho(X,Y) = -1$ si y s\'olo si $P(Y = \alpha X + \beta$) = 1 para alg\'un real $\alpha$ y $\beta$ con $\alpha < 0$.
\end{itemize}

\vspace{0.5cm}

Se usa $\rho(X,Y)$ como una medida de las dependencias de $X$ y $Y$. Si $X$ y $Y$ tienen varianza distintas de cero, entonces $\rho(X,Y)$ toma alg\'un valor en el intervalo $[-1, 1]$ y este valor debe ser interpretado de la forma en que los valores $-1, 0, 1$ surgen:

\begin{itemize}
\item Si $X$ y $Y$ son independientes entonces $\rho(X,Y) = 0$.
\item $Y $ es una funci\'on creciente de $X$ si y s\'olo si $\rho(X,Y) = 1$.
\item $Y $ es una funci\'on decreciente de $X$ si y s\'olo si $\rho(X,Y) = -1$.
\end{itemize}

\vspace{0.3cm}

Si $\rho(X,Y) = 0$  se dice que $X$ y $Y$ son \texttt{no correlacionados}.


\vspace{0.8cm}

{\Large Funciones generadoras de momentos}

\vspace{0.3cm}

Si $X$ es una variable aleatoria tomando los valores en $\{ 0,1,2, \dots\}$ la funci\'on generadora de probabilidad es definida como 

\vspace{0.2cm}

\[
G_X(s) = E(s^X) = \sum_{k = 0}^{\infty}s^{k}P(X =k)
\]

\vspace{0.3cm}

Las funciones generadoras de probabilidad son muy \'utiles, pero s\'olo cuando las variables aleatorias relacionados toman valores enteros no negativos. Para variables aleatorias es necesario hacer una modificaci\'on de esta ecuaci\'on.

\vspace{0.3cm}

La \texttt{funci\'on generadora de momentos (mgf)} de la variable aleatoria de $X$ es la funci\'on $M_X$ definida como

\vspace{0.2cm}

\[
M_{X}(t) = E(e^{tX})
\]

\vspace{0.2cm}

para todo $t \in \mathbb{R}$ para el cual la esperanza existe.


\vspace{0.5cm}

Si $X$ toma valores en $\{0,1,2, \dots \}$, entonces

\vspace{0.2cm}

\[
M_{X}(t) = E(e^{tX}) = G_{X}(e^t)
\]

\vspace{0.2cm}

con la sustituci\'on de $s = e^t$. En general:

\vspace{0.5cm}

\[
M_{X}(t) = E(e^{tX}) = \begin{dcases}
\sum_{x}e^{tx}P(X =x) & \ \ \text{si}\ \ X \ \ \text{es discreta}\\
\bigintsss_{-\infty}^{\infty}e^{tx}f_{X}(x)dx & \ \ \text{si}\ \ X \ \ \text{es continua}
\end{dcases}
\]

\vspace{0.3cm}

siempre que esta suma o integral converga absolutamente.

\vspace{0.5cm}

\textbf{Ejemplo} Si $X$ tiene una distribuci\'on normal con media $0$ y varianza $1$, entonces

\vspace{0.3cm}


\begin{align*}
 M_{X}(t) &= \bigintsss_{-\infty}^{\infty}e^{tx}\dfrac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}x^2}dx \\
 &= e^{\frac{1}{2}t^2}\bigintsss_{-\infty}^{\infty}\dfrac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}(x -t)^2}dx \\
 & = e^{\frac{1}{2}t^2}
\end{align*}

\vspace{0.3cm}

desde que el integrando es la \'ultima integral es la funci\'on densidad de la distribuci\'on normal con media $t$ y varianza $1$, la integral tiene que ser $1$. El momento $M_{X}(t)$ existe para todo $t \in \mathbb{R}$.


\vspace{0.5cm}

La dificultad de la existencia de $E(e^{tX})$ puede ser evitada usando la funci\'on de variable compleja $\phi_{X}(t) = E(e^{itX})$ llamada \texttt{funci\'on caracter\'istica} de $X$ que existe para todo $t \in \mathbb{R}$.  Es importante decir que $E(e^{tX})$ existe en alguna vecindad del origen y la raz\'on est\'a contenida en el teorema de unicidad de funciones generadores de momentos. 

\vspace{0.5cm}

La funci\'on generadora de momentos sigue la expansi\'on que muestra que $M_{X}(t)$ es la funci\'on generadora exponencial de los momentos de $X$:


\vspace{0.3cm}

\begin{align*}
M_{X}(t) = E(e^{tX}) = E\Bigl(1 +tX + \dfrac{1}{2!}(tX)^2 + \cdots \Bigr) \\
 = 1 + tE(X) + \dfrac{1}{2!}t^2E(X^2) + \cdots
\end{align*}


\vspace{0.3cm}

\textbf{Teorema 2} Si $M_{X}(t)$ existe en una vecindad de $0$, entonces para $k = 1, 2 \dots$

\vspace{0.2cm}

\[
E(X^k) = M_{X}^{(k)}(0)
\]

\vspace{0.2cm}

la \texttt{k-\'esima} derivada de $M_X(t)$ evaluada en $t = 0$.


\vspace{0.6cm}

Sea la funci\'on lineal $aX + b$ de la variable aleatoria $X$. Si $a ,b \in \mathbb{R}$


\vspace{0.2cm}

\begin{align*}
M_{aX +b}(t) = E(e^{t(aX + b)}) = E(e^{atX}e^{tb}) \\
= e^{tb}E(e^{(at)X})
\end{align*}

\vspace{0.2cm}


lo que produce que $M_{aX + b}(t) = e^{tb}M_{X}(at)$.


\vspace{0.5cm}

\textbf{Teorema 3} Si $X$ e $Y$ son variablea aleatorias independientes, entonces $X + Y$ tiene una funci\'on generadora de momentos:

\vspace{0.2cm}

\[
M_{X +Y}(t) = M_{X}(t)M_{Y}(t)
\]


\vspace{0.3cm}

Tenemos por independencia y propiedad de la esperanza para la variable aleatoria $X$ y $Y$:

\begin{align*}
M_{X +Y}(t) = E(e^{t(x + y)}) = E(e^{tX}e^{tY}) \\
= E(e^{tX})E(e^{tY})
\end{align*}

\vspace{0.3cm}

En general la suma $S = X_1 + \cdots + X_n$ de $n$ variables aleatorias independientes tiene una funci\'on generadora de momentos de la forma:

\vspace{0.2cm}

\[
M_{S}(t) = M_{X_1}(t)\cdots M_{X_n}(t)
\]

\vspace{0.5cm}


\textbf{Teorema 4 (Unicidad)} Si la funci\'on generadora de momentos $M_{X}$ satisface $M_{X}(t) = E(e^{tX}) < \infty$ para todo $t$ satisfaciendo $-\delta < t < \delta$ para alg\'un $\delta > 0$, entonces existe una \'unica distribuci\'on con  funci\'on generadora de momentos $M_{X}$. Adem\'as bajo esta condici\'on, se tiene que $E(X^{k}) < \infty$ para $k = 1,2, \dots$ y

\vspace{0.3cm}

\[
M_{X}(t) = \displaystyle\sum_{k =0}^{\infty}\dfrac{1}{k!}E(X^k) \ \ \text{para} \ \ \vert t\vert < \delta.
\]

\vspace{0.5cm}

\textbf{Ejemplo} Sean $X$ e $Y$ dos variables aleatorias independiendientes, teniendo $X$ una distribuci\'on normal con par\'ametros $\mu_1$ y $\sigma_1^2$ y $Y$  una distribuci\'on normal con par\'ametros $\mu_2$ y $\sigma_2^2$, veamos que propiedades tiene la suma $Z = X +Y$:

\vspace{0.3cm}

Sea $U$ una variable aleatoria que tiene una distribuci\'on normal con par\'ametros $\mu$ y $\sigma^2$. La funci\'on generadora de momentos de $U$ es

\vspace{0.3cm}

\begin{align*}
M_U(t) &= \bigintsss_{-\infty}^{\infty}e^{tu}\dfrac{1}{\sqrt{2\pi\sigma^2}}\exp{\Bigl(-\dfrac{1}{2\sigma^2}(u -\mu)^2}\Bigr)du \\
=& e^{\mu t}\bigintsss_{-\infty}^{\infty}e^{x\sigma t}\dfrac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}x^2}dx \ \ \ \text{sustituyendo} \ \ \ x = \dfrac{u -\mu}{\sigma}\\
=& \exp(\mu t + \frac{1}{2}\sigma^2t^2)
\end{align*}

\vspace{0.3cm}

Por el teorema 3

\begin{align*}
M_{Z}(t) = M_{X}(t)M_{Y}(t) \\
 = \exp{(\mu_1 t + \frac{1}{2}\sigma_1^2t^2)}\exp{(\mu_2 t + \frac{1}{2}\sigma_2^2t^2)} \\
 =\exp\Bigl[ (\mu_1 + \mu_2)t + \frac{1}{2}(\sigma_1^2 + \sigma_2^2)t^2\Bigr]
\end{align*}

\vspace{0.3cm}

Luego se deduce que es la funci\'on generadora de momentos  de la distribuci\'on normal con par\'ametros $\mu_1 + \mu_2$ y $\sigma_1^2 + \sigma_2^2$. As\'i $Z$ tiene esta distribuci\'on  apelando al teorema de unicidad.

\vspace{0.8cm}

{\Large Algunos resultados}

\vspace{0.3cm}

Sea $-\infty \leq a < b < \infty$ una funci\'on $g: (a ,b) \rightarrow \mathbb{R}$ es \texttt{convexa} si

\vspace{0.2cm}

\[
g([1 -t]u + tv) \geq (1 -t)g(u) + tg(v)
\]

\vspace{0.2cm}

para $t \in [0,1]$ y $u, v \in (a,b)$.


\vspace{0.5cm}


Presentamos el teorema de soporte de hiperplanos que servir\'a para la desigualdad de Jensen, basado en el gr\'afico siguiente

\vspace{0.2cm}

\begin{figure}[h]
\centering
\includegraphics[scale=.55]{j1.png}
\end{figure}

\vspace{0.3cm}

\textbf{Teorema 5} Sea $g:(a,b) \rightarrow \mathbb{R}$ convexa y sea $w \in (u,v)$. Entonces existe un $\alpha \in \mathbb{R}$ tal que 

\vspace{0.2cm}

\[
g(x) \geq g(w) + \alpha(x -w)\ \ \text{para}\ \ \ x\in (a,b)
\]


\vspace{0.5cm}

\textbf{Desigualdad de Jensen} Sea $X$ una variable aleatoria que toma valores en el intervalo (posiblemente infinito) $(a ,b)$ tal que $E(X)$ existe y sea $g:(a, b)\rightarrow \mathbb{R}$ una funci\'on convexa tal que $E\vert g(X) \vert < \infty$. Entonces

\vspace{0.2cm}

\[
E(g(X)) \geq g(E(X)).
\]

\vspace{0.5cm}

Sea $X$ que toma valores en $(a ,b)$ con media $\mu = E(X)$. Sea $g$ una funci\'on convexa sobre este intervalo satisfaciendo $E\vert g(X) \vert < \infty$. Por el teorema 5 con $w = \mu$ existe un $\alpha \in \mathbb{R}$ tal que $g(x) \geq g(\mu) + \alpha(x -\mu)$. Por tanto $g(X) \geq g(\mu) + \alpha(X -\mu)$. Tomando esperanza, obtenemos $E(g(X) \geq g(\mu)$.

\vspace{0.3cm}


\textbf{Ejemplo} La funci\'on $g(x) = -\log x$ es convexa en el intervalo $(0, \infty)$. Por la desigualdad de Jensen aplicada a una variable aleatoria $X$ positiva con media finita

\vspace{0.3cm}

\[
E(\log X) \leq \log(E(X))
\]

\vspace{0.3cm}

Supongamos que $X$ es una variable aleatoria discreta que es igual de probable de tener los valores $x_1, x_2, \dots, x_n$. Entonces

\vspace{0.2cm}

\[
E(\log X) =\dfrac{1}{n}\displaystyle\sum_{i = 1}^{n}\log x_i = \log \gamma, \ \ \ E(X) = \overline{X}
\]

\vspace{0.3cm}

donde 

\vspace{0.3cm}

\[
\gamma = \Bigl( \prod_{i=1}^{n}x_i\Bigr)^{1/n}, \ \ \ \ \ \  \overline{x} = \dfrac{1}{n}\displaystyle\sum_{i=1}^{n}x_i
\]

\vspace{0.2cm}

son la media geom\'etrica y aritm\'etica de los valores $x_i$ respectivamente, con la propiedad que $\log \gamma \leq \log \overline{X}$ que conduce a que  $\gamma \leq \overline{x}$.
 \end{document}
